# Advanced Muon Optimizer Features - Results Report

**Date:** 2025-12-25
**Experiment:** Variance Reduction and Cautious Weight Decay for nGPT
**Configuration:** Gated nGPT, Muon (lr=0.003), Polar Express, alpha=0.15, batch=32, 400 steps

---

## Executive Summary

**Finding: Combining Variance Reduction and Cautious Weight Decay provides the best results for nGPT.**

We tested two advanced features ported from `modded-nanogpt`:
1. **Variance Reduction (NorMuon):** Low-rank variance estimation to scale updates.
2. **Cautious Weight Decay:** Decoupled weight decay that only applies when gradient and parameter signs match.

### Key Results

| Configuration | Val Loss | Final Train | Tokens/sec | Improvement | Status |
|---------------|----------|-------------|------------|-------------|--------|
| Base (Polar Express) | 6.4246 | 6.5270 | 26,317 | Baseline | |
| + Variance Reduction | 6.3923 | 6.3738 | 24,554 | +0.50% | |
| + Cautious WD | 6.3792 | 6.3761 | 27,838 | +0.71% | |
| **+ VR + Cautious WD** | **6.3505** | **6.4009** | **24,948** | **+1.15%** | **‚≠ê BEST** |

**Total Improvement:** +1.15% over Polar Express baseline, and **~6.3%** over the initial Adam baseline (6.775).

---

## Detailed Analysis

### 1. Variance Reduction (NorMuon)
- **Effect:** Provided a consistent improvement in convergence quality.
- **Cost:** ~7% throughput reduction in PyTorch fallback.
- **Insight:** Adaptive scaling of the orthogonalized update helps stable convergence on the hypersphere.

### 2. Cautious Weight Decay
- **Effect:** Surprisingly effective even for nGPT where weights are normalized.
- **Throughput:** Negligible cost (actually appeared faster in this run, likely noise).
- **Insight:** The "cautious" gate helps preserve useful features while regularizing.

### 3. Synergistic Effects
- The combination of both features (VR + CWD) was additive, yielding the lowest validation loss of **6.3505**.
- This configuration is now the recommended "State-of-the-Art" for nGPT experiments.

---

## Recommendations

For all future nGPT training at this scale:
- **Optimizer:** Muon with Polar Express
- **Learning Rate:** 0.003
- **Advanced Features:** Enable both Variance Reduction and Cautious Weight Decay.
- **Weight Decay:** 0.01 (with Cautious WD enabled).

---
*Report generated by Gemini CLI.*
