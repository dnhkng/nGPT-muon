nGPT Architectural Experiment: A1_ne
================================================================================
Model: 11 layers x 768 dim (matching modded-nanogpt)
Hypothesis: Baseline
================================================================================

Loading FineWeb data (subset for fast experiments)...
  Loading data/fineweb10B/fineweb_train_000001.bin...
  Loading data/fineweb10B/fineweb_train_000002.bin...
  Loading data/fineweb10B/fineweb_train_000003.bin...
  Loading data/fineweb10B/fineweb_train_000004.bin...
  Vocab size: 50257
  Train tokens: 400,001,536
  Val tokens: 100,000,384

Model config (modded-nanogpt size):
  vocab_size: 50257
  n_layer: 11
  n_embd: 768
  n_head: 6
  block_size: 128
Using GatedBlock (H3) with gate_init=0.0

Total parameters: 155,166,743

Optimizer: SimpleMuon (lr=0.003, momentum=0.95, geodesic=baseline, orthog=newton_schulz)
  2D weight params (Muon): 77,856,768
  1D params (Adam): 77,309,975

Starting training (5000 steps)...
--------------------------------------------------------------------------------
Step    0: loss = 10.9685
Step   50: loss = 7.5764
  [Val @ step 50]: train=7.5764, val=7.3620
Step  100: loss = 7.0826
  [Val @ step 100]: train=7.0826, val=6.9451
Step  150: loss = 6.6519
  [Val @ step 150]: train=6.6519, val=6.7924
Step  200: loss = 6.6327
  [Val @ step 200]: train=6.6327, val=6.6451
Step  250: loss = 6.5493
  [Val @ step 250]: train=6.5493, val=6.5467
Step  300: loss = 6.2643
  [Val @ step 300]: train=6.2643, val=6.4764
Step  350: loss = 6.5118
  [Val @ step 350]: train=6.5118, val=6.4164
Step  400: loss = 6.4533
  [Val @ step 400]: train=6.4533, val=6.3861
Step  450: loss = 6.3952
  [Val @ step 450]: train=6.3952, val=6.3723
Step  500: loss = 6.3209
  [Val @ step 500]: train=6.3209, val=6.3428
Step  550: loss = 6.1318
  [Val @ step 550]: train=6.1318, val=6.3168
Step  600: loss = 6.1480
  [Val @ step 600]: train=6.1480, val=6.2821
Step  650: loss = 6.2938
  [Val @ step 650]: train=6.2938, val=6.2856
Step  700: loss = 6.1565
  [Val @ step 700]: train=6.1565, val=6.2173
Step  750: loss = 6.3193
  [Val @ step 750]: train=6.3193, val=6.1992
Step  800: loss = 6.1381
  [Val @ step 800]: train=6.1381, val=6.2042
Step  850: loss = 6.0328
  [Val @ step 850]: train=6.0328, val=6.1510
Step  900: loss = 6.0175
  [Val @ step 900]: train=6.0175, val=6.1714
Step  950: loss = 6.2491
  [Val @ step 950]: train=6.2491, val=6.1345
Step 1000: loss = 6.0618
  [Val @ step 1000]: train=6.0618, val=6.1066
Step 1050: loss = 6.1096
  [Val @ step 1050]: train=6.1096, val=6.1082
Step 1100: loss = 6.0580
  [Val @ step 1100]: train=6.0580, val=6.0839
Step 1150: loss = 5.9576
  [Val @ step 1150]: train=5.9576, val=6.0573
Step 1200: loss = 6.0813
  [Val @ step 1200]: train=6.0813, val=6.0164
Step 1250: loss = 6.0785
  [Val @ step 1250]: train=6.0785, val=6.0289
Step 1300: loss = 5.9743
  [Val @ step 1300]: train=5.9743, val=6.0402
Step 1350: loss = 6.0429
  [Val @ step 1350]: train=6.0429, val=5.9823
Step 1400: loss = 5.8935
  [Val @ step 1400]: train=5.8935, val=5.9893
Step 1450: loss = 5.9382
  [Val @ step 1450]: train=5.9382, val=6.0380
Step 1500: loss = 5.9519
  [Val @ step 1500]: train=5.9519, val=5.9845
Step 1550: loss = 6.0073
  [Val @ step 1550]: train=6.0073, val=5.9700
Step 1600: loss = 5.9400
  [Val @ step 1600]: train=5.9400, val=5.9999
Step 1650: loss = 5.7946
  [Val @ step 1650]: train=5.7946, val=5.9732
Step 1700: loss = 6.0082
  [Val @ step 1700]: train=6.0082, val=5.9539
Step 1750: loss = 5.9702
  [Val @ step 1750]: train=5.9702, val=5.9263
Step 1800: loss = 6.0714
  [Val @ step 1800]: train=6.0714, val=5.9618
Step 1850: loss = 5.9556
  [Val @ step 1850]: train=5.9556, val=5.9205
Step 1900: loss = 5.9000
  [Val @ step 1900]: train=5.9000, val=5.9488
Step 1950: loss = 5.8715
  [Val @ step 1950]: train=5.8715, val=5.9194
Step 2000: loss = 5.7789
  [Val @ step 2000]: train=5.7789, val=5.9390
Step 2050: loss = 6.0076
  [Val @ step 2050]: train=6.0076, val=5.8802
Step 2100: loss = 5.9685
  [Val @ step 2100]: train=5.9685, val=5.9307
Step 2150: loss = 5.8649
  [Val @ step 2150]: train=5.8649, val=5.9004
Step 2200: loss = 5.8880
  [Val @ step 2200]: train=5.8880, val=5.8861
Step 2250: loss = 5.9909
  [Val @ step 2250]: train=5.9909, val=5.9451
Step 2300: loss = 5.6914
  [Val @ step 2300]: train=5.6914, val=5.8762
Step 2350: loss = 5.9554
  [Val @ step 2350]: train=5.9554, val=5.8933
Step 2400: loss = 5.8700
  [Val @ step 2400]: train=5.8700, val=5.8887
Step 2450: loss = 6.0254
  [Val @ step 2450]: train=6.0254, val=5.9064
Step 2500: loss = 5.7347
  [Val @ step 2500]: train=5.7347, val=5.8624
Step 2550: loss = 5.9363
  [Val @ step 2550]: train=5.9363, val=5.8259
Step 2600: loss = 5.8587
  [Val @ step 2600]: train=5.8587, val=5.8469
Step 2650: loss = 5.7602
  [Val @ step 2650]: train=5.7602, val=5.8917
Step 2700: loss = 5.8176
  [Val @ step 2700]: train=5.8176, val=5.8437
Step 2750: loss = 5.8579
  [Val @ step 2750]: train=5.8579, val=5.8469
Step 2800: loss = 5.8605
  [Val @ step 2800]: train=5.8605, val=5.8491
Step 2850: loss = 5.8522
  [Val @ step 2850]: train=5.8522, val=5.8483
Step 2900: loss = 5.8011
  [Val @ step 2900]: train=5.8011, val=5.8337
Step 2950: loss = 5.8194
  [Val @ step 2950]: train=5.8194, val=5.8762
Step 3000: loss = 5.7299
  [Val @ step 3000]: train=5.7299, val=5.8092
Step 3050: loss = 5.9455
  [Val @ step 3050]: train=5.9455, val=5.8473
Step 3100: loss = 5.7517
  [Val @ step 3100]: train=5.7517, val=5.8086
Step 3150: loss = 5.7778
  [Val @ step 3150]: train=5.7778, val=5.7783
Step 3200: loss = 5.9226
  [Val @ step 3200]: train=5.9226, val=5.8349
Step 3250: loss = 5.7497
  [Val @ step 3250]: train=5.7497, val=5.8299
Step 3300: loss = 5.7828
  [Val @ step 3300]: train=5.7828, val=5.7990
Step 3350: loss = 5.7792
  [Val @ step 3350]: train=5.7792, val=5.8512
Step 3400: loss = 5.8184
  [Val @ step 3400]: train=5.8184, val=5.8524
Step 3450: loss = 5.6899
  [Val @ step 3450]: train=5.6899, val=5.8568
Step 3500: loss = 5.9685
  [Val @ step 3500]: train=5.9685, val=5.7988
Step 3550: loss = 5.6936
  [Val @ step 3550]: train=5.6936, val=5.7642
Step 3600: loss = 5.8622
  [Val @ step 3600]: train=5.8622, val=5.8240
Step 3650: loss = 5.6400
  [Val @ step 3650]: train=5.6400, val=5.7967
Step 3700: loss = 5.6542
  [Val @ step 3700]: train=5.6542, val=5.7447
Step 3750: loss = 5.7541
  [Val @ step 3750]: train=5.7541, val=5.7516
Step 3800: loss = 5.7248
  [Val @ step 3800]: train=5.7248, val=5.7564
Step 3850: loss = 5.8400
  [Val @ step 3850]: train=5.8400, val=5.7773
Step 3900: loss = 5.9179
  [Val @ step 3900]: train=5.9179, val=5.7867
Step 3950: loss = 5.8383
  [Val @ step 3950]: train=5.8383, val=5.8051
Step 4000: loss = 5.7839
  [Val @ step 4000]: train=5.7839, val=5.7661
Step 4050: loss = 5.8875
  [Val @ step 4050]: train=5.8875, val=5.7578
Step 4100: loss = 5.8060
  [Val @ step 4100]: train=5.8060, val=5.7802
Step 4150: loss = 5.7765
  [Val @ step 4150]: train=5.7765, val=5.7697
Step 4200: loss = 5.7573
  [Val @ step 4200]: train=5.7573, val=5.7714
Step 4250: loss = 5.7624
  [Val @ step 4250]: train=5.7624, val=5.7432
Step 4300: loss = 5.6867
  [Val @ step 4300]: train=5.6867, val=5.7227
Step 4350: loss = 5.8792
  [Val @ step 4350]: train=5.8792, val=5.7831
Step 4400: loss = 5.9479
  [Val @ step 4400]: train=5.9479, val=5.7599
Step 4450: loss = 5.7492
  [Val @ step 4450]: train=5.7492, val=5.7653
Step 4500: loss = 5.8248
  [Val @ step 4500]: train=5.8248, val=5.7468
Step 4550: loss = 5.6049
  [Val @ step 4550]: train=5.6049, val=5.7470
Step 4600: loss = 5.7586
  [Val @ step 4600]: train=5.7586, val=5.7614
Step 4650: loss = 5.6644
  [Val @ step 4650]: train=5.6644, val=5.7456
Step 4700: loss = 5.6518
  [Val @ step 4700]: train=5.6518, val=5.7614
Step 4750: loss = 5.6743
  [Val @ step 4750]: train=5.6743, val=5.7597
Step 4800: loss = 5.6890
  [Val @ step 4800]: train=5.6890, val=5.7339
Step 4850: loss = 5.5906
  [Val @ step 4850]: train=5.5906, val=5.7523
Step 4900: loss = 5.8667
  [Val @ step 4900]: train=5.8667, val=5.7500
Step 4950: loss = 5.7915
  [Val @ step 4950]: train=5.7915, val=5.7474
--------------------------------------------------------------------------------
Computing final validation loss...
Validation loss: 5.7084

Results saved to time_to_target_stage1_20251225_225546.jsonl
Training time: 664.9s
Throughput: 30,803 tokens/sec
Projections: 715/5000
nGPT Architectural Experiment: A1_po
================================================================================
Model: 11 layers x 768 dim (matching modded-nanogpt)
Hypothesis: Baseline
================================================================================

Loading FineWeb data (subset for fast experiments)...
  Loading data/fineweb10B/fineweb_train_000001.bin...
  Loading data/fineweb10B/fineweb_train_000002.bin...
  Loading data/fineweb10B/fineweb_train_000003.bin...
  Loading data/fineweb10B/fineweb_train_000004.bin...
  Vocab size: 50257
  Train tokens: 400,001,536
  Val tokens: 100,000,384

Model config (modded-nanogpt size):
  vocab_size: 50257
  n_layer: 11
  n_embd: 768
  n_head: 6
  block_size: 128
Using GatedBlock (H3) with gate_init=0.0

Total parameters: 155,166,743

Optimizer: SimpleMuon (lr=0.003, momentum=0.95, geodesic=baseline, orthog=polar_express)
  2D weight params (Muon): 77,856,768
  1D params (Adam): 77,309,975

Starting training (5000 steps)...
--------------------------------------------------------------------------------
Step    0: loss = 10.9621
Step   50: loss = 7.2238
  [Val @ step 50]: train=7.2238, val=7.3576
Step  100: loss = 6.9453
  [Val @ step 100]: train=6.9453, val=6.8695
Step  150: loss = 6.6814
  [Val @ step 150]: train=6.6814, val=6.7269
Step  200: loss = 6.5604
  [Val @ step 200]: train=6.5604, val=6.6402
Step  250: loss = 6.6848
  [Val @ step 250]: train=6.6848, val=6.5492
Step  300: loss = 6.4144
  [Val @ step 300]: train=6.4144, val=6.4330
Step  350: loss = 6.3991
  [Val @ step 350]: train=6.3991, val=6.3549
Step  400: loss = 6.3119
  [Val @ step 400]: train=6.3119, val=6.3790
Step  450: loss = 6.4848
  [Val @ step 450]: train=6.4848, val=6.3073
Step  500: loss = 6.3867
  [Val @ step 500]: train=6.3867, val=6.2809
Step  550: loss = 6.0749
  [Val @ step 550]: train=6.0749, val=6.2315
Step  600: loss = 6.1053
  [Val @ step 600]: train=6.1053, val=6.2089
Step  650: loss = 6.1739
  [Val @ step 650]: train=6.1739, val=6.1846
Step  700: loss = 6.0487
  [Val @ step 700]: train=6.0487, val=6.1905
Step  750: loss = 6.0150
  [Val @ step 750]: train=6.0150, val=6.1546
Step  800: loss = 6.2708
  [Val @ step 800]: train=6.2708, val=6.1159
Step  850: loss = 5.9957
  [Val @ step 850]: train=5.9957, val=6.1189
Step  900: loss = 6.2992
  [Val @ step 900]: train=6.2992, val=6.0436
Step  950: loss = 6.0406
  [Val @ step 950]: train=6.0406, val=6.0757
Step 1000: loss = 5.8642
  [Val @ step 1000]: train=5.8642, val=6.0687
Step 1050: loss = 6.2463
  [Val @ step 1050]: train=6.2463, val=6.0323
Step 1100: loss = 6.0687
  [Val @ step 1100]: train=6.0687, val=6.0625
Step 1150: loss = 5.9348
  [Val @ step 1150]: train=5.9348, val=6.0281
Step 1200: loss = 5.9334
  [Val @ step 1200]: train=5.9334, val=6.0086
Step 1250: loss = 5.9446
  [Val @ step 1250]: train=5.9446, val=6.0011
Step 1300: loss = 5.8136
  [Val @ step 1300]: train=5.8136, val=5.9649
Step 1350: loss = 5.9698
  [Val @ step 1350]: train=5.9698, val=5.9329
Step 1400: loss = 5.8305
  [Val @ step 1400]: train=5.8305, val=5.9912
Step 1450: loss = 5.8686
  [Val @ step 1450]: train=5.8686, val=5.9406
Step 1500: loss = 5.8013
  [Val @ step 1500]: train=5.8013, val=5.9201
Step 1550: loss = 6.0061
  [Val @ step 1550]: train=6.0061, val=5.9067
Step 1600: loss = 5.9711
  [Val @ step 1600]: train=5.9711, val=5.9223
Step 1650: loss = 5.8281
  [Val @ step 1650]: train=5.8281, val=5.8945
Step 1700: loss = 5.9558
  [Val @ step 1700]: train=5.9558, val=5.8828
Step 1750: loss = 5.7633
  [Val @ step 1750]: train=5.7633, val=5.9064
Step 1800: loss = 5.7741
  [Val @ step 1800]: train=5.7741, val=5.9021
Step 1850: loss = 5.7623
  [Val @ step 1850]: train=5.7623, val=5.8870
Step 1900: loss = 6.0278
  [Val @ step 1900]: train=6.0278, val=5.8712
Step 1950: loss = 5.8034
  [Val @ step 1950]: train=5.8034, val=5.9179
Step 2000: loss = 5.7624
  [Val @ step 2000]: train=5.7624, val=5.8674
Step 2050: loss = 5.8410
  [Val @ step 2050]: train=5.8410, val=5.8975
Step 2100: loss = 5.8767
  [Val @ step 2100]: train=5.8767, val=5.8346
Step 2150: loss = 5.7756
  [Val @ step 2150]: train=5.7756, val=5.8103
Step 2200: loss = 5.8912
  [Val @ step 2200]: train=5.8912, val=5.8370
Step 2250: loss = 5.5127
  [Val @ step 2250]: train=5.5127, val=5.8129
Step 2300: loss = 5.8470
  [Val @ step 2300]: train=5.8470, val=5.7998
Step 2350: loss = 5.9078
  [Val @ step 2350]: train=5.9078, val=5.8396
Step 2400: loss = 5.7338
  [Val @ step 2400]: train=5.7338, val=5.7932
Step 2450: loss = 5.8991
  [Val @ step 2450]: train=5.8991, val=5.8151
Step 2500: loss = 5.7472
  [Val @ step 2500]: train=5.7472, val=5.7833
Step 2550: loss = 5.9584
  [Val @ step 2550]: train=5.9584, val=5.8376
Step 2600: loss = 5.7326
  [Val @ step 2600]: train=5.7326, val=5.7790
Step 2650: loss = 5.7615
  [Val @ step 2650]: train=5.7615, val=5.7507
Step 2700: loss = 5.8407
  [Val @ step 2700]: train=5.8407, val=5.7718
Step 2750: loss = 5.7644
  [Val @ step 2750]: train=5.7644, val=5.7930
Step 2800: loss = 5.7609
  [Val @ step 2800]: train=5.7609, val=5.8180
Step 2850: loss = 5.8079
  [Val @ step 2850]: train=5.8079, val=5.7544
Step 2900: loss = 5.7562
  [Val @ step 2900]: train=5.7562, val=5.7823
Step 2950: loss = 5.7344
  [Val @ step 2950]: train=5.7344, val=5.7459
Step 3000: loss = 5.8429
  [Val @ step 3000]: train=5.8429, val=5.7667
Step 3050: loss = 5.7063
  [Val @ step 3050]: train=5.7063, val=5.7792
Step 3100: loss = 5.7674
  [Val @ step 3100]: train=5.7674, val=5.7431
Step 3150: loss = 5.6457
  [Val @ step 3150]: train=5.6457, val=5.7758
Step 3200: loss = 5.9466
  [Val @ step 3200]: train=5.9466, val=5.8159
Step 3250: loss = 5.7399
  [Val @ step 3250]: train=5.7399, val=5.7517
Step 3300: loss = 5.6506
  [Val @ step 3300]: train=5.6506, val=5.7569
Step 3350: loss = 5.6657
  [Val @ step 3350]: train=5.6657, val=5.7036
Step 3400: loss = 5.7601
  [Val @ step 3400]: train=5.7601, val=5.7298
Step 3450: loss = 5.4801
  [Val @ step 3450]: train=5.4801, val=5.8078
Step 3500: loss = 5.6529
  [Val @ step 3500]: train=5.6529, val=5.7318
Step 3550: loss = 5.7032
  [Val @ step 3550]: train=5.7032, val=5.7497
Step 3600: loss = 5.7859
  [Val @ step 3600]: train=5.7859, val=5.7236
Step 3650: loss = 5.6795
  [Val @ step 3650]: train=5.6795, val=5.6972
Step 3700: loss = 5.6570
  [Val @ step 3700]: train=5.6570, val=5.7494
Step 3750: loss = 5.6763
  [Val @ step 3750]: train=5.6763, val=5.7037
Step 3800: loss = 5.7232
  [Val @ step 3800]: train=5.7232, val=5.7170
Step 3850: loss = 5.5879
  [Val @ step 3850]: train=5.5879, val=5.6931
Step 3900: loss = 5.6519
  [Val @ step 3900]: train=5.6519, val=5.7064
Step 3950: loss = 5.7918
  [Val @ step 3950]: train=5.7918, val=5.7058
Step 4000: loss = 5.5711
  [Val @ step 4000]: train=5.5711, val=5.7679
Step 4050: loss = 5.6872
  [Val @ step 4050]: train=5.6872, val=5.7286
Step 4100: loss = 5.7390
  [Val @ step 4100]: train=5.7390, val=5.7136
Step 4150: loss = 5.7496
  [Val @ step 4150]: train=5.7496, val=5.7015
Step 4200: loss = 5.7119
  [Val @ step 4200]: train=5.7119, val=5.6820
Step 4250: loss = 5.6841
  [Val @ step 4250]: train=5.6841, val=5.6919
Step 4300: loss = 5.7224
  [Val @ step 4300]: train=5.7224, val=5.6954
Step 4350: loss = 5.5808
  [Val @ step 4350]: train=5.5808, val=5.6870
Step 4400: loss = 5.7090
  [Val @ step 4400]: train=5.7090, val=5.6912
Step 4450: loss = 5.7364
  [Val @ step 4450]: train=5.7364, val=5.6881
Step 4500: loss = 5.6544
  [Val @ step 4500]: train=5.6544, val=5.6679
Step 4550: loss = 5.5420
  [Val @ step 4550]: train=5.5420, val=5.6666
Step 4600: loss = 5.7990
  [Val @ step 4600]: train=5.7990, val=5.6989
Step 4650: loss = 5.3319
  [Val @ step 4650]: train=5.3319, val=5.7356
Step 4700: loss = 5.4489
  [Val @ step 4700]: train=5.4489, val=5.6920
Step 4750: loss = 5.6853
  [Val @ step 4750]: train=5.6853, val=5.6834
Step 4800: loss = 5.7225
  [Val @ step 4800]: train=5.7225, val=5.6857
Step 4850: loss = 5.7154
  [Val @ step 4850]: train=5.7154, val=5.7065
Step 4900: loss = 5.5352
  [Val @ step 4900]: train=5.5352, val=5.7238
Step 4950: loss = 5.7268
  [Val @ step 4950]: train=5.7268, val=5.6674
--------------------------------------------------------------------------------
Computing final validation loss...
Validation loss: 5.7006

Results saved to time_to_target_stage1_20251225_225546.jsonl
Training time: 892.8s
Throughput: 22,939 tokens/sec
Projections: 715/5000
nGPT Architectural Experiment: A2_ne
================================================================================
Model: 11 layers x 768 dim (matching modded-nanogpt)
Hypothesis: Baseline
================================================================================

Loading FineWeb data (subset for fast experiments)...
  Loading data/fineweb10B/fineweb_train_000001.bin...
  Loading data/fineweb10B/fineweb_train_000002.bin...
  Loading data/fineweb10B/fineweb_train_000003.bin...
  Loading data/fineweb10B/fineweb_train_000004.bin...
  Vocab size: 50257
  Train tokens: 400,001,536
  Val tokens: 100,000,384

Model config (modded-nanogpt size):
  vocab_size: 50257
  n_layer: 11
  n_embd: 768
  n_head: 6
  block_size: 128
Using GatedBlock (H3) with gate_init=0.0

Total parameters: 155,166,743

Optimizer: SimpleMuon (lr=0.01, momentum=0.95, geodesic=baseline, orthog=newton_schulz)
  2D weight params (Muon): 77,856,768
  1D params (Adam): 77,309,975

Starting training (5000 steps)...
--------------------------------------------------------------------------------
Step    0: loss = 10.9549
Step   50: loss = 7.1441
  [Val @ step 50]: train=7.1441, val=7.0236
Step  100: loss = 6.6864
  [Val @ step 100]: train=6.6864, val=6.7564
Step  150: loss = 6.7024
  [Val @ step 150]: train=6.7024, val=6.6795
Step  200: loss = 6.5927
  [Val @ step 200]: train=6.5927, val=6.5908
Step  250: loss = 6.6638
  [Val @ step 250]: train=6.6638, val=6.5453
Step  300: loss = 6.6924
  [Val @ step 300]: train=6.6924, val=6.5211
Step  350: loss = 6.5731
  [Val @ step 350]: train=6.5731, val=6.4968
Step  400: loss = 6.4102
  [Val @ step 400]: train=6.4102, val=6.4539
Step  450: loss = 6.2817
  [Val @ step 450]: train=6.2817, val=6.4405
Step  500: loss = 6.5329
  [Val @ step 500]: train=6.5329, val=6.4147
Step  550: loss = 6.0852
  [Val @ step 550]: train=6.0852, val=6.3638
Step  600: loss = 6.3931
  [Val @ step 600]: train=6.3931, val=6.3761
Step  650: loss = 6.2447
  [Val @ step 650]: train=6.2447, val=6.3627
Step  700: loss = 6.1503
  [Val @ step 700]: train=6.1503, val=6.3343
Step  750: loss = 6.4787
  [Val @ step 750]: train=6.4787, val=6.3117
Step  800: loss = 6.2087
  [Val @ step 800]: train=6.2087, val=6.3071
Step  850: loss = 6.2462
  [Val @ step 850]: train=6.2462, val=6.3029
Step  900: loss = 6.3694
  [Val @ step 900]: train=6.3694, val=6.2782
Step  950: loss = 6.1404
  [Val @ step 950]: train=6.1404, val=6.2697
Step 1000: loss = 6.1681
  [Val @ step 1000]: train=6.1681, val=6.2123
Step 1050: loss = 6.1932
  [Val @ step 1050]: train=6.1932, val=6.2486
Step 1100: loss = 6.1726
  [Val @ step 1100]: train=6.1726, val=6.2428
Step 1150: loss = 6.2078
  [Val @ step 1150]: train=6.2078, val=6.2295
Step 1200: loss = 6.2042
  [Val @ step 1200]: train=6.2042, val=6.2219
Step 1250: loss = 6.3469
  [Val @ step 1250]: train=6.3469, val=6.1810
Step 1300: loss = 6.1825
  [Val @ step 1300]: train=6.1825, val=6.2137
Step 1350: loss = 6.3058
  [Val @ step 1350]: train=6.3058, val=6.1903
Step 1400: loss = 5.9737
  [Val @ step 1400]: train=5.9737, val=6.2267
Step 1450: loss = 5.9783
  [Val @ step 1450]: train=5.9783, val=6.1925
Step 1500: loss = 6.0920
  [Val @ step 1500]: train=6.0920, val=6.1984
Step 1550: loss = 6.0733
  [Val @ step 1550]: train=6.0733, val=6.2070
Step 1600: loss = 6.1373
  [Val @ step 1600]: train=6.1373, val=6.1994
Step 1650: loss = 6.2976
  [Val @ step 1650]: train=6.2976, val=6.2372
Step 1700: loss = 6.1510
  [Val @ step 1700]: train=6.1510, val=6.1883
Step 1750: loss = 6.1679
  [Val @ step 1750]: train=6.1679, val=6.1535
Step 1800: loss = 6.0129
  [Val @ step 1800]: train=6.0129, val=6.1713
Step 1850: loss = 6.2722
  [Val @ step 1850]: train=6.2722, val=6.2083
Step 1900: loss = 6.1054
  [Val @ step 1900]: train=6.1054, val=6.1883
Step 1950: loss = 6.1706
  [Val @ step 1950]: train=6.1706, val=6.0971
Step 2000: loss = 6.0251
  [Val @ step 2000]: train=6.0251, val=6.1413
Step 2050: loss = 6.0991
  [Val @ step 2050]: train=6.0991, val=6.1881
Step 2100: loss = 6.1063
  [Val @ step 2100]: train=6.1063, val=6.1805
Step 2150: loss = 6.1789
  [Val @ step 2150]: train=6.1789, val=6.1521
Step 2200: loss = 6.2136
  [Val @ step 2200]: train=6.2136, val=6.1739
Step 2250: loss = 5.9668
  [Val @ step 2250]: train=5.9668, val=6.1378
Step 2300: loss = 6.1181
  [Val @ step 2300]: train=6.1181, val=6.1392
Step 2350: loss = 6.3485
  [Val @ step 2350]: train=6.3485, val=6.1862
Step 2400: loss = 6.2075
  [Val @ step 2400]: train=6.2075, val=6.1775
Step 2450: loss = 6.2041
  [Val @ step 2450]: train=6.2041, val=6.1597
Step 2500: loss = 5.9665
  [Val @ step 2500]: train=5.9665, val=6.1448
Step 2550: loss = 6.1099
  [Val @ step 2550]: train=6.1099, val=6.1714
Step 2600: loss = 6.2064
  [Val @ step 2600]: train=6.2064, val=6.1111
Step 2650: loss = 6.1344
  [Val @ step 2650]: train=6.1344, val=6.1716
Step 2700: loss = 5.9727
  [Val @ step 2700]: train=5.9727, val=6.1181
Step 2750: loss = 6.2004
  [Val @ step 2750]: train=6.2004, val=6.1519
Step 2800: loss = 6.2185
  [Val @ step 2800]: train=6.2185, val=6.1494
Step 2850: loss = 6.0829
  [Val @ step 2850]: train=6.0829, val=6.1479
Step 2900: loss = 6.1421
  [Val @ step 2900]: train=6.1421, val=6.1392
Step 2950: loss = 6.0590
  [Val @ step 2950]: train=6.0590, val=6.1254
Step 3000: loss = 6.1442
  [Val @ step 3000]: train=6.1442, val=6.1523
Step 3050: loss = 6.0407
  [Val @ step 3050]: train=6.0407, val=6.1189
Step 3100: loss = 6.0625
  [Val @ step 3100]: train=6.0625, val=6.1474
Step 3150: loss = 6.0103
  [Val @ step 3150]: train=6.0103, val=6.1256
Step 3200: loss = 6.0779
  [Val @ step 3200]: train=6.0779, val=6.1141
Step 3250: loss = 6.1191
  [Val @ step 3250]: train=6.1191, val=6.0992
Step 3300: loss = 6.2135
  [Val @ step 3300]: train=6.2135, val=6.1072
Step 3350: loss = 6.0067
  [Val @ step 3350]: train=6.0067, val=6.1596
Step 3400: loss = 6.1793
  [Val @ step 3400]: train=6.1793, val=6.1392
Step 3450: loss = 5.9283
  [Val @ step 3450]: train=5.9283, val=6.1469
Step 3500: loss = 6.1998
  [Val @ step 3500]: train=6.1998, val=6.1422
Step 3550: loss = 6.2788
  [Val @ step 3550]: train=6.2788, val=6.1407
Step 3600: loss = 6.0302
  [Val @ step 3600]: train=6.0302, val=6.1545
Step 3650: loss = 6.2206
  [Val @ step 3650]: train=6.2206, val=6.1093
Step 3700: loss = 6.1124
  [Val @ step 3700]: train=6.1124, val=6.1064
Step 3750: loss = 5.9526
  [Val @ step 3750]: train=5.9526, val=6.1292
Step 3800: loss = 6.0437
  [Val @ step 3800]: train=6.0437, val=6.1106
Step 3850: loss = 6.3592
  [Val @ step 3850]: train=6.3592, val=6.1384
Step 3900: loss = 6.2318
  [Val @ step 3900]: train=6.2318, val=6.1014
Step 3950: loss = 6.0570
  [Val @ step 3950]: train=6.0570, val=6.1332
Step 4000: loss = 5.9407
  [Val @ step 4000]: train=5.9407, val=6.1588
Step 4050: loss = 6.2106
  [Val @ step 4050]: train=6.2106, val=6.1253
Step 4100: loss = 6.0811
  [Val @ step 4100]: train=6.0811, val=6.0990
Step 4150: loss = 6.1707
  [Val @ step 4150]: train=6.1707, val=6.1227
Step 4200: loss = 6.1404
  [Val @ step 4200]: train=6.1404, val=6.1034
Step 4250: loss = 6.1829
  [Val @ step 4250]: train=6.1829, val=6.1019
Step 4300: loss = 6.0715
  [Val @ step 4300]: train=6.0715, val=6.0921
Step 4350: loss = 6.1517
  [Val @ step 4350]: train=6.1517, val=6.0857
Step 4400: loss = 6.0916
  [Val @ step 4400]: train=6.0916, val=6.0902
Step 4450: loss = 6.2100
  [Val @ step 4450]: train=6.2100, val=6.1138
Step 4500: loss = 6.0582
  [Val @ step 4500]: train=6.0582, val=6.0880
Step 4550: loss = 6.1519
  [Val @ step 4550]: train=6.1519, val=6.0937
Step 4600: loss = 6.0493
  [Val @ step 4600]: train=6.0493, val=6.0958
Step 4650: loss = 6.0054
  [Val @ step 4650]: train=6.0054, val=6.1268
Step 4700: loss = 6.0494
  [Val @ step 4700]: train=6.0494, val=6.0788
Step 4750: loss = 6.1793
  [Val @ step 4750]: train=6.1793, val=6.1255
Step 4800: loss = 6.1288
  [Val @ step 4800]: train=6.1288, val=6.0994
Step 4850: loss = 6.1549
  [Val @ step 4850]: train=6.1549, val=6.1159
Step 4900: loss = 6.0206
  [Val @ step 4900]: train=6.0206, val=6.0677
Step 4950: loss = 6.1200
  [Val @ step 4950]: train=6.1200, val=6.1249
--------------------------------------------------------------------------------
Computing final validation loss...
Validation loss: 6.1139

Results saved to time_to_target_stage1_20251225_225546.jsonl
Training time: 918.1s
Throughput: 22,307 tokens/sec
Projections: 500/5000
nGPT Architectural Experiment: A2_po
================================================================================
Model: 11 layers x 768 dim (matching modded-nanogpt)
Hypothesis: Baseline
================================================================================

Loading FineWeb data (subset for fast experiments)...
  Loading data/fineweb10B/fineweb_train_000001.bin...
  Loading data/fineweb10B/fineweb_train_000002.bin...
  Loading data/fineweb10B/fineweb_train_000003.bin...
  Loading data/fineweb10B/fineweb_train_000004.bin...
  Vocab size: 50257
  Train tokens: 400,001,536
  Val tokens: 100,000,384

Model config (modded-nanogpt size):
  vocab_size: 50257
  n_layer: 11
  n_embd: 768
  n_head: 6
  block_size: 128
Using GatedBlock (H3) with gate_init=0.0

Total parameters: 155,166,743

Optimizer: SimpleMuon (lr=0.01, momentum=0.95, geodesic=baseline, orthog=polar_express)
  2D weight params (Muon): 77,856,768
  1D params (Adam): 77,309,975

Starting training (5000 steps)...
--------------------------------------------------------------------------------
Step    0: loss = 10.9747
Step   50: loss = 7.2527
  [Val @ step 50]: train=7.2527, val=6.9650
Step  100: loss = 6.6333
  [Val @ step 100]: train=6.6333, val=6.7276
Step  150: loss = 6.7215
  [Val @ step 150]: train=6.7215, val=6.6295
Step  200: loss = 6.3565
  [Val @ step 200]: train=6.3565, val=6.6621
Step  250: loss = 6.6074
  [Val @ step 250]: train=6.6074, val=6.5714
Step  300: loss = 6.2432
  [Val @ step 300]: train=6.2432, val=6.4860
Step  350: loss = 6.6112
  [Val @ step 350]: train=6.6112, val=6.4396
Step  400: loss = 6.3973
  [Val @ step 400]: train=6.3973, val=6.3909
Step  450: loss = 6.4323
  [Val @ step 450]: train=6.4323, val=6.3616
Step  500: loss = 6.3296
  [Val @ step 500]: train=6.3296, val=6.3826
Step  550: loss = 6.3090
  [Val @ step 550]: train=6.3090, val=6.3242
Step  600: loss = 6.1066
  [Val @ step 600]: train=6.1066, val=6.3056
Step  650: loss = 6.3387
  [Val @ step 650]: train=6.3387, val=6.2955
Step  700: loss = 6.3885
  [Val @ step 700]: train=6.3885, val=6.2929
Step  750: loss = 6.3116
  [Val @ step 750]: train=6.3116, val=6.2648
Step  800: loss = 6.3406
  [Val @ step 800]: train=6.3406, val=6.3013
Step  850: loss = 6.2877
  [Val @ step 850]: train=6.2877, val=6.2339
Step  900: loss = 6.4504
  [Val @ step 900]: train=6.4504, val=6.2552
Step  950: loss = 6.0864
  [Val @ step 950]: train=6.0864, val=6.2156
Step 1000: loss = 6.2924
  [Val @ step 1000]: train=6.2924, val=6.1971
Step 1050: loss = 6.1537
  [Val @ step 1050]: train=6.1537, val=6.2280
Step 1100: loss = 6.2708
  [Val @ step 1100]: train=6.2708, val=6.2446
Step 1150: loss = 6.4615
  [Val @ step 1150]: train=6.4615, val=6.1735
Step 1200: loss = 6.1328
  [Val @ step 1200]: train=6.1328, val=6.2686
Step 1250: loss = 6.1665
  [Val @ step 1250]: train=6.1665, val=6.2228
Step 1300: loss = 6.3925
  [Val @ step 1300]: train=6.3925, val=6.2120
Step 1350: loss = 6.0815
  [Val @ step 1350]: train=6.0815, val=6.1344
Step 1400: loss = 5.9837
  [Val @ step 1400]: train=5.9837, val=6.1582
Step 1450: loss = 6.2102
  [Val @ step 1450]: train=6.2102, val=6.2068
Step 1500: loss = 6.1250
  [Val @ step 1500]: train=6.1250, val=6.1449
Step 1550: loss = 6.1403
  [Val @ step 1550]: train=6.1403, val=6.1332
Step 1600: loss = 6.0943
  [Val @ step 1600]: train=6.0943, val=6.1747
Step 1650: loss = 6.2591
  [Val @ step 1650]: train=6.2591, val=6.1828
Step 1700: loss = 6.0089
  [Val @ step 1700]: train=6.0089, val=6.1602
Step 1750: loss = 6.2335
  [Val @ step 1750]: train=6.2335, val=6.1503
Step 1800: loss = 6.0604
  [Val @ step 1800]: train=6.0604, val=6.2069
Step 1850: loss = 5.9709
  [Val @ step 1850]: train=5.9709, val=6.1015
Step 1900: loss = 6.0016
  [Val @ step 1900]: train=6.0016, val=6.1323
Step 1950: loss = 6.2071
  [Val @ step 1950]: train=6.2071, val=6.1723
Step 2000: loss = 6.0180
  [Val @ step 2000]: train=6.0180, val=6.1338
Step 2050: loss = 6.1161
  [Val @ step 2050]: train=6.1161, val=6.1315
Step 2100: loss = 6.1136
  [Val @ step 2100]: train=6.1136, val=6.1135
Step 2150: loss = 6.2396
  [Val @ step 2150]: train=6.2396, val=6.1315
Step 2200: loss = 6.1533
  [Val @ step 2200]: train=6.1533, val=6.0686
Step 2250: loss = 5.9448
  [Val @ step 2250]: train=5.9448, val=6.1319
Step 2300: loss = 5.9823
  [Val @ step 2300]: train=5.9823, val=6.1148
Step 2350: loss = 6.2036
  [Val @ step 2350]: train=6.2036, val=6.1142
Step 2400: loss = 6.0784
  [Val @ step 2400]: train=6.0784, val=6.0745
Step 2450: loss = 5.9851
  [Val @ step 2450]: train=5.9851, val=6.0729
Step 2500: loss = 6.0004
  [Val @ step 2500]: train=6.0004, val=6.1213
Step 2550: loss = 5.9942
  [Val @ step 2550]: train=5.9942, val=6.1073
Step 2600: loss = 6.0024
  [Val @ step 2600]: train=6.0024, val=6.0828
Step 2650: loss = 6.1835
  [Val @ step 2650]: train=6.1835, val=6.1203
Step 2700: loss = 6.0384
  [Val @ step 2700]: train=6.0384, val=6.0822
Step 2750: loss = 6.3457
  [Val @ step 2750]: train=6.3457, val=6.1186
Step 2800: loss = 6.1298
  [Val @ step 2800]: train=6.1298, val=6.0787
Step 2850: loss = 6.0601
  [Val @ step 2850]: train=6.0601, val=6.0756
Step 2900: loss = 5.9621
  [Val @ step 2900]: train=5.9621, val=6.0813
Step 2950: loss = 5.9457
  [Val @ step 2950]: train=5.9457, val=6.0857
Step 3000: loss = 5.9384
  [Val @ step 3000]: train=5.9384, val=6.0954
Step 3050: loss = 6.1710
  [Val @ step 3050]: train=6.1710, val=6.0157
Step 3100: loss = 5.9317
  [Val @ step 3100]: train=5.9317, val=6.0892
Step 3150: loss = 6.0351
  [Val @ step 3150]: train=6.0351, val=6.0946
Step 3200: loss = 6.0513
  [Val @ step 3200]: train=6.0513, val=6.0894
Step 3250: loss = 6.0599
  [Val @ step 3250]: train=6.0599, val=6.0702
Step 3300: loss = 6.0224
  [Val @ step 3300]: train=6.0224, val=6.0984
Step 3350: loss = 6.0583
  [Val @ step 3350]: train=6.0583, val=6.0670
Step 3400: loss = 6.0578
  [Val @ step 3400]: train=6.0578, val=6.0520
Step 3450: loss = 5.9890
  [Val @ step 3450]: train=5.9890, val=6.0749
Step 3500: loss = 6.1124
  [Val @ step 3500]: train=6.1124, val=6.0355
Step 3550: loss = 6.0767
  [Val @ step 3550]: train=6.0767, val=6.0906
Step 3600: loss = 6.1737
  [Val @ step 3600]: train=6.1737, val=6.0802
Step 3650: loss = 5.9918
  [Val @ step 3650]: train=5.9918, val=6.0391
Step 3700: loss = 6.0967
  [Val @ step 3700]: train=6.0967, val=6.0848
Step 3750: loss = 5.9783
  [Val @ step 3750]: train=5.9783, val=6.0792
Step 3800: loss = 6.0765
  [Val @ step 3800]: train=6.0765, val=6.0827
Step 3850: loss = 6.0937
  [Val @ step 3850]: train=6.0937, val=6.0404
Step 3900: loss = 5.9778
  [Val @ step 3900]: train=5.9778, val=6.0876
Step 3950: loss = 6.0572
  [Val @ step 3950]: train=6.0572, val=6.0616
Step 4000: loss = 6.0499
  [Val @ step 4000]: train=6.0499, val=6.0904
Step 4050: loss = 5.8251
  [Val @ step 4050]: train=5.8251, val=6.0577
Step 4100: loss = 6.1394
  [Val @ step 4100]: train=6.1394, val=6.0582
Step 4150: loss = 5.8155
  [Val @ step 4150]: train=5.8155, val=6.0625
Step 4200: loss = 6.0541
  [Val @ step 4200]: train=6.0541, val=6.0827
Step 4250: loss = 5.8933
  [Val @ step 4250]: train=5.8933, val=6.0316
Step 4300: loss = 6.0344
  [Val @ step 4300]: train=6.0344, val=6.0531
Step 4350: loss = 5.9856
  [Val @ step 4350]: train=5.9856, val=6.0860
Step 4400: loss = 5.9803
  [Val @ step 4400]: train=5.9803, val=6.0152
Step 4450: loss = 6.0064
  [Val @ step 4450]: train=6.0064, val=6.0660
Step 4500: loss = 6.1017
  [Val @ step 4500]: train=6.1017, val=6.0388
Step 4550: loss = 5.8852
  [Val @ step 4550]: train=5.8852, val=5.9872
Step 4600: loss = 6.0588
  [Val @ step 4600]: train=6.0588, val=6.0253
Step 4650: loss = 6.1113
  [Val @ step 4650]: train=6.1113, val=6.0544
Step 4700: loss = 5.9766
  [Val @ step 4700]: train=5.9766, val=6.0474
Step 4750: loss = 6.0218
  [Val @ step 4750]: train=6.0218, val=6.0107
Step 4800: loss = 6.0166
  [Val @ step 4800]: train=6.0166, val=6.0248
Step 4850: loss = 5.9858
  [Val @ step 4850]: train=5.9858, val=6.0587
Step 4900: loss = 5.8815
  [Val @ step 4900]: train=5.8815, val=6.0523
Step 4950: loss = 6.1104
  [Val @ step 4950]: train=6.1104, val=6.0484
--------------------------------------------------------------------------------
Computing final validation loss...
Validation loss: 6.0127

Results saved to time_to_target_stage1_20251225_225546.jsonl
Training time: 1232.7s
Throughput: 16,614 tokens/sec
Projections: 500/5000
nGPT Architectural Experiment: A3_ne
================================================================================
Model: 11 layers x 768 dim (matching modded-nanogpt)
Hypothesis: Baseline
================================================================================

Loading FineWeb data (subset for fast experiments)...
  Loading data/fineweb10B/fineweb_train_000001.bin...
  Loading data/fineweb10B/fineweb_train_000002.bin...
  Loading data/fineweb10B/fineweb_train_000003.bin...
  Loading data/fineweb10B/fineweb_train_000004.bin...
  Vocab size: 50257
  Train tokens: 400,001,536
  Val tokens: 100,000,384

Model config (modded-nanogpt size):
  vocab_size: 50257
  n_layer: 11
  n_embd: 768
  n_head: 6
  block_size: 128
Using GatedBlock (H3) with gate_init=0.0

Total parameters: 155,166,743

Optimizer: SimpleMuon (lr=0.003, momentum=0.95, geodesic=baseline, orthog=newton_schulz)
  2D weight params (Muon): 77,856,768
  1D params (Adam): 77,309,975

Starting training (5000 steps)...
--------------------------------------------------------------------------------
Step    0: loss = 10.9857
Step   50: loss = 7.3146
  [Val @ step 50]: train=7.3146, val=7.3658
Step  100: loss = 7.1803
  [Val @ step 100]: train=7.1803, val=6.9318
Step  150: loss = 6.8078
  [Val @ step 150]: train=6.8078, val=6.7173
Step  200: loss = 6.6724
  [Val @ step 200]: train=6.6724, val=6.6554
Step  250: loss = 6.6276
  [Val @ step 250]: train=6.6276, val=6.6073
Step  300: loss = 6.3775
  [Val @ step 300]: train=6.3775, val=6.5121
Step  350: loss = 6.3083
  [Val @ step 350]: train=6.3083, val=6.4939
Step  400: loss = 6.3212
  [Val @ step 400]: train=6.3212, val=6.4505
Step  450: loss = 6.4175
  [Val @ step 450]: train=6.4175, val=6.3749
Step  500: loss = 6.4657
  [Val @ step 500]: train=6.4657, val=6.4026
Step  550: loss = 6.1195
  [Val @ step 550]: train=6.1195, val=6.2994
Step  600: loss = 6.2202
  [Val @ step 600]: train=6.2202, val=6.3176
Step  650: loss = 6.3570
  [Val @ step 650]: train=6.3570, val=6.2264
Step  700: loss = 6.2462
  [Val @ step 700]: train=6.2462, val=6.2314
Step  750: loss = 6.1856
  [Val @ step 750]: train=6.1856, val=6.2492
Step  800: loss = 6.1539
  [Val @ step 800]: train=6.1539, val=6.1813
Step  850: loss = 6.2789
  [Val @ step 850]: train=6.2789, val=6.1381
Step  900: loss = 6.0238
  [Val @ step 900]: train=6.0238, val=6.1392
Step  950: loss = 6.0928
  [Val @ step 950]: train=6.0928, val=6.1645
Step 1000: loss = 6.0863
  [Val @ step 1000]: train=6.0863, val=6.1079
Step 1050: loss = 6.1115
  [Val @ step 1050]: train=6.1115, val=6.0551
Step 1100: loss = 5.9638
  [Val @ step 1100]: train=5.9638, val=6.0816
Step 1150: loss = 6.0313
  [Val @ step 1150]: train=6.0313, val=6.0912
Step 1200: loss = 6.1556
  [Val @ step 1200]: train=6.1556, val=6.0957
Step 1250: loss = 6.0608
  [Val @ step 1250]: train=6.0608, val=6.0688
Step 1300: loss = 5.9323
  [Val @ step 1300]: train=5.9323, val=5.9993
Step 1350: loss = 6.0207
  [Val @ step 1350]: train=6.0207, val=6.0375
Step 1400: loss = 6.0020
  [Val @ step 1400]: train=6.0020, val=6.0372
Step 1450: loss = 6.0057
  [Val @ step 1450]: train=6.0057, val=5.9965
Step 1500: loss = 5.8430
  [Val @ step 1500]: train=5.8430, val=6.0050
Step 1550: loss = 6.0623
  [Val @ step 1550]: train=6.0623, val=5.9901
Step 1600: loss = 5.8950
  [Val @ step 1600]: train=5.8950, val=5.9770
Step 1650: loss = 6.0294
  [Val @ step 1650]: train=6.0294, val=5.9766
Step 1700: loss = 5.8547
  [Val @ step 1700]: train=5.8547, val=5.9579
Step 1750: loss = 5.9888
  [Val @ step 1750]: train=5.9888, val=5.9531
Step 1800: loss = 6.0053
  [Val @ step 1800]: train=6.0053, val=5.9448
Step 1850: loss = 6.0301
  [Val @ step 1850]: train=6.0301, val=5.9499
Step 1900: loss = 5.9313
  [Val @ step 1900]: train=5.9313, val=5.9174
Step 1950: loss = 5.9651
  [Val @ step 1950]: train=5.9651, val=5.9540
Step 2000: loss = 6.0504
  [Val @ step 2000]: train=6.0504, val=5.9084
Step 2050: loss = 5.8737
  [Val @ step 2050]: train=5.8737, val=5.9142
Step 2100: loss = 5.9649
  [Val @ step 2100]: train=5.9649, val=5.9060
Step 2150: loss = 5.8581
  [Val @ step 2150]: train=5.8581, val=5.9245
Step 2200: loss = 5.8108
  [Val @ step 2200]: train=5.8108, val=5.8850
Step 2250: loss = 5.8320
  [Val @ step 2250]: train=5.8320, val=5.8812
Step 2300: loss = 5.5830
  [Val @ step 2300]: train=5.5830, val=5.8906
Step 2350: loss = 5.7463
  [Val @ step 2350]: train=5.7463, val=5.9124
Step 2400: loss = 6.0032
  [Val @ step 2400]: train=6.0032, val=5.8920
Step 2450: loss = 5.9429
  [Val @ step 2450]: train=5.9429, val=5.9008
Step 2500: loss = 5.9181
  [Val @ step 2500]: train=5.9181, val=5.8802
Step 2550: loss = 5.8351
  [Val @ step 2550]: train=5.8351, val=5.9177
Step 2600: loss = 5.8242
  [Val @ step 2600]: train=5.8242, val=5.8588
Step 2650: loss = 5.8608
  [Val @ step 2650]: train=5.8608, val=5.8819
Step 2700: loss = 5.9036
  [Val @ step 2700]: train=5.9036, val=5.8847
Step 2750: loss = 5.7643
  [Val @ step 2750]: train=5.7643, val=5.8491
Step 2800: loss = 5.9146
  [Val @ step 2800]: train=5.9146, val=5.8466
Step 2850: loss = 5.7919
  [Val @ step 2850]: train=5.7919, val=5.8832
Step 2900: loss = 5.9794
  [Val @ step 2900]: train=5.9794, val=5.8718
Step 2950: loss = 5.7593
  [Val @ step 2950]: train=5.7593, val=5.8432
Step 3000: loss = 5.9119
  [Val @ step 3000]: train=5.9119, val=5.8095
Step 3050: loss = 5.7812
  [Val @ step 3050]: train=5.7812, val=5.8241
Step 3100: loss = 5.9231
  [Val @ step 3100]: train=5.9231, val=5.8365
Step 3150: loss = 5.7764
  [Val @ step 3150]: train=5.7764, val=5.7882
Step 3200: loss = 5.8966
  [Val @ step 3200]: train=5.8966, val=5.8404
Step 3250: loss = 5.8801
  [Val @ step 3250]: train=5.8801, val=5.7855
Step 3300: loss = 5.7725
  [Val @ step 3300]: train=5.7725, val=5.7971
Step 3350: loss = 5.7912
  [Val @ step 3350]: train=5.7912, val=5.8292
Step 3400: loss = 5.9353
  [Val @ step 3400]: train=5.9353, val=5.8100
Step 3450: loss = 5.7278
  [Val @ step 3450]: train=5.7278, val=5.8109
Step 3500: loss = 5.7053
  [Val @ step 3500]: train=5.7053, val=5.7890
Step 3550: loss = 5.6503
  [Val @ step 3550]: train=5.6503, val=5.7964
Step 3600: loss = 5.7362
  [Val @ step 3600]: train=5.7362, val=5.7275
Step 3650: loss = 5.7975
  [Val @ step 3650]: train=5.7975, val=5.7714
Step 3700: loss = 5.7591
  [Val @ step 3700]: train=5.7591, val=5.7803
Step 3750: loss = 5.6380
  [Val @ step 3750]: train=5.6380, val=5.8270
Step 3800: loss = 5.7448
  [Val @ step 3800]: train=5.7448, val=5.7293
Step 3850: loss = 5.6408
  [Val @ step 3850]: train=5.6408, val=5.7499
Step 3900: loss = 5.7376
  [Val @ step 3900]: train=5.7376, val=5.7834
Step 3950: loss = 5.7298
  [Val @ step 3950]: train=5.7298, val=5.7499
Step 4000: loss = 5.6627
  [Val @ step 4000]: train=5.6627, val=5.7759
Step 4050: loss = 5.8044
  [Val @ step 4050]: train=5.8044, val=5.7689
Step 4100: loss = 5.6173
  [Val @ step 4100]: train=5.6173, val=5.7416
Step 4150: loss = 5.8412
  [Val @ step 4150]: train=5.8412, val=5.7406
Step 4200: loss = 5.7223
  [Val @ step 4200]: train=5.7223, val=5.7415
Step 4250: loss = 5.6963
  [Val @ step 4250]: train=5.6963, val=5.7559
Step 4300: loss = 5.6869
  [Val @ step 4300]: train=5.6869, val=5.7449
Step 4350: loss = 5.5123
  [Val @ step 4350]: train=5.5123, val=5.7436
Step 4400: loss = 5.6688
  [Val @ step 4400]: train=5.6688, val=5.7239
Step 4450: loss = 5.8675
  [Val @ step 4450]: train=5.8675, val=5.7492
Step 4500: loss = 6.0417
  [Val @ step 4500]: train=6.0417, val=5.7376
Step 4550: loss = 5.5711
  [Val @ step 4550]: train=5.5711, val=5.7368
Step 4600: loss = 5.7466
  [Val @ step 4600]: train=5.7466, val=5.7290
Step 4650: loss = 5.6748
  [Val @ step 4650]: train=5.6748, val=5.7505
Step 4700: loss = 5.7604
  [Val @ step 4700]: train=5.7604, val=5.6857
Step 4750: loss = 5.5062
  [Val @ step 4750]: train=5.5062, val=5.7330
Step 4800: loss = 5.5428
  [Val @ step 4800]: train=5.5428, val=5.7498
Step 4850: loss = 5.6072
  [Val @ step 4850]: train=5.6072, val=5.6910
Step 4900: loss = 5.9278
  [Val @ step 4900]: train=5.9278, val=5.6805
Step 4950: loss = 5.6227
  [Val @ step 4950]: train=5.6227, val=5.6959
--------------------------------------------------------------------------------
Computing final validation loss...
Validation loss: 5.6997

Results saved to time_to_target_stage1_20251225_225546.jsonl
Training time: 1470.2s
Throughput: 13,931 tokens/sec
Projections: 715/5000
nGPT Architectural Experiment: A3_po
================================================================================
Model: 11 layers x 768 dim (matching modded-nanogpt)
Hypothesis: Baseline
================================================================================

Loading FineWeb data (subset for fast experiments)...
  Loading data/fineweb10B/fineweb_train_000001.bin...
  Loading data/fineweb10B/fineweb_train_000002.bin...
  Loading data/fineweb10B/fineweb_train_000003.bin...
  Loading data/fineweb10B/fineweb_train_000004.bin...
  Vocab size: 50257
  Train tokens: 400,001,536
  Val tokens: 100,000,384

Model config (modded-nanogpt size):
  vocab_size: 50257
  n_layer: 11
  n_embd: 768
  n_head: 6
  block_size: 128
Using GatedBlock (H3) with gate_init=0.0

Total parameters: 155,166,743

Optimizer: SimpleMuon (lr=0.003, momentum=0.95, geodesic=baseline, orthog=polar_express)
  2D weight params (Muon): 77,856,768
  1D params (Adam): 77,309,975

Starting training (5000 steps)...
--------------------------------------------------------------------------------
Step    0: loss = 10.9865
Step   50: loss = 7.3129
  [Val @ step 50]: train=7.3129, val=7.3470
Step  100: loss = 6.7684
  [Val @ step 100]: train=6.7684, val=6.8882
Step  150: loss = 6.7909
  [Val @ step 150]: train=6.7909, val=6.7224
Step  200: loss = 6.7195
  [Val @ step 200]: train=6.7195, val=6.5961
Step  250: loss = 6.7083
  [Val @ step 250]: train=6.7083, val=6.5244
Step  300: loss = 6.3776
  [Val @ step 300]: train=6.3776, val=6.5242
Step  350: loss = 6.5323
  [Val @ step 350]: train=6.5323, val=6.4258
Step  400: loss = 6.4534
  [Val @ step 400]: train=6.4534, val=6.3536
Step  450: loss = 6.1797
  [Val @ step 450]: train=6.1797, val=6.3212
Step  500: loss = 6.3721
  [Val @ step 500]: train=6.3721, val=6.3422
Step  550: loss = 6.2068
  [Val @ step 550]: train=6.2068, val=6.2465
Step  600: loss = 6.2159
  [Val @ step 600]: train=6.2159, val=6.2313
Step  650: loss = 6.2981
  [Val @ step 650]: train=6.2981, val=6.1943
Step  700: loss = 6.1284
  [Val @ step 700]: train=6.1284, val=6.1884
Step  750: loss = 6.1010
  [Val @ step 750]: train=6.1010, val=6.1640
Step  800: loss = 6.1927
  [Val @ step 800]: train=6.1927, val=6.1163
Step  850: loss = 6.3111
  [Val @ step 850]: train=6.3111, val=6.1392
Step  900: loss = 5.9581
  [Val @ step 900]: train=5.9581, val=6.1061
Step  950: loss = 5.9723
  [Val @ step 950]: train=5.9723, val=6.0769
Step 1000: loss = 5.8916
  [Val @ step 1000]: train=5.8916, val=6.0848
Step 1050: loss = 5.9327
  [Val @ step 1050]: train=5.9327, val=6.0510
Step 1100: loss = 6.1232
  [Val @ step 1100]: train=6.1232, val=6.0371
Step 1150: loss = 6.0603
  [Val @ step 1150]: train=6.0603, val=6.0215
Step 1200: loss = 6.1400
  [Val @ step 1200]: train=6.1400, val=6.0158
Step 1250: loss = 5.9636
  [Val @ step 1250]: train=5.9636, val=5.9941
Step 1300: loss = 5.9448
  [Val @ step 1300]: train=5.9448, val=5.9860
Step 1350: loss = 5.9779
  [Val @ step 1350]: train=5.9779, val=5.9883
Step 1400: loss = 5.9675
  [Val @ step 1400]: train=5.9675, val=5.9554
Step 1450: loss = 5.8357
  [Val @ step 1450]: train=5.8357, val=5.9444
Step 1500: loss = 5.9803
  [Val @ step 1500]: train=5.9803, val=5.9522
Step 1550: loss = 5.9342
  [Val @ step 1550]: train=5.9342, val=5.9546
Step 1600: loss = 5.8850
  [Val @ step 1600]: train=5.8850, val=5.8870
Step 1650: loss = 5.8710
  [Val @ step 1650]: train=5.8710, val=5.8876
Step 1700: loss = 5.7997
  [Val @ step 1700]: train=5.7997, val=5.8684
Step 1750: loss = 5.8961
  [Val @ step 1750]: train=5.8961, val=5.8575
Step 1800: loss = 5.7918
  [Val @ step 1800]: train=5.7918, val=5.9016
Step 1850: loss = 5.8998
  [Val @ step 1850]: train=5.8998, val=5.8429
Step 1900: loss = 5.7749
  [Val @ step 1900]: train=5.7749, val=5.8258
Step 1950: loss = 6.0808
  [Val @ step 1950]: train=6.0808, val=5.8638
Step 2000: loss = 5.9018
  [Val @ step 2000]: train=5.9018, val=5.8683
Step 2050: loss = 5.8616
  [Val @ step 2050]: train=5.8616, val=5.8303
Step 2100: loss = 5.8033
  [Val @ step 2100]: train=5.8033, val=5.8522
Step 2150: loss = 5.9375
  [Val @ step 2150]: train=5.9375, val=5.8462
Step 2200: loss = 5.8784
  [Val @ step 2200]: train=5.8784, val=5.7791
Step 2250: loss = 5.8886
  [Val @ step 2250]: train=5.8886, val=5.8219
Step 2300: loss = 5.8071
  [Val @ step 2300]: train=5.8071, val=5.8226
Step 2350: loss = 6.0167
  [Val @ step 2350]: train=6.0167, val=5.8026
Step 2400: loss = 5.7772
  [Val @ step 2400]: train=5.7772, val=5.7949
Step 2450: loss = 5.8734
  [Val @ step 2450]: train=5.8734, val=5.7531
Step 2500: loss = 5.9348
  [Val @ step 2500]: train=5.9348, val=5.7845
Step 2550: loss = 6.0750
  [Val @ step 2550]: train=6.0750, val=5.7773
Step 2600: loss = 5.7289
  [Val @ step 2600]: train=5.7289, val=5.7780
Step 2650: loss = 5.6898
  [Val @ step 2650]: train=5.6898, val=5.7416
Step 2700: loss = 5.7653
  [Val @ step 2700]: train=5.7653, val=5.7243
Step 2750: loss = 5.7169
  [Val @ step 2750]: train=5.7169, val=5.7706
Step 2800: loss = 5.7215
  [Val @ step 2800]: train=5.7215, val=5.7591
Step 2850: loss = 5.7625
  [Val @ step 2850]: train=5.7625, val=5.8032
Step 2900: loss = 5.7089
  [Val @ step 2900]: train=5.7089, val=5.7124
Step 2950: loss = 5.8404
  [Val @ step 2950]: train=5.8404, val=5.7363
Step 3000: loss = 5.7699
  [Val @ step 3000]: train=5.7699, val=5.7222
Step 3050: loss = 5.8448
  [Val @ step 3050]: train=5.8448, val=5.7739
Step 3100: loss = 5.6813
  [Val @ step 3100]: train=5.6813, val=5.7686
Step 3150: loss = 5.5672
  [Val @ step 3150]: train=5.5672, val=5.7286
Step 3200: loss = 5.6500
  [Val @ step 3200]: train=5.6500, val=5.7142
Step 3250: loss = 5.6569
  [Val @ step 3250]: train=5.6569, val=5.7463
Step 3300: loss = 5.8880
  [Val @ step 3300]: train=5.8880, val=5.6733
Step 3350: loss = 5.6746
  [Val @ step 3350]: train=5.6746, val=5.6578
Step 3400: loss = 5.7287
  [Val @ step 3400]: train=5.7287, val=5.6651
Step 3450: loss = 5.6878
  [Val @ step 3450]: train=5.6878, val=5.6847
Step 3500: loss = 5.5712
  [Val @ step 3500]: train=5.5712, val=5.6627
Step 3550: loss = 5.6106
  [Val @ step 3550]: train=5.6106, val=5.6588
Step 3600: loss = 5.6094
  [Val @ step 3600]: train=5.6094, val=5.6598
Step 3650: loss = 5.5737
  [Val @ step 3650]: train=5.5737, val=5.6914
Step 3700: loss = 5.6725
  [Val @ step 3700]: train=5.6725, val=5.6905
Step 3750: loss = 5.7509
  [Val @ step 3750]: train=5.7509, val=5.6806
Step 3800: loss = 5.7806
  [Val @ step 3800]: train=5.7806, val=5.6513
Step 3850: loss = 5.5930
  [Val @ step 3850]: train=5.5930, val=5.7212
Step 3900: loss = 5.5758
  [Val @ step 3900]: train=5.5758, val=5.6636
Step 3950: loss = 5.4923
  [Val @ step 3950]: train=5.4923, val=5.7088
Step 4000: loss = 5.6638
  [Val @ step 4000]: train=5.6638, val=5.6932
Step 4050: loss = 5.5405
  [Val @ step 4050]: train=5.5405, val=5.6878
Step 4100: loss = 5.5299
  [Val @ step 4100]: train=5.5299, val=5.6567
Step 4150: loss = 5.6248
  [Val @ step 4150]: train=5.6248, val=5.6533
Step 4200: loss = 5.6358
  [Val @ step 4200]: train=5.6358, val=5.6564
Step 4250: loss = 5.8621
  [Val @ step 4250]: train=5.8621, val=5.6484
Step 4300: loss = 5.6731
  [Val @ step 4300]: train=5.6731, val=5.6573
Step 4350: loss = 5.6463
  [Val @ step 4350]: train=5.6463, val=5.6328
Step 4400: loss = 5.5391
  [Val @ step 4400]: train=5.5391, val=5.6749
Step 4450: loss = 5.8098
  [Val @ step 4450]: train=5.8098, val=5.6025
Step 4500: loss = 5.4670
  [Val @ step 4500]: train=5.4670, val=5.6399
Step 4550: loss = 5.6392
  [Val @ step 4550]: train=5.6392, val=5.6164
Step 4600: loss = 5.5831
  [Val @ step 4600]: train=5.5831, val=5.5871
Step 4650: loss = 5.5109
  [Val @ step 4650]: train=5.5109, val=5.6177
Step 4700: loss = 5.5064
  [Val @ step 4700]: train=5.5064, val=5.6583
Step 4750: loss = 5.7198
  [Val @ step 4750]: train=5.7198, val=5.6201
Step 4800: loss = 5.7420
  [Val @ step 4800]: train=5.7420, val=5.5915
Step 4850: loss = 5.6471
  [Val @ step 4850]: train=5.6471, val=5.6204
Step 4900: loss = 5.6283
  [Val @ step 4900]: train=5.6283, val=5.5913
Step 4950: loss = 5.5789
  [Val @ step 4950]: train=5.5789, val=5.6128
--------------------------------------------------------------------------------
Computing final validation loss...
Validation loss: 5.6084

Results saved to time_to_target_stage1_20251225_225546.jsonl
Training time: 1557.8s
Throughput: 13,147 tokens/sec
Projections: 715/5000
nGPT Architectural Experiment: A4_ne
================================================================================
Model: 11 layers x 768 dim (matching modded-nanogpt)
Hypothesis: Baseline
================================================================================

Loading FineWeb data (subset for fast experiments)...
  Loading data/fineweb10B/fineweb_train_000001.bin...
  Loading data/fineweb10B/fineweb_train_000002.bin...
  Loading data/fineweb10B/fineweb_train_000003.bin...
  Loading data/fineweb10B/fineweb_train_000004.bin...
  Vocab size: 50257
  Train tokens: 400,001,536
  Val tokens: 100,000,384

Model config (modded-nanogpt size):
  vocab_size: 50257
  n_layer: 11
  n_embd: 768
  n_head: 6
  block_size: 128
Using GatedBlock (H3) with gate_init=0.0

Total parameters: 155,166,743

Optimizer: SimpleMuon (lr=0.01, momentum=0.9, geodesic=baseline, orthog=newton_schulz)
  2D weight params (Muon): 77,856,768
  1D params (Adam): 77,309,975

Starting training (5000 steps)...
--------------------------------------------------------------------------------
Step    0: loss = 10.9690
Step   50: loss = 7.0228
  [Val @ step 50]: train=7.0228, val=6.8117
Step  100: loss = 6.5681
  [Val @ step 100]: train=6.5681, val=6.5807
Step  150: loss = 6.4586
  [Val @ step 150]: train=6.4586, val=6.5164
Step  200: loss = 6.3501
  [Val @ step 200]: train=6.3501, val=6.4086
Step  250: loss = 6.4697
  [Val @ step 250]: train=6.4697, val=6.3066
Step  300: loss = 6.2624
  [Val @ step 300]: train=6.2624, val=6.2986
Step  350: loss = 6.1749
  [Val @ step 350]: train=6.1749, val=6.2294
Step  400: loss = 6.1844
  [Val @ step 400]: train=6.1844, val=6.2143
Step  450: loss = 6.1663
  [Val @ step 450]: train=6.1663, val=6.1876
Step  500: loss = 6.1430
  [Val @ step 500]: train=6.1430, val=6.1670
Step  550: loss = 5.9459
  [Val @ step 550]: train=5.9459, val=6.1354
Step  600: loss = 6.0603
  [Val @ step 600]: train=6.0603, val=6.1219
Step  650: loss = 6.1575
  [Val @ step 650]: train=6.1575, val=6.1351
Step  700: loss = 6.0918
  [Val @ step 700]: train=6.0918, val=6.1060
Step  750: loss = 5.9890
  [Val @ step 750]: train=5.9890, val=6.0736
Step  800: loss = 6.0538
  [Val @ step 800]: train=6.0538, val=6.0852
Step  850: loss = 6.1551
  [Val @ step 850]: train=6.1551, val=6.0732
Step  900: loss = 6.0424
  [Val @ step 900]: train=6.0424, val=6.0494
Step  950: loss = 6.1787
  [Val @ step 950]: train=6.1787, val=6.0594
Step 1000: loss = 6.1531
  [Val @ step 1000]: train=6.1531, val=6.0381
Step 1050: loss = 6.0348
  [Val @ step 1050]: train=6.0348, val=6.0315
Step 1100: loss = 6.0053
  [Val @ step 1100]: train=6.0053, val=6.0764
Step 1150: loss = 5.9519
  [Val @ step 1150]: train=5.9519, val=6.0183
Step 1200: loss = 5.9455
  [Val @ step 1200]: train=5.9455, val=6.0380
Step 1250: loss = 6.0862
  [Val @ step 1250]: train=6.0862, val=6.0584
Step 1300: loss = 6.0244
  [Val @ step 1300]: train=6.0244, val=6.0424
Step 1350: loss = 5.9407
  [Val @ step 1350]: train=5.9407, val=6.0136
Step 1400: loss = 6.0105
  [Val @ step 1400]: train=6.0105, val=6.0176
Step 1450: loss = 6.0626
  [Val @ step 1450]: train=6.0626, val=6.0283
Step 1500: loss = 6.0063
  [Val @ step 1500]: train=6.0063, val=5.9993
Step 1550: loss = 5.9792
  [Val @ step 1550]: train=5.9792, val=6.0469
Step 1600: loss = 6.0371
  [Val @ step 1600]: train=6.0371, val=5.9992
Step 1650: loss = 5.9440
  [Val @ step 1650]: train=5.9440, val=5.9899
Step 1700: loss = 5.9682
  [Val @ step 1700]: train=5.9682, val=5.9827
Step 1750: loss = 6.0670
  [Val @ step 1750]: train=6.0670, val=6.0008
Step 1800: loss = 5.9858
  [Val @ step 1800]: train=5.9858, val=5.9810
Step 1850: loss = 6.0148
  [Val @ step 1850]: train=6.0148, val=6.0263
Step 1900: loss = 5.9047
  [Val @ step 1900]: train=5.9047, val=5.9691
Step 1950: loss = 6.1219
  [Val @ step 1950]: train=6.1219, val=6.0098
Step 2000: loss = 6.0631
  [Val @ step 2000]: train=6.0631, val=6.0042
Step 2050: loss = 5.8817
  [Val @ step 2050]: train=5.8817, val=6.0268
Step 2100: loss = 5.9433
  [Val @ step 2100]: train=5.9433, val=5.9882
Step 2150: loss = 5.9841
  [Val @ step 2150]: train=5.9841, val=5.9854
Step 2200: loss = 5.9329
  [Val @ step 2200]: train=5.9329, val=6.0014
Step 2250: loss = 5.9842
  [Val @ step 2250]: train=5.9842, val=5.9726
Step 2300: loss = 5.8448
  [Val @ step 2300]: train=5.8448, val=5.9920
Step 2350: loss = 6.0027
  [Val @ step 2350]: train=6.0027, val=6.0221
Step 2400: loss = 5.9173
  [Val @ step 2400]: train=5.9173, val=5.9706
Step 2450: loss = 6.0729
  [Val @ step 2450]: train=6.0729, val=5.9641
Step 2500: loss = 6.0570
  [Val @ step 2500]: train=6.0570, val=5.9956
Step 2550: loss = 6.1073
  [Val @ step 2550]: train=6.1073, val=5.9783
Step 2600: loss = 5.8937
  [Val @ step 2600]: train=5.8937, val=5.9425
Step 2650: loss = 6.0355
  [Val @ step 2650]: train=6.0355, val=5.9864
Step 2700: loss = 6.0599
  [Val @ step 2700]: train=6.0599, val=5.9450
Step 2750: loss = 5.8675
  [Val @ step 2750]: train=5.8675, val=5.9583
Step 2800: loss = 6.0298
  [Val @ step 2800]: train=6.0298, val=5.9487
Step 2850: loss = 5.9912
  [Val @ step 2850]: train=5.9912, val=5.9671
Step 2900: loss = 6.0740
  [Val @ step 2900]: train=6.0740, val=5.9410
Step 2950: loss = 6.0167
  [Val @ step 2950]: train=6.0167, val=5.9552
Step 3000: loss = 5.9644
  [Val @ step 3000]: train=5.9644, val=5.9919
Step 3050: loss = 5.9988
  [Val @ step 3050]: train=5.9988, val=5.9597
Step 3100: loss = 5.8691
  [Val @ step 3100]: train=5.8691, val=5.9506
Step 3150: loss = 6.0437
  [Val @ step 3150]: train=6.0437, val=5.9549
Step 3200: loss = 5.9027
  [Val @ step 3200]: train=5.9027, val=5.9614
Step 3250: loss = 5.9599
  [Val @ step 3250]: train=5.9599, val=5.9824
Step 3300: loss = 5.8905
  [Val @ step 3300]: train=5.8905, val=5.9381
Step 3350: loss = 6.0808
  [Val @ step 3350]: train=6.0808, val=5.9558
Step 3400: loss = 5.9250
  [Val @ step 3400]: train=5.9250, val=5.9624
Step 3450: loss = 5.9361
  [Val @ step 3450]: train=5.9361, val=5.9167
Step 3500: loss = 5.7983
  [Val @ step 3500]: train=5.7983, val=5.9234
Step 3550: loss = 5.8823
  [Val @ step 3550]: train=5.8823, val=5.9213
Step 3600: loss = 6.0419
  [Val @ step 3600]: train=6.0419, val=5.9593
Step 3650: loss = 6.0159
  [Val @ step 3650]: train=6.0159, val=5.9411
Step 3700: loss = 5.9676
  [Val @ step 3700]: train=5.9676, val=5.9636
Step 3750: loss = 5.8732
  [Val @ step 3750]: train=5.8732, val=5.9255
Step 3800: loss = 6.0508
  [Val @ step 3800]: train=6.0508, val=5.9391
Step 3850: loss = 5.9132
  [Val @ step 3850]: train=5.9132, val=5.9452
Step 3900: loss = 5.9416
  [Val @ step 3900]: train=5.9416, val=5.9588
Step 3950: loss = 5.8845
  [Val @ step 3950]: train=5.8845, val=5.9258
Step 4000: loss = 5.8700
  [Val @ step 4000]: train=5.8700, val=5.9190
Step 4050: loss = 5.8573
  [Val @ step 4050]: train=5.8573, val=5.9404
Step 4100: loss = 5.8263
  [Val @ step 4100]: train=5.8263, val=5.9146
Step 4150: loss = 5.9248
  [Val @ step 4150]: train=5.9248, val=5.9474
Step 4200: loss = 5.9962
  [Val @ step 4200]: train=5.9962, val=5.9555
Step 4250: loss = 5.9229
  [Val @ step 4250]: train=5.9229, val=5.9462
Step 4300: loss = 5.8782
  [Val @ step 4300]: train=5.8782, val=5.9368
Step 4350: loss = 5.9026
  [Val @ step 4350]: train=5.9026, val=5.9166
Step 4400: loss = 5.9742
  [Val @ step 4400]: train=5.9742, val=5.9359
Step 4450: loss = 5.9852
  [Val @ step 4450]: train=5.9852, val=5.9048
Step 4500: loss = 5.9032
  [Val @ step 4500]: train=5.9032, val=5.9185
Step 4550: loss = 5.9083
  [Val @ step 4550]: train=5.9083, val=5.9168
Step 4600: loss = 6.0454
  [Val @ step 4600]: train=6.0454, val=5.9192
Step 4650: loss = 5.7823
  [Val @ step 4650]: train=5.7823, val=5.9190
Step 4700: loss = 5.9374
  [Val @ step 4700]: train=5.9374, val=5.9131
Step 4750: loss = 5.8199
  [Val @ step 4750]: train=5.8199, val=5.9139
Step 4800: loss = 5.9007
  [Val @ step 4800]: train=5.9007, val=5.9478
Step 4850: loss = 5.9863
  [Val @ step 4850]: train=5.9863, val=5.9176
Step 4900: loss = 5.9168
  [Val @ step 4900]: train=5.9168, val=5.9234
Step 4950: loss = 5.9777
  [Val @ step 4950]: train=5.9777, val=5.9078
--------------------------------------------------------------------------------
Computing final validation loss...
Validation loss: 5.8847

Results saved to time_to_target_stage1_20251225_225546.jsonl
Training time: 3749.2s
Throughput: 10,925 tokens/sec
Projections: 500/5000
nGPT Architectural Experiment: B1_vr0_cwd0
================================================================================
Model: 11 layers x 768 dim (matching modded-nanogpt)
Hypothesis: Baseline
================================================================================

Loading FineWeb data (subset for fast experiments)...
  Loading data/fineweb10B/fineweb_train_000001.bin...
  Loading data/fineweb10B/fineweb_train_000002.bin...
  Loading data/fineweb10B/fineweb_train_000003.bin...
  Loading data/fineweb10B/fineweb_train_000004.bin...
  Vocab size: 50257
  Train tokens: 400,001,536
  Val tokens: 100,000,384

Model config (modded-nanogpt size):
  vocab_size: 50257
  n_layer: 11
  n_embd: 768
  n_head: 6
  block_size: 128
Using GatedBlock (H3) with gate_init=0.0

Total parameters: 155,166,743

Optimizer: SimpleMuon (lr=0.003, momentum=0.95, geodesic=baseline, orthog=newton_schulz)
  2D weight params (Muon): 77,856,768
  1D params (Adam): 77,309,975

Starting training (5000 steps)...
--------------------------------------------------------------------------------
Step    0: loss = 10.9583
Step   50: loss = 7.2301
  [Val @ step 50]: train=7.2301, val=7.4512
Step  100: loss = 7.0667
  [Val @ step 100]: train=7.0667, val=6.9339
Step  150: loss = 6.6885
  [Val @ step 150]: train=6.6885, val=6.7668
Step  200: loss = 6.6381
  [Val @ step 200]: train=6.6381, val=6.6511
Step  250: loss = 6.4139
  [Val @ step 250]: train=6.4139, val=6.5876
Step  300: loss = 6.5668
  [Val @ step 300]: train=6.5668, val=6.5463
Step  350: loss = 6.5752
  [Val @ step 350]: train=6.5752, val=6.4648
Step  400: loss = 6.4052
  [Val @ step 400]: train=6.4052, val=6.3873
Step  450: loss = 6.3359
  [Val @ step 450]: train=6.3359, val=6.3940
Step  500: loss = 6.2951
  [Val @ step 500]: train=6.2951, val=6.3098
Step  550: loss = 6.1492
  [Val @ step 550]: train=6.1492, val=6.3437
Step  600: loss = 6.2617
  [Val @ step 600]: train=6.2617, val=6.2747
Step  650: loss = 6.1832
  [Val @ step 650]: train=6.1832, val=6.2216
Step  700: loss = 6.0862
  [Val @ step 700]: train=6.0862, val=6.2395
Step  750: loss = 6.1932
  [Val @ step 750]: train=6.1932, val=6.1960
Step  800: loss = 5.8750
  [Val @ step 800]: train=5.8750, val=6.2380
Step  850: loss = 6.1536
  [Val @ step 850]: train=6.1536, val=6.1385
Step  900: loss = 6.1626
  [Val @ step 900]: train=6.1626, val=6.1642
Step  950: loss = 6.2801
  [Val @ step 950]: train=6.2801, val=6.1288
Step 1000: loss = 6.1737
  [Val @ step 1000]: train=6.1737, val=6.1177
Step 1050: loss = 6.0990
  [Val @ step 1050]: train=6.0990, val=6.0577
Step 1100: loss = 5.8754
  [Val @ step 1100]: train=5.8754, val=6.0892
Step 1150: loss = 6.1277
  [Val @ step 1150]: train=6.1277, val=6.0746
Step 1200: loss = 5.9984
  [Val @ step 1200]: train=5.9984, val=6.0680
Step 1250: loss = 5.9661
  [Val @ step 1250]: train=5.9661, val=6.0331
Step 1300: loss = 5.9471
  [Val @ step 1300]: train=5.9471, val=5.9834
Step 1350: loss = 6.0753
  [Val @ step 1350]: train=6.0753, val=6.0208
Step 1400: loss = 5.9563
  [Val @ step 1400]: train=5.9563, val=6.0069
Step 1450: loss = 5.9010
  [Val @ step 1450]: train=5.9010, val=5.9699
Step 1500: loss = 5.8890
  [Val @ step 1500]: train=5.8890, val=5.9620
Step 1550: loss = 6.0803
  [Val @ step 1550]: train=6.0803, val=5.9813
Step 1600: loss = 5.9126
  [Val @ step 1600]: train=5.9126, val=6.0051
Step 1650: loss = 6.0222
  [Val @ step 1650]: train=6.0222, val=5.9685
Step 1700: loss = 5.8725
  [Val @ step 1700]: train=5.8725, val=5.9613
Step 1750: loss = 5.7802
  [Val @ step 1750]: train=5.7802, val=5.9792
Step 1800: loss = 5.9414
  [Val @ step 1800]: train=5.9414, val=5.9120
Step 1850: loss = 5.8856
  [Val @ step 1850]: train=5.8856, val=5.9161
Step 1900: loss = 5.7874
  [Val @ step 1900]: train=5.7874, val=5.9454
Step 1950: loss = 5.8896
  [Val @ step 1950]: train=5.8896, val=5.9503
Step 2000: loss = 5.8497
  [Val @ step 2000]: train=5.8497, val=5.9278
Step 2050: loss = 5.9656
  [Val @ step 2050]: train=5.9656, val=5.9238
Step 2100: loss = 6.0442
  [Val @ step 2100]: train=6.0442, val=5.9232
Step 2150: loss = 5.8208
  [Val @ step 2150]: train=5.8208, val=5.8855
Step 2200: loss = 5.9458
  [Val @ step 2200]: train=5.9458, val=5.8838
Step 2250: loss = 5.9846
  [Val @ step 2250]: train=5.9846, val=5.8942
Step 2300: loss = 5.9483
  [Val @ step 2300]: train=5.9483, val=5.8418
Step 2350: loss = 5.9141
  [Val @ step 2350]: train=5.9141, val=5.8694
Step 2400: loss = 5.8357
  [Val @ step 2400]: train=5.8357, val=5.8323
Step 2450: loss = 5.8517
  [Val @ step 2450]: train=5.8517, val=5.8297
Step 2500: loss = 5.8417
  [Val @ step 2500]: train=5.8417, val=5.8582
Step 2550: loss = 5.8579
  [Val @ step 2550]: train=5.8579, val=5.8830
Step 2600: loss = 5.7646
  [Val @ step 2600]: train=5.7646, val=5.8732
Step 2650: loss = 5.8113
  [Val @ step 2650]: train=5.8113, val=5.7972
Step 2700: loss = 5.9703
  [Val @ step 2700]: train=5.9703, val=5.8396
Step 2750: loss = 5.6828
  [Val @ step 2750]: train=5.6828, val=5.8657
Step 2800: loss = 6.0061
  [Val @ step 2800]: train=6.0061, val=5.8520
Step 2850: loss = 5.8353
  [Val @ step 2850]: train=5.8353, val=5.8751
Step 2900: loss = 5.7203
  [Val @ step 2900]: train=5.7203, val=5.8330
Step 2950: loss = 5.9219
  [Val @ step 2950]: train=5.9219, val=5.8374
Step 3000: loss = 5.8664
  [Val @ step 3000]: train=5.8664, val=5.8445
Step 3050: loss = 5.8825
  [Val @ step 3050]: train=5.8825, val=5.8362
Step 3100: loss = 5.8224
  [Val @ step 3100]: train=5.8224, val=5.8050
Step 3150: loss = 5.7279
  [Val @ step 3150]: train=5.7279, val=5.8540
Step 3200: loss = 5.8077
  [Val @ step 3200]: train=5.8077, val=5.8000
Step 3250: loss = 5.5037
  [Val @ step 3250]: train=5.5037, val=5.8066
Step 3300: loss = 5.7122
  [Val @ step 3300]: train=5.7122, val=5.7952
Step 3350: loss = 5.8222
  [Val @ step 3350]: train=5.8222, val=5.8018
Step 3400: loss = 5.7829
  [Val @ step 3400]: train=5.7829, val=5.8013
Step 3450: loss = 5.8601
  [Val @ step 3450]: train=5.8601, val=5.7806
Step 3500: loss = 5.7320
  [Val @ step 3500]: train=5.7320, val=5.7957
Step 3550: loss = 5.8994
  [Val @ step 3550]: train=5.8994, val=5.7980
Step 3600: loss = 5.6780
  [Val @ step 3600]: train=5.6780, val=5.8065
Step 3650: loss = 5.9073
  [Val @ step 3650]: train=5.9073, val=5.7776
Step 3700: loss = 5.5736
  [Val @ step 3700]: train=5.5736, val=5.7805
Step 3750: loss = 5.8861
  [Val @ step 3750]: train=5.8861, val=5.7843
Step 3800: loss = 5.9316
  [Val @ step 3800]: train=5.9316, val=5.7685
Step 3850: loss = 5.9215
  [Val @ step 3850]: train=5.9215, val=5.7603
Step 3900: loss = 5.6334
  [Val @ step 3900]: train=5.6334, val=5.7582
Step 3950: loss = 5.7032
  [Val @ step 3950]: train=5.7032, val=5.7536
Step 4000: loss = 5.8074
  [Val @ step 4000]: train=5.8074, val=5.7878
Step 4050: loss = 5.6610
  [Val @ step 4050]: train=5.6610, val=5.7331
Step 4100: loss = 5.8624
  [Val @ step 4100]: train=5.8624, val=5.7880
Step 4150: loss = 5.9049
  [Val @ step 4150]: train=5.9049, val=5.8120
Step 4200: loss = 5.7489
  [Val @ step 4200]: train=5.7489, val=5.7980
Step 4250: loss = 5.6156
  [Val @ step 4250]: train=5.6156, val=5.7551
Step 4300: loss = 5.8616
  [Val @ step 4300]: train=5.8616, val=5.7725
Step 4350: loss = 5.6801
  [Val @ step 4350]: train=5.6801, val=5.7780
Step 4400: loss = 5.7465
  [Val @ step 4400]: train=5.7465, val=5.7275
Step 4450: loss = 5.7711
  [Val @ step 4450]: train=5.7711, val=5.7545
Step 4500: loss = 5.8055
  [Val @ step 4500]: train=5.8055, val=5.7886
Step 4550: loss = 5.6049
  [Val @ step 4550]: train=5.6049, val=5.7646
Step 4600: loss = 5.7392
  [Val @ step 4600]: train=5.7392, val=5.7474
Step 4650: loss = 5.7176
  [Val @ step 4650]: train=5.7176, val=5.7540
Step 4700: loss = 5.7145
  [Val @ step 4700]: train=5.7145, val=5.7576
Step 4750: loss = 5.6635
  [Val @ step 4750]: train=5.6635, val=5.7311
Step 4800: loss = 5.8116
  [Val @ step 4800]: train=5.8116, val=5.7577
Step 4850: loss = 5.6756
  [Val @ step 4850]: train=5.6756, val=5.7557
Step 4900: loss = 5.7457
  [Val @ step 4900]: train=5.7457, val=5.7782
Step 4950: loss = 5.6573
  [Val @ step 4950]: train=5.6573, val=5.7035
--------------------------------------------------------------------------------
Computing final validation loss...
Validation loss: 5.7282

Results saved to time_to_target_stage1_20251225_225546.jsonl
Training time: 2696.9s
Throughput: 7,594 tokens/sec
Projections: 715/5000
nGPT Architectural Experiment: B2_vr1_cwd0
================================================================================
Model: 11 layers x 768 dim (matching modded-nanogpt)
Hypothesis: Baseline
================================================================================

Loading FineWeb data (subset for fast experiments)...
  Loading data/fineweb10B/fineweb_train_000001.bin...
  Loading data/fineweb10B/fineweb_train_000002.bin...
  Loading data/fineweb10B/fineweb_train_000003.bin...
  Loading data/fineweb10B/fineweb_train_000004.bin...
  Vocab size: 50257
  Train tokens: 400,001,536
  Val tokens: 100,000,384

Model config (modded-nanogpt size):
  vocab_size: 50257
  n_layer: 11
  n_embd: 768
  n_head: 6
  block_size: 128
Using GatedBlock (H3) with gate_init=0.0

Total parameters: 155,166,743

Optimizer: SimpleMuon (lr=0.003, momentum=0.95, geodesic=baseline, orthog=newton_schulz)
  2D weight params (Muon): 77,856,768
  1D params (Adam): 77,309,975

Starting training (5000 steps)...
--------------------------------------------------------------------------------
Step    0: loss = 10.9801
Step   50: loss = 7.3043
  [Val @ step 50]: train=7.3043, val=7.3253
Step  100: loss = 6.8031
  [Val @ step 100]: train=6.8031, val=6.9695
Step  150: loss = 6.7281
  [Val @ step 150]: train=6.7281, val=6.7560
Step  200: loss = 6.4347
  [Val @ step 200]: train=6.4347, val=6.6993
Step  250: loss = 6.1809
  [Val @ step 250]: train=6.1809, val=6.5274
Step  300: loss = 6.5171
  [Val @ step 300]: train=6.5171, val=6.5328
Step  350: loss = 6.3251
  [Val @ step 350]: train=6.3251, val=6.5000
Step  400: loss = 6.5742
  [Val @ step 400]: train=6.5742, val=6.4534
Step  450: loss = 6.2598
  [Val @ step 450]: train=6.2598, val=6.3668
Step  500: loss = 6.0194
  [Val @ step 500]: train=6.0194, val=6.3469
Step  550: loss = 6.2660
  [Val @ step 550]: train=6.2660, val=6.2617
Step  600: loss = 6.2807
  [Val @ step 600]: train=6.2807, val=6.3015
Step  650: loss = 6.0369
  [Val @ step 650]: train=6.0369, val=6.2778
Step  700: loss = 6.2096
  [Val @ step 700]: train=6.2096, val=6.2237
Step  750: loss = 6.1346
  [Val @ step 750]: train=6.1346, val=6.2096
Step  800: loss = 6.2848
  [Val @ step 800]: train=6.2848, val=6.2204
Step  850: loss = 6.2895
  [Val @ step 850]: train=6.2895, val=6.1331
Step  900: loss = 5.9692
  [Val @ step 900]: train=5.9692, val=6.1617
Step  950: loss = 6.1235
  [Val @ step 950]: train=6.1235, val=6.0971
Step 1000: loss = 5.9871
  [Val @ step 1000]: train=5.9871, val=6.1088
Step 1050: loss = 6.1129
  [Val @ step 1050]: train=6.1129, val=6.0950
Step 1100: loss = 6.1485
  [Val @ step 1100]: train=6.1485, val=6.1413
Step 1150: loss = 6.0125
  [Val @ step 1150]: train=6.0125, val=6.0718
Step 1200: loss = 5.8806
  [Val @ step 1200]: train=5.8806, val=6.0205
Step 1250: loss = 5.8322
  [Val @ step 1250]: train=5.8322, val=6.0637
Step 1300: loss = 6.0427
  [Val @ step 1300]: train=6.0427, val=6.0849
Step 1350: loss = 6.0653
  [Val @ step 1350]: train=6.0653, val=6.0143
Step 1400: loss = 6.0613
  [Val @ step 1400]: train=6.0613, val=6.0118
Step 1450: loss = 5.8623
  [Val @ step 1450]: train=5.8623, val=5.9975
Step 1500: loss = 5.9060
  [Val @ step 1500]: train=5.9060, val=5.9406
Step 1550: loss = 6.0654
  [Val @ step 1550]: train=6.0654, val=5.9746
Step 1600: loss = 5.9120
  [Val @ step 1600]: train=5.9120, val=5.9866
Step 1650: loss = 5.9069
  [Val @ step 1650]: train=5.9069, val=5.9423
Step 1700: loss = 5.7948
  [Val @ step 1700]: train=5.7948, val=5.9696
Step 1750: loss = 5.8936
  [Val @ step 1750]: train=5.8936, val=5.9331
Step 1800: loss = 5.9564
  [Val @ step 1800]: train=5.9564, val=5.9405
Step 1850: loss = 5.7571
  [Val @ step 1850]: train=5.7571, val=5.9113
Step 1900: loss = 5.9719
  [Val @ step 1900]: train=5.9719, val=5.9140
Step 1950: loss = 5.9148
  [Val @ step 1950]: train=5.9148, val=5.9103
Step 2000: loss = 5.8779
  [Val @ step 2000]: train=5.8779, val=5.9586
Step 2050: loss = 5.7713
  [Val @ step 2050]: train=5.7713, val=5.9049
Step 2100: loss = 6.0014
  [Val @ step 2100]: train=6.0014, val=5.9047
Step 2150: loss = 5.8902
  [Val @ step 2150]: train=5.8902, val=5.9411
Step 2200: loss = 5.9586
  [Val @ step 2200]: train=5.9586, val=5.9137
Step 2250: loss = 5.9108
  [Val @ step 2250]: train=5.9108, val=5.8516
Step 2300: loss = 6.0105
  [Val @ step 2300]: train=6.0105, val=5.8961
Step 2350: loss = 5.8856
  [Val @ step 2350]: train=5.8856, val=5.8594
Step 2400: loss = 5.7974
  [Val @ step 2400]: train=5.7974, val=5.8900
Step 2450: loss = 5.8997
  [Val @ step 2450]: train=5.8997, val=5.8734
Step 2500: loss = 5.7117
  [Val @ step 2500]: train=5.7117, val=5.8495
Step 2550: loss = 5.7847
  [Val @ step 2550]: train=5.7847, val=5.8826
Step 2600: loss = 5.8566
  [Val @ step 2600]: train=5.8566, val=5.8344
Step 2650: loss = 5.9825
  [Val @ step 2650]: train=5.9825, val=5.8527
Step 2700: loss = 5.8340
  [Val @ step 2700]: train=5.8340, val=5.8343
Step 2750: loss = 5.6242
  [Val @ step 2750]: train=5.6242, val=5.8285
Step 2800: loss = 5.8327
  [Val @ step 2800]: train=5.8327, val=5.8319
Step 2850: loss = 5.6962
  [Val @ step 2850]: train=5.6962, val=5.8652
Step 2900: loss = 5.7152
  [Val @ step 2900]: train=5.7152, val=5.8546
Step 2950: loss = 5.7838
  [Val @ step 2950]: train=5.7838, val=5.8278
Step 3000: loss = 5.9549
  [Val @ step 3000]: train=5.9549, val=5.8241
Step 3050: loss = 5.8135
  [Val @ step 3050]: train=5.8135, val=5.8153
Step 3100: loss = 5.8392
  [Val @ step 3100]: train=5.8392, val=5.8256
Step 3150: loss = 5.8572
  [Val @ step 3150]: train=5.8572, val=5.8275
Step 3200: loss = 5.7992
  [Val @ step 3200]: train=5.7992, val=5.7820
Step 3250: loss = 5.5985
  [Val @ step 3250]: train=5.5985, val=5.8161
Step 3300: loss = 5.7313
  [Val @ step 3300]: train=5.7313, val=5.7779
Step 3350: loss = 5.7342
  [Val @ step 3350]: train=5.7342, val=5.7867
Step 3400: loss = 5.7153
  [Val @ step 3400]: train=5.7153, val=5.7669
Step 3450: loss = 5.7325
  [Val @ step 3450]: train=5.7325, val=5.7811
Step 3500: loss = 5.7626
  [Val @ step 3500]: train=5.7626, val=5.7786
Step 3550: loss = 5.8314
  [Val @ step 3550]: train=5.8314, val=5.7933
Step 3600: loss = 5.7356
  [Val @ step 3600]: train=5.7356, val=5.7740
Step 3650: loss = 5.8555
  [Val @ step 3650]: train=5.8555, val=5.8188
Step 3700: loss = 5.6611
  [Val @ step 3700]: train=5.6611, val=5.7771
Step 3750: loss = 5.6475
  [Val @ step 3750]: train=5.6475, val=5.7953
Step 3800: loss = 5.6897
  [Val @ step 3800]: train=5.6897, val=5.7867
Step 3850: loss = 5.7826
  [Val @ step 3850]: train=5.7826, val=5.7545
Step 3900: loss = 5.8294
  [Val @ step 3900]: train=5.8294, val=5.7515
Step 3950: loss = 5.7506
  [Val @ step 3950]: train=5.7506, val=5.7799
Step 4000: loss = 5.7503
  [Val @ step 4000]: train=5.7503, val=5.7304
Step 4050: loss = 5.5953
  [Val @ step 4050]: train=5.5953, val=5.7429
Step 4100: loss = 5.8279
  [Val @ step 4100]: train=5.8279, val=5.7574
Step 4150: loss = 5.7189
  [Val @ step 4150]: train=5.7189, val=5.7548
Step 4200: loss = 5.6693
  [Val @ step 4200]: train=5.6693, val=5.7501
Step 4250: loss = 5.8488
  [Val @ step 4250]: train=5.8488, val=5.7428
Step 4300: loss = 5.6618
  [Val @ step 4300]: train=5.6618, val=5.7279
Step 4350: loss = 5.8223
  [Val @ step 4350]: train=5.8223, val=5.7429
Step 4400: loss = 5.7265
  [Val @ step 4400]: train=5.7265, val=5.6954
Step 4450: loss = 5.7090
  [Val @ step 4450]: train=5.7090, val=5.6940
Step 4500: loss = 5.7294
  [Val @ step 4500]: train=5.7294, val=5.7220
Step 4550: loss = 5.8410
  [Val @ step 4550]: train=5.8410, val=5.7034
Step 4600: loss = 5.7875
  [Val @ step 4600]: train=5.7875, val=5.7173
Step 4650: loss = 5.7136
  [Val @ step 4650]: train=5.7136, val=5.7427
Step 4700: loss = 5.5625
  [Val @ step 4700]: train=5.5625, val=5.7184
Step 4750: loss = 5.7025
  [Val @ step 4750]: train=5.7025, val=5.7102
Step 4800: loss = 5.8529
  [Val @ step 4800]: train=5.8529, val=5.7335
Step 4850: loss = 5.7607
  [Val @ step 4850]: train=5.7607, val=5.7492
Step 4900: loss = 5.7083
  [Val @ step 4900]: train=5.7083, val=5.6906
Step 4950: loss = 5.6935
  [Val @ step 4950]: train=5.6935, val=5.6926
--------------------------------------------------------------------------------
Computing final validation loss...
Validation loss: 5.7133

Results saved to time_to_target_stage1_20251225_225546.jsonl
Training time: 3335.2s
Throughput: 6,141 tokens/sec
Projections: 715/5000
nGPT Architectural Experiment: A4_po
================================================================================
Model: 11 layers x 768 dim (matching modded-nanogpt)
Hypothesis: Baseline
================================================================================

Loading FineWeb data (subset for fast experiments)...
  Loading data/fineweb10B/fineweb_train_000001.bin...
  Loading data/fineweb10B/fineweb_train_000002.bin...
  Loading data/fineweb10B/fineweb_train_000003.bin...
  Loading data/fineweb10B/fineweb_train_000004.bin...
  Vocab size: 50257
  Train tokens: 400,001,536
  Val tokens: 100,000,384

Model config (modded-nanogpt size):
  vocab_size: 50257
  n_layer: 11
  n_embd: 768
  n_head: 6
  block_size: 128
Using GatedBlock (H3) with gate_init=0.0

Total parameters: 155,166,743

Optimizer: SimpleMuon (lr=0.01, momentum=0.9, geodesic=baseline, orthog=polar_express)
  2D weight params (Muon): 77,856,768
  1D params (Adam): 77,309,975

Starting training (5000 steps)...
--------------------------------------------------------------------------------
Step    0: loss = 10.9753
Step   50: loss = 6.8289
  [Val @ step 50]: train=6.8289, val=6.7867
Step  100: loss = 6.5709
  [Val @ step 100]: train=6.5709, val=6.5062
Step  150: loss = 6.5035
  [Val @ step 150]: train=6.5035, val=6.4440
Step  200: loss = 6.4440
  [Val @ step 200]: train=6.4440, val=6.3127
Step  250: loss = 6.2064
  [Val @ step 250]: train=6.2064, val=6.3061
Step  300: loss = 6.3929
  [Val @ step 300]: train=6.3929, val=6.2542
Step  350: loss = 6.2747
  [Val @ step 350]: train=6.2747, val=6.2766
Step  400: loss = 6.0226
  [Val @ step 400]: train=6.0226, val=6.1844
Step  450: loss = 6.1501
  [Val @ step 450]: train=6.1501, val=6.1630
Step  500: loss = 6.1436
  [Val @ step 500]: train=6.1436, val=6.1442
Step  550: loss = 6.0791
  [Val @ step 550]: train=6.0791, val=6.1070
Step  600: loss = 6.0140
  [Val @ step 600]: train=6.0140, val=6.1080
Step  650: loss = 6.1383
  [Val @ step 650]: train=6.1383, val=6.0980
Step  700: loss = 6.1689
  [Val @ step 700]: train=6.1689, val=6.0786
Step  750: loss = 6.0002
  [Val @ step 750]: train=6.0002, val=6.0577
Step  800: loss = 6.0920
  [Val @ step 800]: train=6.0920, val=6.0582
Step  850: loss = 5.9857
  [Val @ step 850]: train=5.9857, val=6.0194
Step  900: loss = 6.0068
  [Val @ step 900]: train=6.0068, val=6.0304
Step  950: loss = 5.9844
  [Val @ step 950]: train=5.9844, val=6.0206
Step 1000: loss = 5.9604
  [Val @ step 1000]: train=5.9604, val=6.0269
Step 1050: loss = 6.0945
  [Val @ step 1050]: train=6.0945, val=6.0101
Step 1100: loss = 5.9934
  [Val @ step 1100]: train=5.9934, val=5.9777
Step 1150: loss = 6.0857
  [Val @ step 1150]: train=6.0857, val=6.0177
Step 1200: loss = 5.9460
  [Val @ step 1200]: train=5.9460, val=5.9904
Step 1250: loss = 5.9765
  [Val @ step 1250]: train=5.9765, val=5.9817
Step 1300: loss = 6.0001
  [Val @ step 1300]: train=6.0001, val=5.9811
Step 1350: loss = 5.9363
  [Val @ step 1350]: train=5.9363, val=5.9887
Step 1400: loss = 5.9082
  [Val @ step 1400]: train=5.9082, val=5.9589
Step 1450: loss = 5.9849
  [Val @ step 1450]: train=5.9849, val=6.0078
Step 1500: loss = 5.9021
  [Val @ step 1500]: train=5.9021, val=5.9588
Step 1550: loss = 6.0790
  [Val @ step 1550]: train=6.0790, val=5.9580
Step 1600: loss = 5.8526
  [Val @ step 1600]: train=5.8526, val=5.9629
Step 1650: loss = 5.9217
  [Val @ step 1650]: train=5.9217, val=5.9659
Step 1700: loss = 5.9607
  [Val @ step 1700]: train=5.9607, val=5.9636
Step 1750: loss = 5.9605
  [Val @ step 1750]: train=5.9605, val=5.9396
Step 1800: loss = 6.0394
  [Val @ step 1800]: train=6.0394, val=5.9435
Step 1850: loss = 5.9840
  [Val @ step 1850]: train=5.9840, val=5.9574
Step 1900: loss = 5.8922
  [Val @ step 1900]: train=5.8922, val=5.9462
Step 1950: loss = 5.8832
  [Val @ step 1950]: train=5.8832, val=5.9590
Step 2000: loss = 5.9053
  [Val @ step 2000]: train=5.9053, val=5.9149
Step 2050: loss = 5.8526
  [Val @ step 2050]: train=5.8526, val=5.9107
Step 2100: loss = 5.8959
  [Val @ step 2100]: train=5.8959, val=5.8851
Step 2150: loss = 6.0041
  [Val @ step 2150]: train=6.0041, val=5.9226
Step 2200: loss = 5.9592
  [Val @ step 2200]: train=5.9592, val=5.9336
Step 2250: loss = 5.9022
  [Val @ step 2250]: train=5.9022, val=5.9284
Step 2300: loss = 5.8974
  [Val @ step 2300]: train=5.8974, val=5.9289
Step 2350: loss = 5.8453
  [Val @ step 2350]: train=5.8453, val=5.8976
Step 2400: loss = 5.8810
  [Val @ step 2400]: train=5.8810, val=5.9089
Step 2450: loss = 5.8113
  [Val @ step 2450]: train=5.8113, val=5.8946
Step 2500: loss = 5.8625
  [Val @ step 2500]: train=5.8625, val=5.8915
Step 2550: loss = 5.9166
  [Val @ step 2550]: train=5.9166, val=5.8919
Step 2600: loss = 5.8880
  [Val @ step 2600]: train=5.8880, val=5.8904
Step 2650: loss = 5.7641
  [Val @ step 2650]: train=5.7641, val=5.8711
Step 2700: loss = 5.9349
  [Val @ step 2700]: train=5.9349, val=5.8802
Step 2750: loss = 5.8649
  [Val @ step 2750]: train=5.8649, val=5.8605
Step 2800: loss = 5.7529
  [Val @ step 2800]: train=5.7529, val=5.8996
Step 2850: loss = 5.7872
  [Val @ step 2850]: train=5.7872, val=5.8690
Step 2900: loss = 5.8387
  [Val @ step 2900]: train=5.8387, val=5.8766
Step 2950: loss = 5.7410
  [Val @ step 2950]: train=5.7410, val=5.8752
Step 3000: loss = 5.8315
  [Val @ step 3000]: train=5.8315, val=5.8820
Step 3050: loss = 5.9354
  [Val @ step 3050]: train=5.9354, val=5.8630
Step 3100: loss = 5.8183
  [Val @ step 3100]: train=5.8183, val=5.8535
Step 3150: loss = 5.8176
  [Val @ step 3150]: train=5.8176, val=5.8427
Step 3200: loss = 5.7806
  [Val @ step 3200]: train=5.7806, val=5.8658
Step 3250: loss = 5.7814
  [Val @ step 3250]: train=5.7814, val=5.8696
Step 3300: loss = 5.7907
  [Val @ step 3300]: train=5.7907, val=5.8824
Step 3350: loss = 5.9092
  [Val @ step 3350]: train=5.9092, val=5.8822
Step 3400: loss = 5.8017
  [Val @ step 3400]: train=5.8017, val=5.8574
Step 3450: loss = 5.7953
  [Val @ step 3450]: train=5.7953, val=5.8626
Step 3500: loss = 5.9144
  [Val @ step 3500]: train=5.9144, val=5.8435
Step 3550: loss = 5.8716
  [Val @ step 3550]: train=5.8716, val=5.8250
Step 3600: loss = 5.7277
  [Val @ step 3600]: train=5.7277, val=5.8496
Step 3650: loss = 5.8214
  [Val @ step 3650]: train=5.8214, val=5.8569
Step 3700: loss = 5.9528
  [Val @ step 3700]: train=5.9528, val=5.8475
Step 3750: loss = 5.8520
  [Val @ step 3750]: train=5.8520, val=5.8246
Step 3800: loss = 5.8276
  [Val @ step 3800]: train=5.8276, val=5.8402
Step 3850: loss = 5.7945
  [Val @ step 3850]: train=5.7945, val=5.8457
Step 3900: loss = 5.8052
  [Val @ step 3900]: train=5.8052, val=5.8013
Step 3950: loss = 5.8700
  [Val @ step 3950]: train=5.8700, val=5.8123
Step 4000: loss = 5.7825
  [Val @ step 4000]: train=5.7825, val=5.8148
Step 4050: loss = 5.7900
  [Val @ step 4050]: train=5.7900, val=5.8038
Step 4100: loss = 5.9446
  [Val @ step 4100]: train=5.9446, val=5.8259
Step 4150: loss = 5.8546
  [Val @ step 4150]: train=5.8546, val=5.7936
Step 4200: loss = 5.6431
  [Val @ step 4200]: train=5.6431, val=5.7961
Step 4250: loss = 5.8305
  [Val @ step 4250]: train=5.8305, val=5.8635
Step 4300: loss = 5.8543
  [Val @ step 4300]: train=5.8543, val=5.8467
Step 4350: loss = 5.8324
  [Val @ step 4350]: train=5.8324, val=5.8106
Step 4400: loss = 5.6971
  [Val @ step 4400]: train=5.6971, val=5.8010
Step 4450: loss = 5.7724
  [Val @ step 4450]: train=5.7724, val=5.8165
Step 4500: loss = 5.8237
  [Val @ step 4500]: train=5.8237, val=5.8101
Step 4550: loss = 5.7560
  [Val @ step 4550]: train=5.7560, val=5.8169
Step 4600: loss = 5.7638
  [Val @ step 4600]: train=5.7638, val=5.7900
Step 4650: loss = 5.8402
  [Val @ step 4650]: train=5.8402, val=5.8432
Step 4700: loss = 5.8540
  [Val @ step 4700]: train=5.8540, val=5.8180
Step 4750: loss = 5.8332
  [Val @ step 4750]: train=5.8332, val=5.8113
Step 4800: loss = 5.7458
  [Val @ step 4800]: train=5.7458, val=5.7936
Step 4850: loss = 5.8913
  [Val @ step 4850]: train=5.8913, val=5.7898
Step 4900: loss = 5.8332
  [Val @ step 4900]: train=5.8332, val=5.7884
Step 4950: loss = 5.8760
  [Val @ step 4950]: train=5.8760, val=5.8192
--------------------------------------------------------------------------------
Computing final validation loss...
Validation loss: 5.8166

Results saved to time_to_target_stage1_20251225_225546.jsonl
Training time: 4774.5s
Throughput: 8,579 tokens/sec
Projections: 500/5000
nGPT Architectural Experiment: B3_vr0_cwd1
================================================================================
Model: 11 layers x 768 dim (matching modded-nanogpt)
Hypothesis: Baseline
================================================================================

Loading FineWeb data (subset for fast experiments)...
  Loading data/fineweb10B/fineweb_train_000001.bin...
  Loading data/fineweb10B/fineweb_train_000002.bin...
  Loading data/fineweb10B/fineweb_train_000003.bin...
  Loading data/fineweb10B/fineweb_train_000004.bin...
  Vocab size: 50257
  Train tokens: 400,001,536
  Val tokens: 100,000,384

Model config (modded-nanogpt size):
  vocab_size: 50257
  n_layer: 11
  n_embd: 768
  n_head: 6
  block_size: 128
Using GatedBlock (H3) with gate_init=0.0

Total parameters: 155,166,743

Optimizer: SimpleMuon (lr=0.003, momentum=0.95, geodesic=baseline, orthog=newton_schulz)
  2D weight params (Muon): 77,856,768
  1D params (Adam): 77,309,975

Starting training (5000 steps)...
--------------------------------------------------------------------------------
Step    0: loss = 10.9767
Step   50: loss = 7.2836
  [Val @ step 50]: train=7.2836, val=7.4493
Step  100: loss = 7.0364
  [Val @ step 100]: train=7.0364, val=7.0024
Step  150: loss = 6.9542
  [Val @ step 150]: train=6.9542, val=6.7605
Step  200: loss = 6.6743
  [Val @ step 200]: train=6.6743, val=6.6729
Step  250: loss = 6.5973
  [Val @ step 250]: train=6.5973, val=6.5381
Step  300: loss = 6.4478
  [Val @ step 300]: train=6.4478, val=6.5086
Step  350: loss = 6.4905
  [Val @ step 350]: train=6.4905, val=6.4474
Step  400: loss = 6.6385
  [Val @ step 400]: train=6.6385, val=6.4092
Step  450: loss = 6.4251
  [Val @ step 450]: train=6.4251, val=6.3589
Step  500: loss = 5.9851
  [Val @ step 500]: train=5.9851, val=6.3009
Step  550: loss = 6.2504
  [Val @ step 550]: train=6.2504, val=6.2812
Step  600: loss = 6.2359
  [Val @ step 600]: train=6.2359, val=6.2317
Step  650: loss = 6.2144
  [Val @ step 650]: train=6.2144, val=6.2897
Step  700: loss = 6.0513
  [Val @ step 700]: train=6.0513, val=6.2000
Step  750: loss = 6.3129
  [Val @ step 750]: train=6.3129, val=6.1667
Step  800: loss = 6.2747
  [Val @ step 800]: train=6.2747, val=6.1667
Step  850: loss = 6.1803
  [Val @ step 850]: train=6.1803, val=6.1493
Step  900: loss = 6.1361
  [Val @ step 900]: train=6.1361, val=6.1257
Step  950: loss = 6.0311
  [Val @ step 950]: train=6.0311, val=6.1188
Step 1000: loss = 6.0843
  [Val @ step 1000]: train=6.0843, val=6.1479
Step 1050: loss = 6.0280
  [Val @ step 1050]: train=6.0280, val=6.0934
Step 1100: loss = 6.2386
  [Val @ step 1100]: train=6.2386, val=6.1030
Step 1150: loss = 6.2298
  [Val @ step 1150]: train=6.2298, val=6.0377
Step 1200: loss = 6.0259
  [Val @ step 1200]: train=6.0259, val=6.0666
Step 1250: loss = 6.0502
  [Val @ step 1250]: train=6.0502, val=6.0502
Step 1300: loss = 5.9042
  [Val @ step 1300]: train=5.9042, val=6.0394
Step 1350: loss = 6.0161
  [Val @ step 1350]: train=6.0161, val=6.0109
Step 1400: loss = 6.1855
  [Val @ step 1400]: train=6.1855, val=5.9996
Step 1450: loss = 5.9845
  [Val @ step 1450]: train=5.9845, val=5.9755
Step 1500: loss = 5.9868
  [Val @ step 1500]: train=5.9868, val=6.0048
Step 1550: loss = 5.9413
  [Val @ step 1550]: train=5.9413, val=5.9984
Step 1600: loss = 5.8254
  [Val @ step 1600]: train=5.8254, val=5.9529
Step 1650: loss = 6.0417
  [Val @ step 1650]: train=6.0417, val=5.9556
Step 1700: loss = 6.0061
  [Val @ step 1700]: train=6.0061, val=5.9236
Step 1750: loss = 5.8881
  [Val @ step 1750]: train=5.8881, val=5.9687
Step 1800: loss = 6.0125
  [Val @ step 1800]: train=6.0125, val=5.9291
Step 1850: loss = 5.9500
  [Val @ step 1850]: train=5.9500, val=5.9030
Step 1900: loss = 5.9207
  [Val @ step 1900]: train=5.9207, val=5.9077
Step 1950: loss = 5.9411
  [Val @ step 1950]: train=5.9411, val=5.9007
Step 2000: loss = 6.0592
  [Val @ step 2000]: train=6.0592, val=5.9231
Step 2050: loss = 5.7795
  [Val @ step 2050]: train=5.7795, val=5.9221
Step 2100: loss = 5.9144
  [Val @ step 2100]: train=5.9144, val=5.8870
Step 2150: loss = 5.7609
  [Val @ step 2150]: train=5.7609, val=5.8686
Step 2200: loss = 5.7933
  [Val @ step 2200]: train=5.7933, val=5.8777
Step 2250: loss = 5.7882
  [Val @ step 2250]: train=5.7882, val=5.8952
Step 2300: loss = 5.8282
  [Val @ step 2300]: train=5.8282, val=5.8751
Step 2350: loss = 5.8522
  [Val @ step 2350]: train=5.8522, val=5.8604
Step 2400: loss = 5.8398
  [Val @ step 2400]: train=5.8398, val=5.8245
Step 2450: loss = 5.8468
  [Val @ step 2450]: train=5.8468, val=5.8565
Step 2500: loss = 5.7595
  [Val @ step 2500]: train=5.7595, val=5.8736
Step 2550: loss = 5.8206
  [Val @ step 2550]: train=5.8206, val=5.8305
Step 2600: loss = 5.9353
  [Val @ step 2600]: train=5.9353, val=5.9141
Step 2650: loss = 5.8326
  [Val @ step 2650]: train=5.8326, val=5.8383
Step 2700: loss = 5.9940
  [Val @ step 2700]: train=5.9940, val=5.8817
Step 2750: loss = 5.7266
  [Val @ step 2750]: train=5.7266, val=5.8595
Step 2800: loss = 5.7308
  [Val @ step 2800]: train=5.7308, val=5.8526
Step 2850: loss = 5.8842
  [Val @ step 2850]: train=5.8842, val=5.8759
Step 2900: loss = 5.8156
  [Val @ step 2900]: train=5.8156, val=5.8392
Step 2950: loss = 5.7122
  [Val @ step 2950]: train=5.7122, val=5.8589
Step 3000: loss = 5.9621
  [Val @ step 3000]: train=5.9621, val=5.8289
Step 3050: loss = 5.6599
  [Val @ step 3050]: train=5.6599, val=5.8334
Step 3100: loss = 5.7093
  [Val @ step 3100]: train=5.7093, val=5.8282
Step 3150: loss = 5.9041
  [Val @ step 3150]: train=5.9041, val=5.8164
Step 3200: loss = 5.6706
  [Val @ step 3200]: train=5.6706, val=5.8508
Step 3250: loss = 5.6633
  [Val @ step 3250]: train=5.6633, val=5.8105
Step 3300: loss = 5.9005
  [Val @ step 3300]: train=5.9005, val=5.8145
Step 3350: loss = 5.8661
  [Val @ step 3350]: train=5.8661, val=5.8120
Step 3400: loss = 5.7166
  [Val @ step 3400]: train=5.7166, val=5.7975
Step 3450: loss = 5.8692
  [Val @ step 3450]: train=5.8692, val=5.7790
Step 3500: loss = 5.7707
  [Val @ step 3500]: train=5.7707, val=5.7738
Step 3550: loss = 5.8086
  [Val @ step 3550]: train=5.8086, val=5.7913
Step 3600: loss = 5.9297
  [Val @ step 3600]: train=5.9297, val=5.8139
Step 3650: loss = 5.6866
  [Val @ step 3650]: train=5.6866, val=5.8123
Step 3700: loss = 5.7347
  [Val @ step 3700]: train=5.7347, val=5.7926
Step 3750: loss = 5.8861
  [Val @ step 3750]: train=5.8861, val=5.8096
Step 3800: loss = 5.8300
  [Val @ step 3800]: train=5.8300, val=5.7636
Step 3850: loss = 5.7833
  [Val @ step 3850]: train=5.7833, val=5.7602
Step 3900: loss = 5.9042
  [Val @ step 3900]: train=5.9042, val=5.7708
Step 3950: loss = 5.6918
  [Val @ step 3950]: train=5.6918, val=5.7645
Step 4000: loss = 5.5542
  [Val @ step 4000]: train=5.5542, val=5.7865
Step 4050: loss = 5.8818
  [Val @ step 4050]: train=5.8818, val=5.8109
Step 4100: loss = 5.6681
  [Val @ step 4100]: train=5.6681, val=5.7923
Step 4150: loss = 5.8298
  [Val @ step 4150]: train=5.8298, val=5.8080
Step 4200: loss = 5.9300
  [Val @ step 4200]: train=5.9300, val=5.7961
Step 4250: loss = 5.7385
  [Val @ step 4250]: train=5.7385, val=5.7653
Step 4300: loss = 5.6695
  [Val @ step 4300]: train=5.6695, val=5.7839
Step 4350: loss = 5.6530
  [Val @ step 4350]: train=5.6530, val=5.7785
Step 4400: loss = 5.6391
  [Val @ step 4400]: train=5.6391, val=5.7502
Step 4450: loss = 5.7290
  [Val @ step 4450]: train=5.7290, val=5.7856
Step 4500: loss = 5.7916
  [Val @ step 4500]: train=5.7916, val=5.7362
Step 4550: loss = 5.5465
  [Val @ step 4550]: train=5.5465, val=5.7610
Step 4600: loss = 5.8521
  [Val @ step 4600]: train=5.8521, val=5.7516
Step 4650: loss = 5.6536
  [Val @ step 4650]: train=5.6536, val=5.7762
Step 4700: loss = 5.8273
  [Val @ step 4700]: train=5.8273, val=5.7687
Step 4750: loss = 5.8827
  [Val @ step 4750]: train=5.8827, val=5.7521
Step 4800: loss = 5.6544
  [Val @ step 4800]: train=5.6544, val=5.7459
Step 4850: loss = 5.7955
  [Val @ step 4850]: train=5.7955, val=5.7359
Step 4900: loss = 5.7996
  [Val @ step 4900]: train=5.7996, val=5.7298
Step 4950: loss = 5.7324
  [Val @ step 4950]: train=5.7324, val=5.7258
--------------------------------------------------------------------------------
Computing final validation loss...
Validation loss: 5.7213

Results saved to time_to_target_stage1_20251225_225546.jsonl
Training time: 3607.9s
Throughput: 5,676 tokens/sec
Projections: 715/5000
nGPT Architectural Experiment: B4_vr1_cwd1
================================================================================
Model: 11 layers x 768 dim (matching modded-nanogpt)
Hypothesis: Baseline
================================================================================

Loading FineWeb data (subset for fast experiments)...
  Loading data/fineweb10B/fineweb_train_000001.bin...
  Loading data/fineweb10B/fineweb_train_000002.bin...
  Loading data/fineweb10B/fineweb_train_000003.bin...
  Loading data/fineweb10B/fineweb_train_000004.bin...
  Vocab size: 50257
  Train tokens: 400,001,536
  Val tokens: 100,000,384

Model config (modded-nanogpt size):
  vocab_size: 50257
  n_layer: 11
  n_embd: 768
  n_head: 6
  block_size: 128
Using GatedBlock (H3) with gate_init=0.0

Total parameters: 155,166,743

Optimizer: SimpleMuon (lr=0.003, momentum=0.95, geodesic=baseline, orthog=newton_schulz)
  2D weight params (Muon): 77,856,768
  1D params (Adam): 77,309,975

Starting training (5000 steps)...
--------------------------------------------------------------------------------
Step    0: loss = 10.9597
Step   50: loss = 7.5382
  [Val @ step 50]: train=7.5382, val=7.3608
Step  100: loss = 6.9491
  [Val @ step 100]: train=6.9491, val=6.9434
Step  150: loss = 6.7336
  [Val @ step 150]: train=6.7336, val=6.8005
Step  200: loss = 6.9377
  [Val @ step 200]: train=6.9377, val=6.6703
Step  250: loss = 6.5205
  [Val @ step 250]: train=6.5205, val=6.5617
Step  300: loss = 6.5730
  [Val @ step 300]: train=6.5730, val=6.4762
Step  350: loss = 6.2286
  [Val @ step 350]: train=6.2286, val=6.4681
Step  400: loss = 6.3894
  [Val @ step 400]: train=6.3894, val=6.3951
Step  450: loss = 6.3469
  [Val @ step 450]: train=6.3469, val=6.3820
Step  500: loss = 6.3636
  [Val @ step 500]: train=6.3636, val=6.3948
Step  550: loss = 6.3081
  [Val @ step 550]: train=6.3081, val=6.3108
Step  600: loss = 6.3821
  [Val @ step 600]: train=6.3821, val=6.2576
Step  650: loss = 6.0719
  [Val @ step 650]: train=6.0719, val=6.2304
Step  700: loss = 6.2972
  [Val @ step 700]: train=6.2972, val=6.2469
Step  750: loss = 5.9429
  [Val @ step 750]: train=5.9429, val=6.1773
Step  800: loss = 6.1326
  [Val @ step 800]: train=6.1326, val=6.1890
Step  850: loss = 6.3334
  [Val @ step 850]: train=6.3334, val=6.1565
Step  900: loss = 6.2731
  [Val @ step 900]: train=6.2731, val=6.1230
Step  950: loss = 6.0921
  [Val @ step 950]: train=6.0921, val=6.1125
Step 1000: loss = 6.0873
  [Val @ step 1000]: train=6.0873, val=6.1539
Step 1050: loss = 6.1000
  [Val @ step 1050]: train=6.1000, val=6.0971
Step 1100: loss = 6.0332
  [Val @ step 1100]: train=6.0332, val=6.1136
Step 1150: loss = 6.0440
  [Val @ step 1150]: train=6.0440, val=6.0503
Step 1200: loss = 6.0148
  [Val @ step 1200]: train=6.0148, val=6.0590
Step 1250: loss = 5.9676
  [Val @ step 1250]: train=5.9676, val=5.9686
Step 1300: loss = 5.9791
  [Val @ step 1300]: train=5.9791, val=6.0489
Step 1350: loss = 5.8529
  [Val @ step 1350]: train=5.8529, val=6.0099
Step 1400: loss = 5.9186
  [Val @ step 1400]: train=5.9186, val=6.0401
Step 1450: loss = 6.0813
  [Val @ step 1450]: train=6.0813, val=5.9585
Step 1500: loss = 5.9333
  [Val @ step 1500]: train=5.9333, val=6.0007
Step 1550: loss = 6.0113
  [Val @ step 1550]: train=6.0113, val=5.9897
Step 1600: loss = 5.9033
  [Val @ step 1600]: train=5.9033, val=5.9542
Step 1650: loss = 5.8263
  [Val @ step 1650]: train=5.8263, val=6.0064
Step 1700: loss = 5.8052
  [Val @ step 1700]: train=5.8052, val=5.9483
Step 1750: loss = 5.8616
  [Val @ step 1750]: train=5.8616, val=5.9387
Step 1800: loss = 5.8962
  [Val @ step 1800]: train=5.8962, val=5.9350
Step 1850: loss = 6.0470
  [Val @ step 1850]: train=6.0470, val=5.9211
Step 1900: loss = 5.9771
  [Val @ step 1900]: train=5.9771, val=5.9219
Step 1950: loss = 5.9698
  [Val @ step 1950]: train=5.9698, val=5.9274
Step 2000: loss = 5.9829
  [Val @ step 2000]: train=5.9829, val=5.9391
Step 2050: loss = 5.9459
  [Val @ step 2050]: train=5.9459, val=5.9387
Step 2100: loss = 5.8105
  [Val @ step 2100]: train=5.8105, val=5.9146
Step 2150: loss = 5.8407
  [Val @ step 2150]: train=5.8407, val=5.9088
Step 2200: loss = 5.8930
  [Val @ step 2200]: train=5.8930, val=5.8845
Step 2250: loss = 6.0188
  [Val @ step 2250]: train=6.0188, val=5.9341
Step 2300: loss = 5.8791
  [Val @ step 2300]: train=5.8791, val=5.8936
Step 2350: loss = 5.7713
  [Val @ step 2350]: train=5.7713, val=5.8714
Step 2400: loss = 5.7308
  [Val @ step 2400]: train=5.7308, val=5.8728
Step 2450: loss = 5.8237
  [Val @ step 2450]: train=5.8237, val=5.8990
Step 2500: loss = 5.9695
  [Val @ step 2500]: train=5.9695, val=5.8757
Step 2550: loss = 5.9750
  [Val @ step 2550]: train=5.9750, val=5.8336
Step 2600: loss = 5.6782
  [Val @ step 2600]: train=5.6782, val=5.8397
Step 2650: loss = 5.9387
  [Val @ step 2650]: train=5.9387, val=5.8999
Step 2700: loss = 5.8393
  [Val @ step 2700]: train=5.8393, val=5.8616
Step 2750: loss = 5.7366
  [Val @ step 2750]: train=5.7366, val=5.8421
Step 2800: loss = 5.8803
  [Val @ step 2800]: train=5.8803, val=5.8282
Step 2850: loss = 5.9065
  [Val @ step 2850]: train=5.9065, val=5.8293
Step 2900: loss = 5.9784
  [Val @ step 2900]: train=5.9784, val=5.8560
Step 2950: loss = 5.7936
  [Val @ step 2950]: train=5.7936, val=5.8345
Step 3000: loss = 5.8745
  [Val @ step 3000]: train=5.8745, val=5.8258
Step 3050: loss = 5.8071
  [Val @ step 3050]: train=5.8071, val=5.8166
Step 3100: loss = 5.9098
  [Val @ step 3100]: train=5.9098, val=5.8366
Step 3150: loss = 5.8811
  [Val @ step 3150]: train=5.8811, val=5.8004
Step 3200: loss = 5.7638
  [Val @ step 3200]: train=5.7638, val=5.8177
Step 3250: loss = 5.7460
  [Val @ step 3250]: train=5.7460, val=5.8157
Step 3300: loss = 5.7690
  [Val @ step 3300]: train=5.7690, val=5.8181
Step 3350: loss = 5.7646
  [Val @ step 3350]: train=5.7646, val=5.8111
Step 3400: loss = 5.7492
  [Val @ step 3400]: train=5.7492, val=5.7793
Step 3450: loss = 5.9555
  [Val @ step 3450]: train=5.9555, val=5.7570
Step 3500: loss = 5.6320
  [Val @ step 3500]: train=5.6320, val=5.7736
Step 3550: loss = 5.7688
  [Val @ step 3550]: train=5.7688, val=5.7941
Step 3600: loss = 5.8101
  [Val @ step 3600]: train=5.8101, val=5.7856
Step 3650: loss = 5.8256
  [Val @ step 3650]: train=5.8256, val=5.8138
Step 3700: loss = 5.7578
  [Val @ step 3700]: train=5.7578, val=5.8029
Step 3750: loss = 5.8383
  [Val @ step 3750]: train=5.8383, val=5.7539
Step 3800: loss = 5.8704
  [Val @ step 3800]: train=5.8704, val=5.7674
Step 3850: loss = 5.6755
  [Val @ step 3850]: train=5.6755, val=5.7954
Step 3900: loss = 5.7784
  [Val @ step 3900]: train=5.7784, val=5.7312
Step 3950: loss = 5.6712
  [Val @ step 3950]: train=5.6712, val=5.7307
Step 4000: loss = 5.7033
  [Val @ step 4000]: train=5.7033, val=5.7632
Step 4050: loss = 5.5284
  [Val @ step 4050]: train=5.5284, val=5.7548
Step 4100: loss = 5.7834
  [Val @ step 4100]: train=5.7834, val=5.7293
Step 4150: loss = 5.4932
  [Val @ step 4150]: train=5.4932, val=5.7348
Step 4200: loss = 5.6806
  [Val @ step 4200]: train=5.6806, val=5.7461
Step 4250: loss = 5.8521
  [Val @ step 4250]: train=5.8521, val=5.7370
Step 4300: loss = 5.6526
  [Val @ step 4300]: train=5.6526, val=5.7177
Step 4350: loss = 5.7474
  [Val @ step 4350]: train=5.7474, val=5.7380
Step 4400: loss = 5.6548
  [Val @ step 4400]: train=5.6548, val=5.7350
Step 4450: loss = 5.7666
  [Val @ step 4450]: train=5.7666, val=5.7411
Step 4500: loss = 5.7926
  [Val @ step 4500]: train=5.7926, val=5.7322
Step 4550: loss = 5.7691
  [Val @ step 4550]: train=5.7691, val=5.7396
Step 4600: loss = 5.4913
  [Val @ step 4600]: train=5.4913, val=5.7325
Step 4650: loss = 5.7083
  [Val @ step 4650]: train=5.7083, val=5.7657
Step 4700: loss = 5.8610
  [Val @ step 4700]: train=5.8610, val=5.7360
Step 4750: loss = 5.7797
  [Val @ step 4750]: train=5.7797, val=5.7074
Step 4800: loss = 5.6640
  [Val @ step 4800]: train=5.6640, val=5.7543
Step 4850: loss = 5.8626
  [Val @ step 4850]: train=5.8626, val=5.7196
Step 4900: loss = 5.7990
  [Val @ step 4900]: train=5.7990, val=5.7130
Step 4950: loss = 5.7799
  [Val @ step 4950]: train=5.7799, val=5.7020
--------------------------------------------------------------------------------
Computing final validation loss...
Validation loss: 5.7142

Results saved to time_to_target_stage1_20251225_225546.jsonl
Training time: 3819.5s
Throughput: 5,362 tokens/sec
Projections: 715/5000
nGPT Architectural Experiment: C_lazy3
================================================================================
Model: 11 layers x 768 dim (matching modded-nanogpt)
Hypothesis: Baseline
================================================================================

Loading FineWeb data (subset for fast experiments)...
  Loading data/fineweb10B/fineweb_train_000001.bin...
  Loading data/fineweb10B/fineweb_train_000002.bin...
  Loading data/fineweb10B/fineweb_train_000003.bin...
  Loading data/fineweb10B/fineweb_train_000004.bin...
  Vocab size: 50257
  Train tokens: 400,001,536
  Val tokens: 100,000,384

Model config (modded-nanogpt size):
  vocab_size: 50257
  n_layer: 11
  n_embd: 768
  n_head: 6
  block_size: 128
Using GatedBlock (H3) with gate_init=0.0

Total parameters: 155,166,743

Optimizer: SimpleMuon (lr=0.003, momentum=0.95, geodesic=baseline, orthog=newton_schulz)
  2D weight params (Muon): 77,856,768
  1D params (Adam): 77,309,975

Starting training (5000 steps)...
--------------------------------------------------------------------------------
Step    0: loss = 10.9906
Step   50: loss = 7.6124
  [Val @ step 50]: train=7.6124, val=7.3916
Step  100: loss = 6.9950
  [Val @ step 100]: train=6.9950, val=7.0143
Step  150: loss = 6.9870
  [Val @ step 150]: train=6.9870, val=6.7598
Step  200: loss = 6.6914
  [Val @ step 200]: train=6.6914, val=6.6199
Step  250: loss = 6.4033
  [Val @ step 250]: train=6.4033, val=6.5833
Step  300: loss = 6.5030
  [Val @ step 300]: train=6.5030, val=6.4484
Step  350: loss = 6.6580
  [Val @ step 350]: train=6.6580, val=6.4983
Step  400: loss = 6.5271
  [Val @ step 400]: train=6.5271, val=6.3623
Step  450: loss = 6.3168
  [Val @ step 450]: train=6.3168, val=6.3538
Step  500: loss = 6.2703
  [Val @ step 500]: train=6.2703, val=6.3536
Step  550: loss = 6.1106
  [Val @ step 550]: train=6.1106, val=6.3106
Step  600: loss = 6.2633
  [Val @ step 600]: train=6.2633, val=6.2647
Step  650: loss = 6.2668
  [Val @ step 650]: train=6.2668, val=6.2247
Step  700: loss = 6.3398
  [Val @ step 700]: train=6.3398, val=6.2489
Step  750: loss = 6.1808
  [Val @ step 750]: train=6.1808, val=6.1808
Step  800: loss = 6.3436
  [Val @ step 800]: train=6.3436, val=6.1732
Step  850: loss = 6.1603
  [Val @ step 850]: train=6.1603, val=6.1634
Step  900: loss = 6.0671
  [Val @ step 900]: train=6.0671, val=6.1680
Step  950: loss = 6.0268
  [Val @ step 950]: train=6.0268, val=6.1162
Step 1000: loss = 6.0422
  [Val @ step 1000]: train=6.0422, val=6.0846
Step 1050: loss = 6.2125
  [Val @ step 1050]: train=6.2125, val=6.0762
Step 1100: loss = 6.1905
  [Val @ step 1100]: train=6.1905, val=6.0825
Step 1150: loss = 6.0371
  [Val @ step 1150]: train=6.0371, val=6.0747
Step 1200: loss = 5.7503
  [Val @ step 1200]: train=5.7503, val=6.1363
Step 1250: loss = 5.9785
  [Val @ step 1250]: train=5.9785, val=6.0154
Step 1300: loss = 5.8970
  [Val @ step 1300]: train=5.8970, val=6.0186
Step 1350: loss = 6.0873
  [Val @ step 1350]: train=6.0873, val=6.0619
Step 1400: loss = 6.0720
  [Val @ step 1400]: train=6.0720, val=6.0404
Step 1450: loss = 5.9341
  [Val @ step 1450]: train=5.9341, val=6.0021
Step 1500: loss = 5.9251
  [Val @ step 1500]: train=5.9251, val=6.0132
Step 1550: loss = 5.9553
  [Val @ step 1550]: train=5.9553, val=5.9399
Step 1600: loss = 5.8771
  [Val @ step 1600]: train=5.8771, val=5.9702
Step 1650: loss = 5.9452
  [Val @ step 1650]: train=5.9452, val=5.9557
Step 1700: loss = 5.9345
  [Val @ step 1700]: train=5.9345, val=5.9676
Step 1750: loss = 5.7657
  [Val @ step 1750]: train=5.7657, val=5.9563
Step 1800: loss = 5.8388
  [Val @ step 1800]: train=5.8388, val=5.9271
Step 1850: loss = 5.8855
  [Val @ step 1850]: train=5.8855, val=5.9316
Step 1900: loss = 6.0536
  [Val @ step 1900]: train=6.0536, val=5.9578
Step 1950: loss = 6.0627
  [Val @ step 1950]: train=6.0627, val=5.9338
Step 2000: loss = 5.7376
  [Val @ step 2000]: train=5.7376, val=5.8932
Step 2050: loss = 5.7661
  [Val @ step 2050]: train=5.7661, val=5.8909
Step 2100: loss = 5.9256
  [Val @ step 2100]: train=5.9256, val=5.8602
Step 2150: loss = 5.9155
  [Val @ step 2150]: train=5.9155, val=5.8919
Step 2200: loss = 6.1013
  [Val @ step 2200]: train=6.1013, val=5.9053
Step 2250: loss = 5.9435
  [Val @ step 2250]: train=5.9435, val=5.8655
Step 2300: loss = 5.8077
  [Val @ step 2300]: train=5.8077, val=5.8762
Step 2350: loss = 5.8868
  [Val @ step 2350]: train=5.8868, val=5.8882
Step 2400: loss = 5.9335
  [Val @ step 2400]: train=5.9335, val=5.8828
Step 2450: loss = 5.8488
  [Val @ step 2450]: train=5.8488, val=5.8499
Step 2500: loss = 5.7525
  [Val @ step 2500]: train=5.7525, val=5.9113
Step 2550: loss = 5.7750
  [Val @ step 2550]: train=5.7750, val=5.8201
Step 2600: loss = 5.8516
  [Val @ step 2600]: train=5.8516, val=5.8454
Step 2650: loss = 5.7879
  [Val @ step 2650]: train=5.7879, val=5.8378
Step 2700: loss = 5.8298
  [Val @ step 2700]: train=5.8298, val=5.8300
Step 2750: loss = 5.9535
  [Val @ step 2750]: train=5.9535, val=5.8623
Step 2800: loss = 5.7802
  [Val @ step 2800]: train=5.7802, val=5.8492
Step 2850: loss = 5.8753
  [Val @ step 2850]: train=5.8753, val=5.8492
Step 2900: loss = 5.7792
  [Val @ step 2900]: train=5.7792, val=5.8421
Step 2950: loss = 5.7727
  [Val @ step 2950]: train=5.7727, val=5.8132
Step 3000: loss = 5.9770
  [Val @ step 3000]: train=5.9770, val=5.8200
Step 3050: loss = 5.7402
  [Val @ step 3050]: train=5.7402, val=5.8109
Step 3100: loss = 5.6873
  [Val @ step 3100]: train=5.6873, val=5.8139
Step 3150: loss = 5.7818
  [Val @ step 3150]: train=5.7818, val=5.8023
Step 3200: loss = 5.8826
  [Val @ step 3200]: train=5.8826, val=5.8002
Step 3250: loss = 5.9197
  [Val @ step 3250]: train=5.9197, val=5.8032
Step 3300: loss = 5.6581
  [Val @ step 3300]: train=5.6581, val=5.8258
Step 3350: loss = 5.8317
  [Val @ step 3350]: train=5.8317, val=5.7907
Step 3400: loss = 5.9300
  [Val @ step 3400]: train=5.9300, val=5.7857
Step 3450: loss = 5.7609
  [Val @ step 3450]: train=5.7609, val=5.7749
Step 3500: loss = 5.7293
  [Val @ step 3500]: train=5.7293, val=5.7918
Step 3550: loss = 5.7631
  [Val @ step 3550]: train=5.7631, val=5.8166
Step 3600: loss = 5.8518
  [Val @ step 3600]: train=5.8518, val=5.7961
Step 3650: loss = 5.8455
  [Val @ step 3650]: train=5.8455, val=5.8199
Step 3700: loss = 5.7804
  [Val @ step 3700]: train=5.7804, val=5.7795
Step 3750: loss = 5.9277
  [Val @ step 3750]: train=5.9277, val=5.7717
Step 3800: loss = 5.9028
  [Val @ step 3800]: train=5.9028, val=5.7895
Step 3850: loss = 5.8908
  [Val @ step 3850]: train=5.8908, val=5.7813
Step 3900: loss = 5.7800
  [Val @ step 3900]: train=5.7800, val=5.7735
Step 3950: loss = 5.6373
  [Val @ step 3950]: train=5.6373, val=5.7960
Step 4000: loss = 5.9148
  [Val @ step 4000]: train=5.9148, val=5.8018
Step 4050: loss = 5.8069
  [Val @ step 4050]: train=5.8069, val=5.8011
Step 4100: loss = 5.7247
  [Val @ step 4100]: train=5.7247, val=5.7751
Step 4150: loss = 5.6473
  [Val @ step 4150]: train=5.6473, val=5.7297
Step 4200: loss = 5.7690
  [Val @ step 4200]: train=5.7690, val=5.7731
Step 4250: loss = 5.7041
  [Val @ step 4250]: train=5.7041, val=5.7399
Step 4300: loss = 5.6266
  [Val @ step 4300]: train=5.6266, val=5.7455
Step 4350: loss = 5.7694
  [Val @ step 4350]: train=5.7694, val=5.7647
Step 4400: loss = 5.7501
  [Val @ step 4400]: train=5.7501, val=5.7518
Step 4450: loss = 5.9669
  [Val @ step 4450]: train=5.9669, val=5.7324
Step 4500: loss = 5.7823
  [Val @ step 4500]: train=5.7823, val=5.7383
Step 4550: loss = 5.6993
  [Val @ step 4550]: train=5.6993, val=5.7506
Step 4600: loss = 5.6915
  [Val @ step 4600]: train=5.6915, val=5.7566
Step 4650: loss = 5.8008
  [Val @ step 4650]: train=5.8008, val=5.7444
Step 4700: loss = 5.8780
  [Val @ step 4700]: train=5.8780, val=5.7679
Step 4750: loss = 5.5915
  [Val @ step 4750]: train=5.5915, val=5.7360
Step 4800: loss = 5.7030
  [Val @ step 4800]: train=5.7030, val=5.7368
Step 4850: loss = 5.7445
  [Val @ step 4850]: train=5.7445, val=5.7340
Step 4900: loss = 5.6162
  [Val @ step 4900]: train=5.6162, val=5.7143
Step 4950: loss = 5.5692
  [Val @ step 4950]: train=5.5692, val=5.7324
--------------------------------------------------------------------------------
Computing final validation loss...
Validation loss: 5.7345

Results saved to time_to_target_stage1_20251225_225546.jsonl
Training time: 3727.1s
Throughput: 5,495 tokens/sec
Projections: 1667/5000
nGPT Architectural Experiment: C_lazy5
================================================================================
Model: 11 layers x 768 dim (matching modded-nanogpt)
Hypothesis: Baseline
================================================================================

Loading FineWeb data (subset for fast experiments)...
  Loading data/fineweb10B/fineweb_train_000001.bin...
  Loading data/fineweb10B/fineweb_train_000002.bin...
  Loading data/fineweb10B/fineweb_train_000003.bin...
  Loading data/fineweb10B/fineweb_train_000004.bin...
  Vocab size: 50257
  Train tokens: 400,001,536
  Val tokens: 100,000,384

Model config (modded-nanogpt size):
  vocab_size: 50257
  n_layer: 11
  n_embd: 768
  n_head: 6
  block_size: 128
Using GatedBlock (H3) with gate_init=0.0

Total parameters: 155,166,743

Optimizer: SimpleMuon (lr=0.003, momentum=0.95, geodesic=baseline, orthog=newton_schulz)
  2D weight params (Muon): 77,856,768
  1D params (Adam): 77,309,975

Starting training (5000 steps)...
--------------------------------------------------------------------------------
Step    0: loss = 10.9843
Step   50: loss = 7.7215
  [Val @ step 50]: train=7.7215, val=7.3442
Step  100: loss = 6.8089
  [Val @ step 100]: train=6.8089, val=6.9797
Step  150: loss = 7.0132
  [Val @ step 150]: train=7.0132, val=6.7522
Step  200: loss = 6.6763
  [Val @ step 200]: train=6.6763, val=6.6520
Step  250: loss = 6.6799
  [Val @ step 250]: train=6.6799, val=6.5791
Step  300: loss = 6.3829
  [Val @ step 300]: train=6.3829, val=6.5184
Step  350: loss = 6.5386
  [Val @ step 350]: train=6.5386, val=6.4642
Step  400: loss = 6.4659
  [Val @ step 400]: train=6.4659, val=6.3946
Step  450: loss = 6.4580
  [Val @ step 450]: train=6.4580, val=6.3461
Step  500: loss = 6.3663
  [Val @ step 500]: train=6.3663, val=6.3207
Step  550: loss = 6.2867
  [Val @ step 550]: train=6.2867, val=6.2905
Step  600: loss = 6.2511
  [Val @ step 600]: train=6.2511, val=6.2347
Step  650: loss = 6.3894
  [Val @ step 650]: train=6.3894, val=6.2153
Step  700: loss = 6.2122
  [Val @ step 700]: train=6.2122, val=6.2158
Step  750: loss = 6.2092
  [Val @ step 750]: train=6.2092, val=6.1612
Step  800: loss = 6.2679
  [Val @ step 800]: train=6.2679, val=6.1245
Step  850: loss = 6.3278
  [Val @ step 850]: train=6.3278, val=6.1341
Step  900: loss = 6.1037
  [Val @ step 900]: train=6.1037, val=6.1294
Step  950: loss = 5.9403
  [Val @ step 950]: train=5.9403, val=6.1439
Step 1000: loss = 6.1945
  [Val @ step 1000]: train=6.1945, val=6.1230
Step 1050: loss = 6.0718
  [Val @ step 1050]: train=6.0718, val=6.0870
Step 1100: loss = 6.0023
  [Val @ step 1100]: train=6.0023, val=6.0736
Step 1150: loss = 6.0807
  [Val @ step 1150]: train=6.0807, val=6.0973
Step 1200: loss = 6.0363
  [Val @ step 1200]: train=6.0363, val=6.1099
Step 1250: loss = 6.0686
  [Val @ step 1250]: train=6.0686, val=6.0622
Step 1300: loss = 6.0309
  [Val @ step 1300]: train=6.0309, val=6.0759
Step 1350: loss = 6.1246
  [Val @ step 1350]: train=6.1246, val=6.0525
Step 1400: loss = 5.8658
  [Val @ step 1400]: train=5.8658, val=6.0169
Step 1450: loss = 5.8695
  [Val @ step 1450]: train=5.8695, val=6.0008
Step 1500: loss = 6.2304
  [Val @ step 1500]: train=6.2304, val=5.9901
Step 1550: loss = 6.1353
  [Val @ step 1550]: train=6.1353, val=5.9887
Step 1600: loss = 5.8694
  [Val @ step 1600]: train=5.8694, val=5.9643
Step 1650: loss = 6.1938
  [Val @ step 1650]: train=6.1938, val=5.9582
Step 1700: loss = 5.8428
  [Val @ step 1700]: train=5.8428, val=5.9874
Step 1750: loss = 5.7107
  [Val @ step 1750]: train=5.7107, val=5.9608
Step 1800: loss = 5.7603
  [Val @ step 1800]: train=5.7603, val=5.9576
Step 1850: loss = 5.7881
  [Val @ step 1850]: train=5.7881, val=5.9203
Step 1900: loss = 6.0979
  [Val @ step 1900]: train=6.0979, val=5.9811
Step 1950: loss = 6.0942
  [Val @ step 1950]: train=6.0942, val=5.9450
Step 2000: loss = 5.9461
  [Val @ step 2000]: train=5.9461, val=5.9112
Step 2050: loss = 5.9068
  [Val @ step 2050]: train=5.9068, val=5.9153
Step 2100: loss = 6.1807
  [Val @ step 2100]: train=6.1807, val=5.8997
Step 2150: loss = 5.9148
  [Val @ step 2150]: train=5.9148, val=5.9045
Step 2200: loss = 6.0018
  [Val @ step 2200]: train=6.0018, val=5.8675
Step 2250: loss = 5.8180
  [Val @ step 2250]: train=5.8180, val=5.8594
Step 2300: loss = 5.7547
  [Val @ step 2300]: train=5.7547, val=5.8722
Step 2350: loss = 5.9504
  [Val @ step 2350]: train=5.9504, val=5.8421
Step 2400: loss = 5.7734
  [Val @ step 2400]: train=5.7734, val=5.8790
Step 2450: loss = 5.6284
  [Val @ step 2450]: train=5.6284, val=5.8935
Step 2500: loss = 5.7521
  [Val @ step 2500]: train=5.7521, val=5.8615
Step 2550: loss = 5.7561
  [Val @ step 2550]: train=5.7561, val=5.8304
Step 2600: loss = 5.8447
  [Val @ step 2600]: train=5.8447, val=5.8164
Step 2650: loss = 5.8057
  [Val @ step 2650]: train=5.8057, val=5.8731
Step 2700: loss = 5.7925
  [Val @ step 2700]: train=5.7925, val=5.8714
Step 2750: loss = 5.6294
  [Val @ step 2750]: train=5.6294, val=5.8043
Step 2800: loss = 5.9495
  [Val @ step 2800]: train=5.9495, val=5.8357
Step 2850: loss = 5.6602
  [Val @ step 2850]: train=5.6602, val=5.8264
Step 2900: loss = 5.5582
  [Val @ step 2900]: train=5.5582, val=5.8365
Step 2950: loss = 5.8356
  [Val @ step 2950]: train=5.8356, val=5.8411
Step 3000: loss = 5.8413
  [Val @ step 3000]: train=5.8413, val=5.8234
Step 3050: loss = 5.7959
  [Val @ step 3050]: train=5.7959, val=5.8746
Step 3100: loss = 5.8788
  [Val @ step 3100]: train=5.8788, val=5.8244
Step 3150: loss = 6.0099
  [Val @ step 3150]: train=6.0099, val=5.8180
Step 3200: loss = 5.8216
  [Val @ step 3200]: train=5.8216, val=5.8362
Step 3250: loss = 5.9169
  [Val @ step 3250]: train=5.9169, val=5.8472
Step 3300: loss = 5.8005
  [Val @ step 3300]: train=5.8005, val=5.7965
Step 3350: loss = 5.8932
  [Val @ step 3350]: train=5.8932, val=5.7944
Step 3400: loss = 5.7011
  [Val @ step 3400]: train=5.7011, val=5.8161
Step 3450: loss = 5.5753
  [Val @ step 3450]: train=5.5753, val=5.8310
Step 3500: loss = 5.7864
  [Val @ step 3500]: train=5.7864, val=5.7835
Step 3550: loss = 5.8555
  [Val @ step 3550]: train=5.8555, val=5.8471
Step 3600: loss = 5.7716
  [Val @ step 3600]: train=5.7716, val=5.7808
Step 3650: loss = 5.7450
  [Val @ step 3650]: train=5.7450, val=5.7894
Step 3700: loss = 5.6768
  [Val @ step 3700]: train=5.6768, val=5.8018
Step 3750: loss = 5.7352
  [Val @ step 3750]: train=5.7352, val=5.7924
Step 3800: loss = 5.7307
  [Val @ step 3800]: train=5.7307, val=5.7912
Step 3850: loss = 5.8897
  [Val @ step 3850]: train=5.8897, val=5.7748
Step 3900: loss = 5.8308
  [Val @ step 3900]: train=5.8308, val=5.7917
Step 3950: loss = 5.7208
  [Val @ step 3950]: train=5.7208, val=5.7816
Step 4000: loss = 5.8094
  [Val @ step 4000]: train=5.8094, val=5.7682
Step 4050: loss = 5.7924
  [Val @ step 4050]: train=5.7924, val=5.7759
Step 4100: loss = 5.6668
  [Val @ step 4100]: train=5.6668, val=5.7975
Step 4150: loss = 5.7585
  [Val @ step 4150]: train=5.7585, val=5.7691
Step 4200: loss = 5.9364
  [Val @ step 4200]: train=5.9364, val=5.7942
Step 4250: loss = 5.9376
  [Val @ step 4250]: train=5.9376, val=5.7720
Step 4300: loss = 5.6833
  [Val @ step 4300]: train=5.6833, val=5.7702
Step 4350: loss = 5.6103
  [Val @ step 4350]: train=5.6103, val=5.7705
Step 4400: loss = 5.8433
  [Val @ step 4400]: train=5.8433, val=5.7492
Step 4450: loss = 5.7754
  [Val @ step 4450]: train=5.7754, val=5.7479
Step 4500: loss = 5.6841
  [Val @ step 4500]: train=5.6841, val=5.7857
Step 4550: loss = 5.7341
  [Val @ step 4550]: train=5.7341, val=5.7811
Step 4600: loss = 5.7780
  [Val @ step 4600]: train=5.7780, val=5.7671
Step 4650: loss = 5.9485
  [Val @ step 4650]: train=5.9485, val=5.7776
Step 4700: loss = 5.7202
  [Val @ step 4700]: train=5.7202, val=5.7514
Step 4750: loss = 5.6372
  [Val @ step 4750]: train=5.6372, val=5.7471
Step 4800: loss = 5.8013
  [Val @ step 4800]: train=5.8013, val=5.7884
Step 4850: loss = 5.6759
  [Val @ step 4850]: train=5.6759, val=5.7276
Step 4900: loss = 5.4836
  [Val @ step 4900]: train=5.4836, val=5.7289
Step 4950: loss = 5.7060
  [Val @ step 4950]: train=5.7060, val=5.7556
--------------------------------------------------------------------------------
Computing final validation loss...
Validation loss: 5.7814

Results saved to time_to_target_stage1_20251225_225546.jsonl
Training time: 3796.3s
Throughput: 5,395 tokens/sec
Projections: 1000/5000
nGPT Architectural Experiment: C_lazy7
================================================================================
Model: 11 layers x 768 dim (matching modded-nanogpt)
Hypothesis: Baseline
================================================================================

Loading FineWeb data (subset for fast experiments)...
  Loading data/fineweb10B/fineweb_train_000001.bin...
  Loading data/fineweb10B/fineweb_train_000002.bin...
  Loading data/fineweb10B/fineweb_train_000003.bin...
  Loading data/fineweb10B/fineweb_train_000004.bin...
  Vocab size: 50257
  Train tokens: 400,001,536
  Val tokens: 100,000,384

Model config (modded-nanogpt size):
  vocab_size: 50257
  n_layer: 11
  n_embd: 768
  n_head: 6
  block_size: 128
Using GatedBlock (H3) with gate_init=0.0

Total parameters: 155,166,743

Optimizer: SimpleMuon (lr=0.003, momentum=0.95, geodesic=baseline, orthog=newton_schulz)
  2D weight params (Muon): 77,856,768
  1D params (Adam): 77,309,975

Starting training (5000 steps)...
--------------------------------------------------------------------------------
Step    0: loss = 10.9462
Step   50: loss = 7.3853
  [Val @ step 50]: train=7.3853, val=7.3612
Step  100: loss = 6.8976
  [Val @ step 100]: train=6.8976, val=6.9520
Step  150: loss = 6.7614
  [Val @ step 150]: train=6.7614, val=6.7791
Step  200: loss = 6.6833
  [Val @ step 200]: train=6.6833, val=6.6513
Step  250: loss = 6.4559
  [Val @ step 250]: train=6.4559, val=6.6247
Step  300: loss = 6.4525
  [Val @ step 300]: train=6.4525, val=6.5111
Step  350: loss = 6.4934
  [Val @ step 350]: train=6.4934, val=6.4773
Step  400: loss = 6.3644
  [Val @ step 400]: train=6.3644, val=6.4009
Step  450: loss = 6.4061
  [Val @ step 450]: train=6.4061, val=6.3658
Step  500: loss = 6.3372
  [Val @ step 500]: train=6.3372, val=6.2917
Step  550: loss = 6.1628
  [Val @ step 550]: train=6.1628, val=6.2996
Step  600: loss = 6.3170
  [Val @ step 600]: train=6.3170, val=6.2980
Step  650: loss = 6.2303
  [Val @ step 650]: train=6.2303, val=6.2351
Step  700: loss = 6.1829
  [Val @ step 700]: train=6.1829, val=6.1950
Step  750: loss = 6.1491
  [Val @ step 750]: train=6.1491, val=6.1719
Step  800: loss = 6.0948
  [Val @ step 800]: train=6.0948, val=6.1680
Step  850: loss = 6.1494
  [Val @ step 850]: train=6.1494, val=6.1849
Step  900: loss = 6.1755
  [Val @ step 900]: train=6.1755, val=6.1521
Step  950: loss = 5.8894
  [Val @ step 950]: train=5.8894, val=6.1224
Step 1000: loss = 6.0869
  [Val @ step 1000]: train=6.0869, val=6.1554
Step 1050: loss = 6.2649
  [Val @ step 1050]: train=6.2649, val=6.1119
Step 1100: loss = 6.2041
  [Val @ step 1100]: train=6.2041, val=6.1133
Step 1150: loss = 6.1143
  [Val @ step 1150]: train=6.1143, val=6.0939
Step 1200: loss = 6.0558
  [Val @ step 1200]: train=6.0558, val=6.0443
Step 1250: loss = 6.1952
  [Val @ step 1250]: train=6.1952, val=6.0598
Step 1300: loss = 5.9510
  [Val @ step 1300]: train=5.9510, val=6.0500
Step 1350: loss = 5.8725
  [Val @ step 1350]: train=5.8725, val=6.0375
Step 1400: loss = 6.1546
  [Val @ step 1400]: train=6.1546, val=6.0162
Step 1450: loss = 6.0838
  [Val @ step 1450]: train=6.0838, val=6.0414
Step 1500: loss = 5.8628
  [Val @ step 1500]: train=5.8628, val=6.0138
Step 1550: loss = 5.8575
  [Val @ step 1550]: train=5.8575, val=5.9653
Step 1600: loss = 5.9897
  [Val @ step 1600]: train=5.9897, val=5.9505
Step 1650: loss = 5.8916
  [Val @ step 1650]: train=5.8916, val=6.0190
Step 1700: loss = 5.8756
  [Val @ step 1700]: train=5.8756, val=5.9694
Step 1750: loss = 5.9369
  [Val @ step 1750]: train=5.9369, val=5.9913
Step 1800: loss = 6.0701
  [Val @ step 1800]: train=6.0701, val=5.9445
Step 1850: loss = 5.7071
  [Val @ step 1850]: train=5.7071, val=5.9344
Step 1900: loss = 5.9009
  [Val @ step 1900]: train=5.9009, val=5.9373
Step 1950: loss = 5.8730
  [Val @ step 1950]: train=5.8730, val=5.9116
Step 2000: loss = 5.9332
  [Val @ step 2000]: train=5.9332, val=5.9221
Step 2050: loss = 5.8508
  [Val @ step 2050]: train=5.8508, val=5.9351
Step 2100: loss = 5.8991
  [Val @ step 2100]: train=5.8991, val=5.9005
Step 2150: loss = 5.9715
  [Val @ step 2150]: train=5.9715, val=5.8884
Step 2200: loss = 5.9068
  [Val @ step 2200]: train=5.9068, val=5.9103
Step 2250: loss = 5.8508
  [Val @ step 2250]: train=5.8508, val=5.9279
Step 2300: loss = 5.8509
  [Val @ step 2300]: train=5.8509, val=5.8558
Step 2350: loss = 5.8271
  [Val @ step 2350]: train=5.8271, val=5.8525
Step 2400: loss = 5.9239
  [Val @ step 2400]: train=5.9239, val=5.8604
Step 2450: loss = 5.9435
  [Val @ step 2450]: train=5.9435, val=5.8763
Step 2500: loss = 5.8409
  [Val @ step 2500]: train=5.8409, val=5.8807
Step 2550: loss = 5.7483
  [Val @ step 2550]: train=5.7483, val=5.8696
Step 2600: loss = 5.6718
  [Val @ step 2600]: train=5.6718, val=5.8423
Step 2650: loss = 5.9106
  [Val @ step 2650]: train=5.9106, val=5.8500
Step 2700: loss = 5.8370
  [Val @ step 2700]: train=5.8370, val=5.8282
Step 2750: loss = 5.7918
  [Val @ step 2750]: train=5.7918, val=5.8031
Step 2800: loss = 5.9021
  [Val @ step 2800]: train=5.9021, val=5.8744
Step 2850: loss = 5.5222
  [Val @ step 2850]: train=5.5222, val=5.8245
Step 2900: loss = 5.8924
  [Val @ step 2900]: train=5.8924, val=5.7911
Step 2950: loss = 5.7375
  [Val @ step 2950]: train=5.7375, val=5.8522
Step 3000: loss = 5.9632
  [Val @ step 3000]: train=5.9632, val=5.8532
Step 3050: loss = 5.6755
  [Val @ step 3050]: train=5.6755, val=5.8136
Step 3100: loss = 5.8863
  [Val @ step 3100]: train=5.8863, val=5.8376
Step 3150: loss = 5.7913
  [Val @ step 3150]: train=5.7913, val=5.8267
Step 3200: loss = 5.7163
  [Val @ step 3200]: train=5.7163, val=5.8106
Step 3250: loss = 5.6692
  [Val @ step 3250]: train=5.6692, val=5.8276
Step 3300: loss = 5.7666
  [Val @ step 3300]: train=5.7666, val=5.8346
Step 3350: loss = 5.8364
  [Val @ step 3350]: train=5.8364, val=5.8215
Step 3400: loss = 5.7507
  [Val @ step 3400]: train=5.7507, val=5.8106
Step 3450: loss = 5.8609
  [Val @ step 3450]: train=5.8609, val=5.8172
Step 3500: loss = 5.6062
  [Val @ step 3500]: train=5.6062, val=5.7852
Step 3550: loss = 5.8057
  [Val @ step 3550]: train=5.8057, val=5.7436
Step 3600: loss = 5.7011
  [Val @ step 3600]: train=5.7011, val=5.7979
Step 3650: loss = 5.7856
  [Val @ step 3650]: train=5.7856, val=5.7536
Step 3700: loss = 5.7183
  [Val @ step 3700]: train=5.7183, val=5.7609
Step 3750: loss = 5.8460
  [Val @ step 3750]: train=5.8460, val=5.7909
Step 3800: loss = 5.8180
  [Val @ step 3800]: train=5.8180, val=5.7950
Step 3850: loss = 5.6572
  [Val @ step 3850]: train=5.6572, val=5.7777
Step 3900: loss = 5.6410
  [Val @ step 3900]: train=5.6410, val=5.7898
Step 3950: loss = 5.8305
  [Val @ step 3950]: train=5.8305, val=5.7652
Step 4000: loss = 5.8513
  [Val @ step 4000]: train=5.8513, val=5.7986
Step 4050: loss = 5.8128
  [Val @ step 4050]: train=5.8128, val=5.7719
Step 4100: loss = 5.7331
  [Val @ step 4100]: train=5.7331, val=5.7890
Step 4150: loss = 5.8628
  [Val @ step 4150]: train=5.8628, val=5.7798
Step 4200: loss = 5.6830
  [Val @ step 4200]: train=5.6830, val=5.7806
Step 4250: loss = 5.7476
  [Val @ step 4250]: train=5.7476, val=5.7711
Step 4300: loss = 5.7745
  [Val @ step 4300]: train=5.7745, val=5.7351
Step 4350: loss = 5.8685
  [Val @ step 4350]: train=5.8685, val=5.8060
Step 4400: loss = 5.5730
  [Val @ step 4400]: train=5.5730, val=5.8064
Step 4450: loss = 5.7236
  [Val @ step 4450]: train=5.7236, val=5.7832
Step 4500: loss = 5.7574
  [Val @ step 4500]: train=5.7574, val=5.7411
Step 4550: loss = 5.8285
  [Val @ step 4550]: train=5.8285, val=5.7269
Step 4600: loss = 5.7434
  [Val @ step 4600]: train=5.7434, val=5.7455
Step 4650: loss = 5.5045
  [Val @ step 4650]: train=5.5045, val=5.7508
Step 4700: loss = 5.7098
  [Val @ step 4700]: train=5.7098, val=5.7693
Step 4750: loss = 5.5073
  [Val @ step 4750]: train=5.5073, val=5.7623
Step 4800: loss = 5.6947
  [Val @ step 4800]: train=5.6947, val=5.7383
Step 4850: loss = 5.7916
  [Val @ step 4850]: train=5.7916, val=5.7198
Step 4900: loss = 5.8924
  [Val @ step 4900]: train=5.8924, val=5.7582
Step 4950: loss = 5.7821
  [Val @ step 4950]: train=5.7821, val=5.7236
--------------------------------------------------------------------------------
Computing final validation loss...
Validation loss: 5.7764

Results saved to time_to_target_stage1_20251225_225546.jsonl
Training time: 3825.7s
Throughput: 5,353 tokens/sec
Projections: 715/5000
nGPT Architectural Experiment: C_lazy10
================================================================================
Model: 11 layers x 768 dim (matching modded-nanogpt)
Hypothesis: Baseline
================================================================================

Loading FineWeb data (subset for fast experiments)...
  Loading data/fineweb10B/fineweb_train_000001.bin...
  Loading data/fineweb10B/fineweb_train_000002.bin...
  Loading data/fineweb10B/fineweb_train_000003.bin...
  Loading data/fineweb10B/fineweb_train_000004.bin...
  Vocab size: 50257
  Train tokens: 400,001,536
  Val tokens: 100,000,384

Model config (modded-nanogpt size):
  vocab_size: 50257
  n_layer: 11
  n_embd: 768
  n_head: 6
  block_size: 128
Using GatedBlock (H3) with gate_init=0.0

Total parameters: 155,166,743

Optimizer: SimpleMuon (lr=0.003, momentum=0.95, geodesic=baseline, orthog=newton_schulz)
  2D weight params (Muon): 77,856,768
  1D params (Adam): 77,309,975

Starting training (5000 steps)...
--------------------------------------------------------------------------------
Step    0: loss = 10.9679
Step   50: loss = 7.5024
  [Val @ step 50]: train=7.5024, val=7.3872
Step  100: loss = 6.8562
  [Val @ step 100]: train=6.8562, val=6.9745
Step  150: loss = 6.7356
  [Val @ step 150]: train=6.7356, val=6.7595
Step  200: loss = 6.7843
  [Val @ step 200]: train=6.7843, val=6.6661
Step  250: loss = 6.4997
  [Val @ step 250]: train=6.4997, val=6.5876
Step  300: loss = 6.4105
  [Val @ step 300]: train=6.4105, val=6.5231
Step  350: loss = 6.2379
  [Val @ step 350]: train=6.2379, val=6.4814
Step  400: loss = 6.2591
  [Val @ step 400]: train=6.2591, val=6.3937
Step  450: loss = 6.4335
  [Val @ step 450]: train=6.4335, val=6.3598
Step  500: loss = 6.4135
  [Val @ step 500]: train=6.4135, val=6.3117
Step  550: loss = 6.1368
  [Val @ step 550]: train=6.1368, val=6.3096
Step  600: loss = 6.4669
  [Val @ step 600]: train=6.4669, val=6.2543
Step  650: loss = 6.1415
  [Val @ step 650]: train=6.1415, val=6.2417
Step  700: loss = 6.1787
  [Val @ step 700]: train=6.1787, val=6.2313
Step  750: loss = 6.4156
  [Val @ step 750]: train=6.4156, val=6.2312
Step  800: loss = 6.3030
  [Val @ step 800]: train=6.3030, val=6.2194
Step  850: loss = 6.1279
  [Val @ step 850]: train=6.1279, val=6.1689
Step  900: loss = 6.1830
  [Val @ step 900]: train=6.1830, val=6.1234
Step  950: loss = 6.1617
  [Val @ step 950]: train=6.1617, val=6.1541
Step 1000: loss = 6.1535
  [Val @ step 1000]: train=6.1535, val=6.1039
Step 1050: loss = 6.0183
  [Val @ step 1050]: train=6.0183, val=6.1332
Step 1100: loss = 6.1966
  [Val @ step 1100]: train=6.1966, val=6.0815
Step 1150: loss = 5.9893
  [Val @ step 1150]: train=5.9893, val=6.0361
Step 1200: loss = 6.0654
  [Val @ step 1200]: train=6.0654, val=6.0572
Step 1250: loss = 6.2282
  [Val @ step 1250]: train=6.2282, val=6.0252
Step 1300: loss = 6.0186
  [Val @ step 1300]: train=6.0186, val=6.0032
Step 1350: loss = 5.9700
  [Val @ step 1350]: train=5.9700, val=6.0361
Step 1400: loss = 6.0271
  [Val @ step 1400]: train=6.0271, val=6.0462
Step 1450: loss = 5.9134
  [Val @ step 1450]: train=5.9134, val=5.9636
Step 1500: loss = 6.1306
  [Val @ step 1500]: train=6.1306, val=5.9860
Step 1550: loss = 5.9750
  [Val @ step 1550]: train=5.9750, val=5.9845
Step 1600: loss = 5.9185
  [Val @ step 1600]: train=5.9185, val=5.9516
Step 1650: loss = 6.0525
  [Val @ step 1650]: train=6.0525, val=5.9782
Step 1700: loss = 5.7295
  [Val @ step 1700]: train=5.7295, val=5.9749
Step 1750: loss = 5.9017
  [Val @ step 1750]: train=5.9017, val=5.9561
Step 1800: loss = 5.8469
  [Val @ step 1800]: train=5.8469, val=5.9544
Step 1850: loss = 5.9739
  [Val @ step 1850]: train=5.9739, val=5.9021
Step 1900: loss = 5.8135
  [Val @ step 1900]: train=5.8135, val=5.9395
Step 1950: loss = 5.9890
  [Val @ step 1950]: train=5.9890, val=5.9533
Step 2000: loss = 5.8534
  [Val @ step 2000]: train=5.8534, val=5.9248
Step 2050: loss = 5.9641
  [Val @ step 2050]: train=5.9641, val=5.8858
Step 2100: loss = 5.7944
  [Val @ step 2100]: train=5.7944, val=5.9140
Step 2150: loss = 5.8212
  [Val @ step 2150]: train=5.8212, val=5.9274
Step 2200: loss = 5.7757
  [Val @ step 2200]: train=5.7757, val=5.9129
Step 2250: loss = 5.9080
  [Val @ step 2250]: train=5.9080, val=5.8891
Step 2300: loss = 5.8119
  [Val @ step 2300]: train=5.8119, val=5.9300
Step 2350: loss = 5.9784
  [Val @ step 2350]: train=5.9784, val=5.8597
Step 2400: loss = 5.9380
  [Val @ step 2400]: train=5.9380, val=5.8587
Step 2450: loss = 5.9225
  [Val @ step 2450]: train=5.9225, val=5.9219
Step 2500: loss = 5.9231
  [Val @ step 2500]: train=5.9231, val=5.8852
Step 2550: loss = 5.7324
  [Val @ step 2550]: train=5.7324, val=5.8834
Step 2600: loss = 5.9964
  [Val @ step 2600]: train=5.9964, val=5.8442
Step 2650: loss = 5.7650
  [Val @ step 2650]: train=5.7650, val=5.8524
Step 2700: loss = 5.7208
  [Val @ step 2700]: train=5.7208, val=5.8438
Step 2750: loss = 5.8831
  [Val @ step 2750]: train=5.8831, val=5.8230
Step 2800: loss = 5.9137
  [Val @ step 2800]: train=5.9137, val=5.8050
Step 2850: loss = 5.7474
  [Val @ step 2850]: train=5.7474, val=5.8327
Step 2900: loss = 5.8138
  [Val @ step 2900]: train=5.8138, val=5.8464
Step 2950: loss = 5.7254
  [Val @ step 2950]: train=5.7254, val=5.8891
Step 3000: loss = 6.0223
  [Val @ step 3000]: train=6.0223, val=5.8569
Step 3050: loss = 5.9822
  [Val @ step 3050]: train=5.9822, val=5.8235
Step 3100: loss = 5.7284
  [Val @ step 3100]: train=5.7284, val=5.8341
Step 3150: loss = 5.7167
  [Val @ step 3150]: train=5.7167, val=5.8026
Step 3200: loss = 5.9309
  [Val @ step 3200]: train=5.9309, val=5.8289
Step 3250: loss = 5.9517
  [Val @ step 3250]: train=5.9517, val=5.7974
Step 3300: loss = 5.7217
  [Val @ step 3300]: train=5.7217, val=5.8024
Step 3350: loss = 5.9556
  [Val @ step 3350]: train=5.9556, val=5.8460
Step 3400: loss = 5.8578
  [Val @ step 3400]: train=5.8578, val=5.8094
Step 3450: loss = 5.7062
  [Val @ step 3450]: train=5.7062, val=5.7932
Step 3500: loss = 5.9573
  [Val @ step 3500]: train=5.9573, val=5.7891
Step 3550: loss = 5.6763
  [Val @ step 3550]: train=5.6763, val=5.8204
Step 3600: loss = 5.9306
  [Val @ step 3600]: train=5.9306, val=5.7990
Step 3650: loss = 5.8005
  [Val @ step 3650]: train=5.8005, val=5.8011
Step 3700: loss = 5.7583
  [Val @ step 3700]: train=5.7583, val=5.7512
Step 3750: loss = 5.7403
  [Val @ step 3750]: train=5.7403, val=5.8037
Step 3800: loss = 5.7806
  [Val @ step 3800]: train=5.7806, val=5.7696
Step 3850: loss = 5.6784
  [Val @ step 3850]: train=5.6784, val=5.8017
Step 3900: loss = 5.8419
  [Val @ step 3900]: train=5.8419, val=5.8158
Step 3950: loss = 5.8298
  [Val @ step 3950]: train=5.8298, val=5.7796
Step 4000: loss = 5.7234
  [Val @ step 4000]: train=5.7234, val=5.8008
Step 4050: loss = 5.7607
  [Val @ step 4050]: train=5.7607, val=5.7679
Step 4100: loss = 5.7468
  [Val @ step 4100]: train=5.7468, val=5.7557
Step 4150: loss = 5.6935
  [Val @ step 4150]: train=5.6935, val=5.7952
Step 4200: loss = 5.6493
  [Val @ step 4200]: train=5.6493, val=5.7637
Step 4250: loss = 5.7012
  [Val @ step 4250]: train=5.7012, val=5.7765
Step 4300: loss = 5.7024
  [Val @ step 4300]: train=5.7024, val=5.8100
Step 4350: loss = 5.8576
  [Val @ step 4350]: train=5.8576, val=5.7453
Step 4400: loss = 5.9530
  [Val @ step 4400]: train=5.9530, val=5.7722
Step 4450: loss = 5.7732
  [Val @ step 4450]: train=5.7732, val=5.7681
Step 4500: loss = 5.6565
  [Val @ step 4500]: train=5.6565, val=5.7652
Step 4550: loss = 5.6667
  [Val @ step 4550]: train=5.6667, val=5.7725
Step 4600: loss = 5.7416
  [Val @ step 4600]: train=5.7416, val=5.7678
Step 4650: loss = 5.8239
  [Val @ step 4650]: train=5.8239, val=5.7523
Step 4700: loss = 5.6694
  [Val @ step 4700]: train=5.6694, val=5.7602
Step 4750: loss = 5.7933
  [Val @ step 4750]: train=5.7933, val=5.7469
Step 4800: loss = 5.7724
  [Val @ step 4800]: train=5.7724, val=5.7474
Step 4850: loss = 5.6415
  [Val @ step 4850]: train=5.6415, val=5.7266
Step 4900: loss = 5.7188
  [Val @ step 4900]: train=5.7188, val=5.7573
Step 4950: loss = 5.6980
  [Val @ step 4950]: train=5.6980, val=5.7198
--------------------------------------------------------------------------------
Computing final validation loss...
Validation loss: 5.7060

Results saved to time_to_target_stage1_20251225_225546.jsonl
Training time: 3809.7s
Throughput: 5,376 tokens/sec
Projections: 500/5000
nGPT Architectural Experiment: C_lazy15
================================================================================
Model: 11 layers x 768 dim (matching modded-nanogpt)
Hypothesis: Baseline
================================================================================

Loading FineWeb data (subset for fast experiments)...
  Loading data/fineweb10B/fineweb_train_000001.bin...
  Loading data/fineweb10B/fineweb_train_000002.bin...
  Loading data/fineweb10B/fineweb_train_000003.bin...
  Loading data/fineweb10B/fineweb_train_000004.bin...
  Vocab size: 50257
  Train tokens: 400,001,536
  Val tokens: 100,000,384

Model config (modded-nanogpt size):
  vocab_size: 50257
  n_layer: 11
  n_embd: 768
  n_head: 6
  block_size: 128
Using GatedBlock (H3) with gate_init=0.0

Total parameters: 155,166,743

Optimizer: SimpleMuon (lr=0.003, momentum=0.95, geodesic=baseline, orthog=newton_schulz)
  2D weight params (Muon): 77,856,768
  1D params (Adam): 77,309,975

Starting training (5000 steps)...
--------------------------------------------------------------------------------
Step    0: loss = 10.9683
Step   50: loss = 7.5600
  [Val @ step 50]: train=7.5600, val=7.3921
Step  100: loss = 6.8850
  [Val @ step 100]: train=6.8850, val=6.9451
Step  150: loss = 6.8556
  [Val @ step 150]: train=6.8556, val=6.7269
Step  200: loss = 6.6177
  [Val @ step 200]: train=6.6177, val=6.6684
Step  250: loss = 6.5916
  [Val @ step 250]: train=6.5916, val=6.5902
Step  300: loss = 6.4703
  [Val @ step 300]: train=6.4703, val=6.4485
Step  350: loss = 6.5486
  [Val @ step 350]: train=6.5486, val=6.4394
Step  400: loss = 6.3156
  [Val @ step 400]: train=6.3156, val=6.4172
Step  450: loss = 6.4812
  [Val @ step 450]: train=6.4812, val=6.3453
Step  500: loss = 6.2759
  [Val @ step 500]: train=6.2759, val=6.3287
Step  550: loss = 6.2832
  [Val @ step 550]: train=6.2832, val=6.3397
Step  600: loss = 6.1317
  [Val @ step 600]: train=6.1317, val=6.2729
Step  650: loss = 6.1780
  [Val @ step 650]: train=6.1780, val=6.2628
Step  700: loss = 6.0380
  [Val @ step 700]: train=6.0380, val=6.2490
Step  750: loss = 6.2537
  [Val @ step 750]: train=6.2537, val=6.2162
Step  800: loss = 6.1993
  [Val @ step 800]: train=6.1993, val=6.1748
Step  850: loss = 6.1229
  [Val @ step 850]: train=6.1229, val=6.1385
Step  900: loss = 6.0974
  [Val @ step 900]: train=6.0974, val=6.1463
Step  950: loss = 5.9534
  [Val @ step 950]: train=5.9534, val=6.1247
Step 1000: loss = 6.1605
  [Val @ step 1000]: train=6.1605, val=6.1147
Step 1050: loss = 5.9948
  [Val @ step 1050]: train=5.9948, val=6.1053
Step 1100: loss = 6.0374
  [Val @ step 1100]: train=6.0374, val=6.0721
Step 1150: loss = 6.1267
  [Val @ step 1150]: train=6.1267, val=6.0397
Step 1200: loss = 5.9194
  [Val @ step 1200]: train=5.9194, val=6.0390
Step 1250: loss = 6.2114
  [Val @ step 1250]: train=6.2114, val=6.0691
Step 1300: loss = 6.0241
  [Val @ step 1300]: train=6.0241, val=6.0040
Step 1350: loss = 6.1309
  [Val @ step 1350]: train=6.1309, val=5.9972
Step 1400: loss = 5.8350
  [Val @ step 1400]: train=5.8350, val=6.0276
Step 1450: loss = 6.0352
  [Val @ step 1450]: train=6.0352, val=5.9768
Step 1500: loss = 5.8897
  [Val @ step 1500]: train=5.8897, val=6.0160
Step 1550: loss = 6.0538
  [Val @ step 1550]: train=6.0538, val=5.9740
Step 1600: loss = 5.9791
  [Val @ step 1600]: train=5.9791, val=5.9097
Step 1650: loss = 5.8786
  [Val @ step 1650]: train=5.8786, val=5.9210
Step 1700: loss = 6.0302
  [Val @ step 1700]: train=6.0302, val=5.9476
Step 1750: loss = 5.8399
  [Val @ step 1750]: train=5.8399, val=5.9568
Step 1800: loss = 5.9850
  [Val @ step 1800]: train=5.9850, val=5.9292
Step 1850: loss = 5.8350
  [Val @ step 1850]: train=5.8350, val=5.9747
Step 1900: loss = 6.0712
  [Val @ step 1900]: train=6.0712, val=5.9042
Step 1950: loss = 5.8332
  [Val @ step 1950]: train=5.8332, val=5.9402
Step 2000: loss = 5.8789
  [Val @ step 2000]: train=5.8789, val=5.9284
Step 2050: loss = 5.9853
  [Val @ step 2050]: train=5.9853, val=5.9235
Step 2100: loss = 5.7687
  [Val @ step 2100]: train=5.7687, val=5.9288
Step 2150: loss = 5.8294
  [Val @ step 2150]: train=5.8294, val=5.8725
Step 2200: loss = 5.7832
  [Val @ step 2200]: train=5.7832, val=5.8955
Step 2250: loss = 5.8679
  [Val @ step 2250]: train=5.8679, val=5.8933
Step 2300: loss = 5.8127
  [Val @ step 2300]: train=5.8127, val=5.8984
Step 2350: loss = 5.8992
  [Val @ step 2350]: train=5.8992, val=5.9039
Step 2400: loss = 6.0079
  [Val @ step 2400]: train=6.0079, val=5.9048
Step 2450: loss = 5.8064
  [Val @ step 2450]: train=5.8064, val=5.8979
Step 2500: loss = 5.7963
  [Val @ step 2500]: train=5.7963, val=5.8687
Step 2550: loss = 5.8500
  [Val @ step 2550]: train=5.8500, val=5.8800
Step 2600: loss = 5.8124
  [Val @ step 2600]: train=5.8124, val=5.8140
Step 2650: loss = 5.8618
  [Val @ step 2650]: train=5.8618, val=5.8479
Step 2700: loss = 5.7695
  [Val @ step 2700]: train=5.7695, val=5.8803
Step 2750: loss = 5.8031
  [Val @ step 2750]: train=5.8031, val=5.8492
Step 2800: loss = 5.8105
  [Val @ step 2800]: train=5.8105, val=5.8642
Step 2850: loss = 5.8581
  [Val @ step 2850]: train=5.8581, val=5.8064
Step 2900: loss = 5.8556
  [Val @ step 2900]: train=5.8556, val=5.8304
Step 2950: loss = 5.7775
  [Val @ step 2950]: train=5.7775, val=5.7963
Step 3000: loss = 5.8060
  [Val @ step 3000]: train=5.8060, val=5.8276
Step 3050: loss = 5.8728
  [Val @ step 3050]: train=5.8728, val=5.8206
Step 3100: loss = 5.8529
  [Val @ step 3100]: train=5.8529, val=5.7949
Step 3150: loss = 5.8314
  [Val @ step 3150]: train=5.8314, val=5.8002
Step 3200: loss = 5.8237
  [Val @ step 3200]: train=5.8237, val=5.8161
Step 3250: loss = 5.6188
  [Val @ step 3250]: train=5.6188, val=5.8345
Step 3300: loss = 5.8146
  [Val @ step 3300]: train=5.8146, val=5.8526
Step 3350: loss = 5.5557
  [Val @ step 3350]: train=5.5557, val=5.7602
Step 3400: loss = 5.8256
  [Val @ step 3400]: train=5.8256, val=5.7705
Step 3450: loss = 5.7486
  [Val @ step 3450]: train=5.7486, val=5.8223
Step 3500: loss = 5.6869
  [Val @ step 3500]: train=5.6869, val=5.7831
Step 3550: loss = 5.6446
  [Val @ step 3550]: train=5.6446, val=5.8088
Step 3600: loss = 5.6978
  [Val @ step 3600]: train=5.6978, val=5.8072
Step 3650: loss = 5.8371
  [Val @ step 3650]: train=5.8371, val=5.7801
Step 3700: loss = 5.8614
  [Val @ step 3700]: train=5.8614, val=5.8321
Step 3750: loss = 5.9389
  [Val @ step 3750]: train=5.9389, val=5.8187
Step 3800: loss = 5.8443
  [Val @ step 3800]: train=5.8443, val=5.7609
Step 3850: loss = 5.7639
  [Val @ step 3850]: train=5.7639, val=5.7792
Step 3900: loss = 5.8525
  [Val @ step 3900]: train=5.8525, val=5.7546
Step 3950: loss = 5.6071
  [Val @ step 3950]: train=5.6071, val=5.7675
Step 4000: loss = 5.8319
  [Val @ step 4000]: train=5.8319, val=5.7969
Step 4050: loss = 5.8255
  [Val @ step 4050]: train=5.8255, val=5.8004
Step 4100: loss = 5.9042
  [Val @ step 4100]: train=5.9042, val=5.8193
Step 4150: loss = 5.6759
  [Val @ step 4150]: train=5.6759, val=5.7909
Step 4200: loss = 5.6984
  [Val @ step 4200]: train=5.6984, val=5.7604
Step 4250: loss = 5.6645
  [Val @ step 4250]: train=5.6645, val=5.7767
Step 4300: loss = 5.7776
  [Val @ step 4300]: train=5.7776, val=5.7193
Step 4350: loss = 5.6352
  [Val @ step 4350]: train=5.6352, val=5.7321
Step 4400: loss = 5.7312
  [Val @ step 4400]: train=5.7312, val=5.7990
Step 4450: loss = 5.8721
  [Val @ step 4450]: train=5.8721, val=5.7329
Step 4500: loss = 5.7716
  [Val @ step 4500]: train=5.7716, val=5.7917
Step 4550: loss = 5.6941
  [Val @ step 4550]: train=5.6941, val=5.7375
Step 4600: loss = 5.8637
  [Val @ step 4600]: train=5.8637, val=5.7389
Step 4650: loss = 5.6595
  [Val @ step 4650]: train=5.6595, val=5.7763
Step 4700: loss = 5.6767
  [Val @ step 4700]: train=5.6767, val=5.7700
Step 4750: loss = 5.6479
  [Val @ step 4750]: train=5.6479, val=5.7685
Step 4800: loss = 5.5919
  [Val @ step 4800]: train=5.5919, val=5.7478
Step 4850: loss = 5.4169
  [Val @ step 4850]: train=5.4169, val=5.7391
Step 4900: loss = 5.7658
  [Val @ step 4900]: train=5.7658, val=5.7261
Step 4950: loss = 5.5439
  [Val @ step 4950]: train=5.5439, val=5.7448
--------------------------------------------------------------------------------
Computing final validation loss...
Validation loss: 5.7701

Results saved to time_to_target_stage1_20251225_225546.jsonl
Training time: 3837.8s
Throughput: 5,336 tokens/sec
Projections: 334/5000
nGPT Architectural Experiment: C_lazy20
================================================================================
Model: 11 layers x 768 dim (matching modded-nanogpt)
Hypothesis: Baseline
================================================================================

Loading FineWeb data (subset for fast experiments)...
  Loading data/fineweb10B/fineweb_train_000001.bin...
  Loading data/fineweb10B/fineweb_train_000002.bin...
  Loading data/fineweb10B/fineweb_train_000003.bin...
  Loading data/fineweb10B/fineweb_train_000004.bin...
  Vocab size: 50257
  Train tokens: 400,001,536
  Val tokens: 100,000,384

Model config (modded-nanogpt size):
  vocab_size: 50257
  n_layer: 11
  n_embd: 768
  n_head: 6
  block_size: 128
Using GatedBlock (H3) with gate_init=0.0

Total parameters: 155,166,743

Optimizer: SimpleMuon (lr=0.003, momentum=0.95, geodesic=baseline, orthog=newton_schulz)
  2D weight params (Muon): 77,856,768
  1D params (Adam): 77,309,975

Starting training (5000 steps)...
--------------------------------------------------------------------------------
Step    0: loss = 10.9715
Step   50: loss = 7.3887
  [Val @ step 50]: train=7.3887, val=7.4267
Step  100: loss = 6.8272
  [Val @ step 100]: train=6.8272, val=6.9504
Step  150: loss = 6.9175
  [Val @ step 150]: train=6.9175, val=6.8202
Step  200: loss = 6.8869
  [Val @ step 200]: train=6.8869, val=6.6368
Step  250: loss = 6.4975
  [Val @ step 250]: train=6.4975, val=6.5326
Step  300: loss = 6.4312
  [Val @ step 300]: train=6.4312, val=6.4630
Step  350: loss = 6.4652
  [Val @ step 350]: train=6.4652, val=6.4676
Step  400: loss = 6.2656
  [Val @ step 400]: train=6.2656, val=6.4161
Step  450: loss = 6.4825
  [Val @ step 450]: train=6.4825, val=6.3549
Step  500: loss = 6.1423
  [Val @ step 500]: train=6.1423, val=6.3454
Step  550: loss = 6.1939
  [Val @ step 550]: train=6.1939, val=6.2861
Step  600: loss = 6.4164
  [Val @ step 600]: train=6.4164, val=6.2134
Step  650: loss = 6.4118
  [Val @ step 650]: train=6.4118, val=6.2624
Step  700: loss = 6.2670
  [Val @ step 700]: train=6.2670, val=6.2320
Step  750: loss = 6.0816
  [Val @ step 750]: train=6.0816, val=6.1940
Step  800: loss = 6.1487
  [Val @ step 800]: train=6.1487, val=6.1714
Step  850: loss = 6.1154
  [Val @ step 850]: train=6.1154, val=6.1743
Step  900: loss = 6.0577
  [Val @ step 900]: train=6.0577, val=6.1244
Step  950: loss = 6.1532
  [Val @ step 950]: train=6.1532, val=6.1047
Step 1000: loss = 6.0979
  [Val @ step 1000]: train=6.0979, val=6.1385
Step 1050: loss = 6.0226
  [Val @ step 1050]: train=6.0226, val=6.0302
Step 1100: loss = 6.0539
  [Val @ step 1100]: train=6.0539, val=6.1010
Step 1150: loss = 5.9867
  [Val @ step 1150]: train=5.9867, val=6.0669
Step 1200: loss = 6.0746
  [Val @ step 1200]: train=6.0746, val=6.0863
Step 1250: loss = 6.0796
  [Val @ step 1250]: train=6.0796, val=6.0284
Step 1300: loss = 6.0491
  [Val @ step 1300]: train=6.0491, val=6.0387
Step 1350: loss = 6.0770
  [Val @ step 1350]: train=6.0770, val=6.0382
Step 1400: loss = 6.0017
  [Val @ step 1400]: train=6.0017, val=5.9769
Step 1450: loss = 5.9635
  [Val @ step 1450]: train=5.9635, val=5.9622
Step 1500: loss = 6.2242
  [Val @ step 1500]: train=6.2242, val=5.9845
Step 1550: loss = 5.8640
  [Val @ step 1550]: train=5.8640, val=6.0136
Step 1600: loss = 5.8071
  [Val @ step 1600]: train=5.8071, val=5.9601
Step 1650: loss = 5.8976
  [Val @ step 1650]: train=5.8976, val=5.9269
Step 1700: loss = 6.2647
  [Val @ step 1700]: train=6.2647, val=5.9483
Step 1750: loss = 5.9347
  [Val @ step 1750]: train=5.9347, val=5.9718
Step 1800: loss = 5.7067
  [Val @ step 1800]: train=5.7067, val=5.9214
Step 1850: loss = 6.0562
  [Val @ step 1850]: train=6.0562, val=5.9397
Step 1900: loss = 5.9710
  [Val @ step 1900]: train=5.9710, val=5.9520
Step 1950: loss = 5.7971
  [Val @ step 1950]: train=5.7971, val=5.9529
Step 2000: loss = 5.9241
  [Val @ step 2000]: train=5.9241, val=5.9325
Step 2050: loss = 6.0046
  [Val @ step 2050]: train=6.0046, val=5.9089
Step 2100: loss = 5.8110
  [Val @ step 2100]: train=5.8110, val=5.9071
Step 2150: loss = 5.9169
  [Val @ step 2150]: train=5.9169, val=5.9288
Step 2200: loss = 5.7044
  [Val @ step 2200]: train=5.7044, val=5.8820
Step 2250: loss = 6.0464
  [Val @ step 2250]: train=6.0464, val=5.8631
Step 2300: loss = 6.1539
  [Val @ step 2300]: train=6.1539, val=5.9058
Step 2350: loss = 5.9890
  [Val @ step 2350]: train=5.9890, val=5.8554
Step 2400: loss = 5.8225
  [Val @ step 2400]: train=5.8225, val=5.8718
Step 2450: loss = 5.8579
  [Val @ step 2450]: train=5.8579, val=5.8648
Step 2500: loss = 5.8108
  [Val @ step 2500]: train=5.8108, val=5.8754
Step 2550: loss = 5.8128
  [Val @ step 2550]: train=5.8128, val=5.8443
Step 2600: loss = 5.9169
  [Val @ step 2600]: train=5.9169, val=5.8641
Step 2650: loss = 5.8543
  [Val @ step 2650]: train=5.8543, val=5.8463
Step 2700: loss = 5.7991
  [Val @ step 2700]: train=5.7991, val=5.8868
Step 2750: loss = 5.8516
  [Val @ step 2750]: train=5.8516, val=5.8419
Step 2800: loss = 5.8888
  [Val @ step 2800]: train=5.8888, val=5.8267
Step 2850: loss = 5.7883
  [Val @ step 2850]: train=5.7883, val=5.8624
Step 2900: loss = 5.8429
  [Val @ step 2900]: train=5.8429, val=5.8757
Step 2950: loss = 5.7881
  [Val @ step 2950]: train=5.7881, val=5.8377
Step 3000: loss = 5.7858
  [Val @ step 3000]: train=5.7858, val=5.8916
Step 3050: loss = 5.7640
  [Val @ step 3050]: train=5.7640, val=5.8476
Step 3100: loss = 5.6638
  [Val @ step 3100]: train=5.6638, val=5.8364
Step 3150: loss = 5.6484
  [Val @ step 3150]: train=5.6484, val=5.8430
Step 3200: loss = 5.7843
  [Val @ step 3200]: train=5.7843, val=5.8040
Step 3250: loss = 5.9425
  [Val @ step 3250]: train=5.9425, val=5.7822
Step 3300: loss = 5.8645
  [Val @ step 3300]: train=5.8645, val=5.8092
Step 3350: loss = 5.5795
  [Val @ step 3350]: train=5.5795, val=5.8172
Step 3400: loss = 5.5147
  [Val @ step 3400]: train=5.5147, val=5.8016
Step 3450: loss = 5.6547
  [Val @ step 3450]: train=5.6547, val=5.8557
Step 3500: loss = 5.8367
  [Val @ step 3500]: train=5.8367, val=5.8238
Step 3550: loss = 5.7884
  [Val @ step 3550]: train=5.7884, val=5.7771
Step 3600: loss = 5.6256
  [Val @ step 3600]: train=5.6256, val=5.8024
Step 3650: loss = 5.6431
  [Val @ step 3650]: train=5.6431, val=5.8327
Step 3700: loss = 5.6791
  [Val @ step 3700]: train=5.6791, val=5.8006
Step 3750: loss = 5.7286
  [Val @ step 3750]: train=5.7286, val=5.8215
Step 3800: loss = 5.7749
  [Val @ step 3800]: train=5.7749, val=5.8071
Step 3850: loss = 5.7793
  [Val @ step 3850]: train=5.7793, val=5.7777
Step 3900: loss = 5.8633
  [Val @ step 3900]: train=5.8633, val=5.7599
Step 3950: loss = 5.8066
  [Val @ step 3950]: train=5.8066, val=5.7578
Step 4000: loss = 5.8046
  [Val @ step 4000]: train=5.8046, val=5.7745
Step 4050: loss = 5.8432
  [Val @ step 4050]: train=5.8432, val=5.7470
Step 4100: loss = 5.6674
  [Val @ step 4100]: train=5.6674, val=5.7641
Step 4150: loss = 5.8728
  [Val @ step 4150]: train=5.8728, val=5.7603
Step 4200: loss = 5.7645
  [Val @ step 4200]: train=5.7645, val=5.7335
Step 4250: loss = 5.5074
  [Val @ step 4250]: train=5.5074, val=5.7644
Step 4300: loss = 5.5781
  [Val @ step 4300]: train=5.5781, val=5.7962
Step 4350: loss = 5.6893
  [Val @ step 4350]: train=5.6893, val=5.7716
Step 4400: loss = 5.6925
  [Val @ step 4400]: train=5.6925, val=5.7753
Step 4450: loss = 5.6196
  [Val @ step 4450]: train=5.6196, val=5.7987
Step 4500: loss = 5.6941
  [Val @ step 4500]: train=5.6941, val=5.7847
Step 4550: loss = 5.7039
  [Val @ step 4550]: train=5.7039, val=5.7724
Step 4600: loss = 5.8220
  [Val @ step 4600]: train=5.8220, val=5.7749
Step 4650: loss = 5.7007
  [Val @ step 4650]: train=5.7007, val=5.7804
Step 4700: loss = 5.7327
  [Val @ step 4700]: train=5.7327, val=5.7644
Step 4750: loss = 5.7287
  [Val @ step 4750]: train=5.7287, val=5.7619
Step 4800: loss = 5.5809
  [Val @ step 4800]: train=5.5809, val=5.7686
Step 4850: loss = 5.6900
  [Val @ step 4850]: train=5.6900, val=5.7781
Step 4900: loss = 5.8059
  [Val @ step 4900]: train=5.8059, val=5.7437
Step 4950: loss = 5.7861
  [Val @ step 4950]: train=5.7861, val=5.7355
--------------------------------------------------------------------------------
Computing final validation loss...
Validation loss: 5.7275

Results saved to time_to_target_stage1_20251225_225546.jsonl
Training time: 3905.8s
Throughput: 5,243 tokens/sec
Projections: 250/5000
nGPT Architectural Experiment: D_lr0.0005
================================================================================
Model: 11 layers x 768 dim (matching modded-nanogpt)
Hypothesis: Baseline
================================================================================

Loading FineWeb data (subset for fast experiments)...
  Loading data/fineweb10B/fineweb_train_000001.bin...
  Loading data/fineweb10B/fineweb_train_000002.bin...
  Loading data/fineweb10B/fineweb_train_000003.bin...
  Loading data/fineweb10B/fineweb_train_000004.bin...
  Vocab size: 50257
  Train tokens: 400,001,536
  Val tokens: 100,000,384

Model config (modded-nanogpt size):
  vocab_size: 50257
  n_layer: 11
  n_embd: 768
  n_head: 6
  block_size: 128
Using GatedBlock (H3) with gate_init=0.0

Total parameters: 155,166,743

Optimizer: SimpleMuon (lr=0.0005, momentum=0.95, geodesic=baseline, orthog=newton_schulz)
  2D weight params (Muon): 77,856,768
  1D params (Adam): 77,309,975

Starting training (5000 steps)...
--------------------------------------------------------------------------------
Step    0: loss = 10.9715
Step   50: loss = 10.2633
  [Val @ step 50]: train=10.2633, val=10.2994
Step  100: loss = 9.0287
  [Val @ step 100]: train=9.0287, val=8.9357
Step  150: loss = 7.5844
  [Val @ step 150]: train=7.5844, val=7.7182
Step  200: loss = 7.2322
  [Val @ step 200]: train=7.2322, val=7.4008
Step  250: loss = 7.3843
  [Val @ step 250]: train=7.3843, val=7.2622
Step  300: loss = 7.1392
  [Val @ step 300]: train=7.1392, val=7.1899
Step  350: loss = 7.1000
  [Val @ step 350]: train=7.1000, val=7.1121
Step  400: loss = 7.1628
  [Val @ step 400]: train=7.1628, val=7.0150
Step  450: loss = 6.8172
  [Val @ step 450]: train=6.8172, val=6.9608
Step  500: loss = 7.0198
  [Val @ step 500]: train=7.0198, val=6.9178
Step  550: loss = 6.9923
  [Val @ step 550]: train=6.9923, val=6.8789
Step  600: loss = 6.8950
  [Val @ step 600]: train=6.8950, val=6.8515
Step  650: loss = 6.7643
  [Val @ step 650]: train=6.7643, val=6.8393
Step  700: loss = 6.5511
  [Val @ step 700]: train=6.5511, val=6.7547
Step  750: loss = 6.7628
  [Val @ step 750]: train=6.7628, val=6.6990
Step  800: loss = 6.4870
  [Val @ step 800]: train=6.4870, val=6.7096
Step  850: loss = 6.6782
  [Val @ step 850]: train=6.6782, val=6.6283
Step  900: loss = 6.5251
  [Val @ step 900]: train=6.5251, val=6.5813
Step  950: loss = 6.6697
  [Val @ step 950]: train=6.6697, val=6.5825
Step 1000: loss = 6.5124
  [Val @ step 1000]: train=6.5124, val=6.5413
Step 1050: loss = 6.4986
  [Val @ step 1050]: train=6.4986, val=6.5709
Step 1100: loss = 6.5164
  [Val @ step 1100]: train=6.5164, val=6.5182
Step 1150: loss = 6.5872
  [Val @ step 1150]: train=6.5872, val=6.4875
Step 1200: loss = 6.4376
  [Val @ step 1200]: train=6.4376, val=6.5038
Step 1250: loss = 6.5329
  [Val @ step 1250]: train=6.5329, val=6.4724
Step 1300: loss = 6.4356
  [Val @ step 1300]: train=6.4356, val=6.4193
Step 1350: loss = 6.3157
  [Val @ step 1350]: train=6.3157, val=6.4578
Step 1400: loss = 6.5131
  [Val @ step 1400]: train=6.5131, val=6.4393
Step 1450: loss = 6.6122
  [Val @ step 1450]: train=6.6122, val=6.4235
Step 1500: loss = 6.3124
  [Val @ step 1500]: train=6.3124, val=6.3884
Step 1550: loss = 6.3369
  [Val @ step 1550]: train=6.3369, val=6.3766
Step 1600: loss = 6.3455
  [Val @ step 1600]: train=6.3455, val=6.3825
Step 1650: loss = 6.1411
  [Val @ step 1650]: train=6.1411, val=6.3831
Step 1700: loss = 6.1655
  [Val @ step 1700]: train=6.1655, val=6.3243
Step 1750: loss = 6.3944
  [Val @ step 1750]: train=6.3944, val=6.3369
Step 1800: loss = 6.1685
  [Val @ step 1800]: train=6.1685, val=6.3565
Step 1850: loss = 6.3392
  [Val @ step 1850]: train=6.3392, val=6.2992
Step 1900: loss = 6.2998
  [Val @ step 1900]: train=6.2998, val=6.2912
Step 1950: loss = 6.0913
  [Val @ step 1950]: train=6.0913, val=6.2661
Step 2000: loss = 6.2019
  [Val @ step 2000]: train=6.2019, val=6.2737
Step 2050: loss = 6.1600
  [Val @ step 2050]: train=6.1600, val=6.2640
Step 2100: loss = 6.1308
  [Val @ step 2100]: train=6.1308, val=6.2126
Step 2150: loss = 6.0782
  [Val @ step 2150]: train=6.0782, val=6.2237
Step 2200: loss = 6.1938
  [Val @ step 2200]: train=6.1938, val=6.2659
Step 2250: loss = 6.0826
  [Val @ step 2250]: train=6.0826, val=6.1989
Step 2300: loss = 6.1841
  [Val @ step 2300]: train=6.1841, val=6.2172
Step 2350: loss = 6.1388
  [Val @ step 2350]: train=6.1388, val=6.1901
Step 2400: loss = 6.2056
  [Val @ step 2400]: train=6.2056, val=6.2164
Step 2450: loss = 6.1607
  [Val @ step 2450]: train=6.1607, val=6.1807
Step 2500: loss = 6.2582
  [Val @ step 2500]: train=6.2582, val=6.1639
Step 2550: loss = 6.2521
  [Val @ step 2550]: train=6.2521, val=6.1818
Step 2600: loss = 6.2171
  [Val @ step 2600]: train=6.2171, val=6.1156
Step 2650: loss = 6.1564
  [Val @ step 2650]: train=6.1564, val=6.1794
Step 2700: loss = 6.2856
  [Val @ step 2700]: train=6.2856, val=6.1827
Step 2750: loss = 6.2061
  [Val @ step 2750]: train=6.2061, val=6.1846
Step 2800: loss = 6.2842
  [Val @ step 2800]: train=6.2842, val=6.1331
Step 2850: loss = 6.1829
  [Val @ step 2850]: train=6.1829, val=6.1605
Step 2900: loss = 6.1737
  [Val @ step 2900]: train=6.1737, val=6.1351
Step 2950: loss = 5.9727
  [Val @ step 2950]: train=5.9727, val=6.1510
Step 3000: loss = 6.0839
  [Val @ step 3000]: train=6.0839, val=6.1173
Step 3050: loss = 5.9291
  [Val @ step 3050]: train=5.9291, val=6.0960
Step 3100: loss = 6.1713
  [Val @ step 3100]: train=6.1713, val=6.0704
Step 3150: loss = 6.2721
  [Val @ step 3150]: train=6.2721, val=6.1078
Step 3200: loss = 6.3790
  [Val @ step 3200]: train=6.3790, val=6.0927
Step 3250: loss = 6.1150
  [Val @ step 3250]: train=6.1150, val=6.0720
Step 3300: loss = 6.2935
  [Val @ step 3300]: train=6.2935, val=6.0422
Step 3350: loss = 6.2386
  [Val @ step 3350]: train=6.2386, val=6.0532
Step 3400: loss = 6.0194
  [Val @ step 3400]: train=6.0194, val=6.0539
Step 3450: loss = 6.1419
  [Val @ step 3450]: train=6.1419, val=6.0801
Step 3500: loss = 6.0459
  [Val @ step 3500]: train=6.0459, val=6.0931
Step 3550: loss = 6.0973
  [Val @ step 3550]: train=6.0973, val=6.0612
Step 3600: loss = 5.9911
  [Val @ step 3600]: train=5.9911, val=6.0761
Step 3650: loss = 5.9235
  [Val @ step 3650]: train=5.9235, val=6.0343
Step 3700: loss = 6.0652
  [Val @ step 3700]: train=6.0652, val=6.0695
Step 3750: loss = 6.2017
  [Val @ step 3750]: train=6.2017, val=6.0275
Step 3800: loss = 6.0012
  [Val @ step 3800]: train=6.0012, val=6.0528
Step 3850: loss = 6.0761
  [Val @ step 3850]: train=6.0761, val=5.9986
Step 3900: loss = 6.1180
  [Val @ step 3900]: train=6.1180, val=5.9887
Step 3950: loss = 5.9644
  [Val @ step 3950]: train=5.9644, val=6.0340
Step 4000: loss = 6.0686
  [Val @ step 4000]: train=6.0686, val=5.9927
Step 4050: loss = 6.1158
  [Val @ step 4050]: train=6.1158, val=5.9754
Step 4100: loss = 5.8038
  [Val @ step 4100]: train=5.8038, val=5.9863
Step 4150: loss = 6.2299
  [Val @ step 4150]: train=6.2299, val=6.0176
Step 4200: loss = 6.0912
  [Val @ step 4200]: train=6.0912, val=5.9912
Step 4250: loss = 6.1174
  [Val @ step 4250]: train=6.1174, val=5.9809
Step 4300: loss = 5.9111
  [Val @ step 4300]: train=5.9111, val=5.9857
Step 4350: loss = 5.8761
  [Val @ step 4350]: train=5.8761, val=6.0006
Step 4400: loss = 6.1782
  [Val @ step 4400]: train=6.1782, val=5.9739
Step 4450: loss = 5.8908
  [Val @ step 4450]: train=5.8908, val=5.9839
Step 4500: loss = 5.9352
  [Val @ step 4500]: train=5.9352, val=6.0023
Step 4550: loss = 5.9146
  [Val @ step 4550]: train=5.9146, val=5.9632
Step 4600: loss = 6.0170
  [Val @ step 4600]: train=6.0170, val=5.9839
Step 4650: loss = 5.9568
  [Val @ step 4650]: train=5.9568, val=5.9791
Step 4700: loss = 6.0101
  [Val @ step 4700]: train=6.0101, val=6.0014
Step 4750: loss = 6.0756
  [Val @ step 4750]: train=6.0756, val=5.9624
Step 4800: loss = 5.7519
  [Val @ step 4800]: train=5.7519, val=5.9841
Step 4850: loss = 5.7792
  [Val @ step 4850]: train=5.7792, val=5.9387
Step 4900: loss = 5.9773
  [Val @ step 4900]: train=5.9773, val=5.9442
Step 4950: loss = 5.7362
  [Val @ step 4950]: train=5.7362, val=5.9756
--------------------------------------------------------------------------------
Computing final validation loss...
Validation loss: 5.9275

Results saved to time_to_target_stage1_20251225_225546.jsonl
Training time: 3894.6s
Throughput: 5,259 tokens/sec
Projections: 715/5000
nGPT Architectural Experiment: D_lr0.001
================================================================================
Model: 11 layers x 768 dim (matching modded-nanogpt)
Hypothesis: Baseline
================================================================================

Loading FineWeb data (subset for fast experiments)...
  Loading data/fineweb10B/fineweb_train_000001.bin...
  Loading data/fineweb10B/fineweb_train_000002.bin...
  Loading data/fineweb10B/fineweb_train_000003.bin...
  Loading data/fineweb10B/fineweb_train_000004.bin...
  Vocab size: 50257
  Train tokens: 400,001,536
  Val tokens: 100,000,384

Model config (modded-nanogpt size):
  vocab_size: 50257
  n_layer: 11
  n_embd: 768
  n_head: 6
  block_size: 128
Using GatedBlock (H3) with gate_init=0.0

Total parameters: 155,166,743

Optimizer: SimpleMuon (lr=0.001, momentum=0.95, geodesic=baseline, orthog=newton_schulz)
  2D weight params (Muon): 77,856,768
  1D params (Adam): 77,309,975

Starting training (5000 steps)...
--------------------------------------------------------------------------------
Step    0: loss = 10.9744
Step   50: loss = 9.2083
  [Val @ step 50]: train=9.2083, val=9.1207
Step  100: loss = 7.1683
  [Val @ step 100]: train=7.1683, val=7.4258
Step  150: loss = 7.3798
  [Val @ step 150]: train=7.3798, val=7.2457
Step  200: loss = 7.0886
  [Val @ step 200]: train=7.0886, val=7.0864
Step  250: loss = 7.1079
  [Val @ step 250]: train=7.1079, val=6.9649
Step  300: loss = 6.9171
  [Val @ step 300]: train=6.9171, val=6.8633
Step  350: loss = 6.7298
  [Val @ step 350]: train=6.7298, val=6.7679
Step  400: loss = 6.6375
  [Val @ step 400]: train=6.6375, val=6.7217
Step  450: loss = 6.6464
  [Val @ step 450]: train=6.6464, val=6.6489
Step  500: loss = 6.4497
  [Val @ step 500]: train=6.4497, val=6.6182
Step  550: loss = 6.5877
  [Val @ step 550]: train=6.5877, val=6.5696
Step  600: loss = 6.3401
  [Val @ step 600]: train=6.3401, val=6.5882
Step  650: loss = 6.4037
  [Val @ step 650]: train=6.4037, val=6.4891
Step  700: loss = 6.3161
  [Val @ step 700]: train=6.3161, val=6.4801
Step  750: loss = 6.3643
  [Val @ step 750]: train=6.3643, val=6.4281
Step  800: loss = 6.4486
  [Val @ step 800]: train=6.4486, val=6.4388
Step  850: loss = 6.3404
  [Val @ step 850]: train=6.3404, val=6.3833
Step  900: loss = 6.4381
  [Val @ step 900]: train=6.4381, val=6.3439
Step  950: loss = 6.2553
  [Val @ step 950]: train=6.2553, val=6.3074
Step 1000: loss = 6.4105
  [Val @ step 1000]: train=6.4105, val=6.3272
Step 1050: loss = 6.1444
  [Val @ step 1050]: train=6.1444, val=6.2789
Step 1100: loss = 6.2263
  [Val @ step 1100]: train=6.2263, val=6.2768
Step 1150: loss = 6.4021
  [Val @ step 1150]: train=6.4021, val=6.2850
Step 1200: loss = 6.3318
  [Val @ step 1200]: train=6.3318, val=6.2668
Step 1250: loss = 6.3098
  [Val @ step 1250]: train=6.3098, val=6.2280
Step 1300: loss = 6.2167
  [Val @ step 1300]: train=6.2167, val=6.2511
Step 1350: loss = 6.2429
  [Val @ step 1350]: train=6.2429, val=6.2142
Step 1400: loss = 6.2579
  [Val @ step 1400]: train=6.2579, val=6.1936
Step 1450: loss = 6.0177
  [Val @ step 1450]: train=6.0177, val=6.1547
Step 1500: loss = 6.1440
  [Val @ step 1500]: train=6.1440, val=6.1756
Step 1550: loss = 6.1159
  [Val @ step 1550]: train=6.1159, val=6.1162
Step 1600: loss = 6.1150
  [Val @ step 1600]: train=6.1150, val=6.1550
Step 1650: loss = 6.1597
  [Val @ step 1650]: train=6.1597, val=6.0960
Step 1700: loss = 6.1418
  [Val @ step 1700]: train=6.1418, val=6.1291
Step 1750: loss = 6.0893
  [Val @ step 1750]: train=6.0893, val=6.1134
Step 1800: loss = 6.2332
  [Val @ step 1800]: train=6.2332, val=6.0982
Step 1850: loss = 6.2323
  [Val @ step 1850]: train=6.2323, val=6.0575
Step 1900: loss = 6.1040
  [Val @ step 1900]: train=6.1040, val=6.0878
Step 1950: loss = 6.0760
  [Val @ step 1950]: train=6.0760, val=6.1025
Step 2000: loss = 6.1085
  [Val @ step 2000]: train=6.1085, val=6.0688
Step 2050: loss = 6.0502
  [Val @ step 2050]: train=6.0502, val=6.0539
Step 2100: loss = 6.0317
  [Val @ step 2100]: train=6.0317, val=6.0523
Step 2150: loss = 6.0288
  [Val @ step 2150]: train=6.0288, val=6.0481
Step 2200: loss = 5.9734
  [Val @ step 2200]: train=5.9734, val=5.9925
Step 2250: loss = 5.8261
  [Val @ step 2250]: train=5.8261, val=6.0748
Step 2300: loss = 5.9160
  [Val @ step 2300]: train=5.9160, val=6.0511
Step 2350: loss = 6.0748
  [Val @ step 2350]: train=6.0748, val=6.0114
Step 2400: loss = 5.9518
  [Val @ step 2400]: train=5.9518, val=5.9967
Step 2450: loss = 6.1268
  [Val @ step 2450]: train=6.1268, val=6.0333
Step 2500: loss = 5.8739
  [Val @ step 2500]: train=5.8739, val=5.9625
Step 2550: loss = 6.0713
  [Val @ step 2550]: train=6.0713, val=6.0010
Step 2600: loss = 5.8270
  [Val @ step 2600]: train=5.8270, val=5.9628
Step 2650: loss = 5.9100
  [Val @ step 2650]: train=5.9100, val=5.9662
Step 2700: loss = 5.8568
  [Val @ step 2700]: train=5.8568, val=5.9617
Step 2750: loss = 6.0993
  [Val @ step 2750]: train=6.0993, val=5.9381
Step 2800: loss = 6.0188
  [Val @ step 2800]: train=6.0188, val=5.9621
Step 2850: loss = 5.7172
  [Val @ step 2850]: train=5.7172, val=5.9198
Step 2900: loss = 5.8725
  [Val @ step 2900]: train=5.8725, val=5.9248
Step 2950: loss = 5.7722
  [Val @ step 2950]: train=5.7722, val=5.9364
Step 3000: loss = 6.0323
  [Val @ step 3000]: train=6.0323, val=5.9300
Step 3050: loss = 5.7203
  [Val @ step 3050]: train=5.7203, val=5.9104
Step 3100: loss = 5.8698
  [Val @ step 3100]: train=5.8698, val=5.9507
Step 3150: loss = 5.9420
  [Val @ step 3150]: train=5.9420, val=5.9186
Step 3200: loss = 5.8607
  [Val @ step 3200]: train=5.8607, val=5.8885
Step 3250: loss = 5.9052
  [Val @ step 3250]: train=5.9052, val=5.9118
Step 3300: loss = 5.9056
  [Val @ step 3300]: train=5.9056, val=5.8775
Step 3350: loss = 5.9089
  [Val @ step 3350]: train=5.9089, val=5.9138
Step 3400: loss = 5.7862
  [Val @ step 3400]: train=5.7862, val=5.8713
Step 3450: loss = 5.8735
  [Val @ step 3450]: train=5.8735, val=5.9025
Step 3500: loss = 5.7487
  [Val @ step 3500]: train=5.7487, val=5.8983
Step 3550: loss = 5.7184
  [Val @ step 3550]: train=5.7184, val=5.9076
Step 3600: loss = 5.9589
  [Val @ step 3600]: train=5.9589, val=5.8876
Step 3650: loss = 5.7703
  [Val @ step 3650]: train=5.7703, val=5.8619
Step 3700: loss = 5.8447
  [Val @ step 3700]: train=5.8447, val=5.8519
Step 3750: loss = 5.9041
  [Val @ step 3750]: train=5.9041, val=5.8269
Step 3800: loss = 5.8705
  [Val @ step 3800]: train=5.8705, val=5.8666
Step 3850: loss = 5.9468
  [Val @ step 3850]: train=5.9468, val=5.8079
Step 3900: loss = 5.6881
  [Val @ step 3900]: train=5.6881, val=5.8701
Step 3950: loss = 5.8420
  [Val @ step 3950]: train=5.8420, val=5.8710
Step 4000: loss = 5.8913
  [Val @ step 4000]: train=5.8913, val=5.8313
Step 4050: loss = 5.8527
  [Val @ step 4050]: train=5.8527, val=5.8110
Step 4100: loss = 5.7230
  [Val @ step 4100]: train=5.7230, val=5.8556
Step 4150: loss = 5.8894
  [Val @ step 4150]: train=5.8894, val=5.8521
Step 4200: loss = 5.5717
  [Val @ step 4200]: train=5.5717, val=5.8265
Step 4250: loss = 5.8670
  [Val @ step 4250]: train=5.8670, val=5.8244
Step 4300: loss = 5.8666
  [Val @ step 4300]: train=5.8666, val=5.8179
Step 4350: loss = 5.7191
  [Val @ step 4350]: train=5.7191, val=5.7730
Step 4400: loss = 5.8767
  [Val @ step 4400]: train=5.8767, val=5.8093
Step 4450: loss = 5.8938
  [Val @ step 4450]: train=5.8938, val=5.7934
Step 4500: loss = 5.8400
  [Val @ step 4500]: train=5.8400, val=5.8060
Step 4550: loss = 5.8319
  [Val @ step 4550]: train=5.8319, val=5.7673
Step 4600: loss = 5.8712
  [Val @ step 4600]: train=5.8712, val=5.8139
Step 4650: loss = 5.8033
  [Val @ step 4650]: train=5.8033, val=5.7780
Step 4700: loss = 5.8049
  [Val @ step 4700]: train=5.8049, val=5.8440
Step 4750: loss = 5.9684
  [Val @ step 4750]: train=5.9684, val=5.7896
Step 4800: loss = 5.7623
  [Val @ step 4800]: train=5.7623, val=5.7757
Step 4850: loss = 5.9512
  [Val @ step 4850]: train=5.9512, val=5.7884
Step 4900: loss = 5.6530
  [Val @ step 4900]: train=5.6530, val=5.7886
Step 4950: loss = 5.6310
  [Val @ step 4950]: train=5.6310, val=5.8289
--------------------------------------------------------------------------------
Computing final validation loss...
Validation loss: 5.7311

Results saved to time_to_target_stage1_20251225_225546.jsonl
Training time: 3918.8s
Throughput: 5,226 tokens/sec
Projections: 715/5000
nGPT Architectural Experiment: D_lr0.003
================================================================================
Model: 11 layers x 768 dim (matching modded-nanogpt)
Hypothesis: Baseline
================================================================================

Loading FineWeb data (subset for fast experiments)...
  Loading data/fineweb10B/fineweb_train_000001.bin...
  Loading data/fineweb10B/fineweb_train_000002.bin...
  Loading data/fineweb10B/fineweb_train_000003.bin...
  Loading data/fineweb10B/fineweb_train_000004.bin...
  Vocab size: 50257
  Train tokens: 400,001,536
  Val tokens: 100,000,384

Model config (modded-nanogpt size):
  vocab_size: 50257
  n_layer: 11
  n_embd: 768
  n_head: 6
  block_size: 128
Using GatedBlock (H3) with gate_init=0.0

Total parameters: 155,166,743

Optimizer: SimpleMuon (lr=0.003, momentum=0.95, geodesic=baseline, orthog=newton_schulz)
  2D weight params (Muon): 77,856,768
  1D params (Adam): 77,309,975

Starting training (5000 steps)...
--------------------------------------------------------------------------------
Step    0: loss = 10.9760
Step   50: loss = 7.1696
  [Val @ step 50]: train=7.1696, val=7.3589
Step  100: loss = 7.0464
  [Val @ step 100]: train=7.0464, val=6.9689
Step  150: loss = 6.7707
  [Val @ step 150]: train=6.7707, val=6.7678
Step  200: loss = 6.7998
  [Val @ step 200]: train=6.7998, val=6.6379
Step  250: loss = 6.5811
  [Val @ step 250]: train=6.5811, val=6.5369
Step  300: loss = 6.4785
  [Val @ step 300]: train=6.4785, val=6.4455
Step  350: loss = 6.6141
  [Val @ step 350]: train=6.6141, val=6.4300
Step  400: loss = 6.4097
  [Val @ step 400]: train=6.4097, val=6.3730
Step  450: loss = 6.4681
  [Val @ step 450]: train=6.4681, val=6.3780
Step  500: loss = 6.3959
  [Val @ step 500]: train=6.3959, val=6.3145
Step  550: loss = 6.2839
  [Val @ step 550]: train=6.2839, val=6.3116
Step  600: loss = 6.2188
  [Val @ step 600]: train=6.2188, val=6.2472
Step  650: loss = 6.2272
  [Val @ step 650]: train=6.2272, val=6.2283
Step  700: loss = 6.1699
  [Val @ step 700]: train=6.1699, val=6.2570
Step  750: loss = 6.3960
  [Val @ step 750]: train=6.3960, val=6.2206
Step  800: loss = 6.0575
  [Val @ step 800]: train=6.0575, val=6.2051
Step  850: loss = 6.1887
  [Val @ step 850]: train=6.1887, val=6.1711
Step  900: loss = 6.1039
  [Val @ step 900]: train=6.1039, val=6.1345
Step  950: loss = 6.0236
  [Val @ step 950]: train=6.0236, val=6.1146
Step 1000: loss = 6.1206
  [Val @ step 1000]: train=6.1206, val=6.0892
Step 1050: loss = 6.0694
  [Val @ step 1050]: train=6.0694, val=6.0811
Step 1100: loss = 6.2067
  [Val @ step 1100]: train=6.2067, val=6.0911
Step 1150: loss = 6.1431
  [Val @ step 1150]: train=6.1431, val=6.0906
Step 1200: loss = 6.1742
  [Val @ step 1200]: train=6.1742, val=6.0494
Step 1250: loss = 6.1244
  [Val @ step 1250]: train=6.1244, val=6.0438
Step 1300: loss = 5.8384
  [Val @ step 1300]: train=5.8384, val=6.0589
Step 1350: loss = 6.1045
  [Val @ step 1350]: train=6.1045, val=6.0247
Step 1400: loss = 6.0227
  [Val @ step 1400]: train=6.0227, val=5.9735
Step 1450: loss = 6.0267
  [Val @ step 1450]: train=6.0267, val=6.0459
Step 1500: loss = 5.8929
  [Val @ step 1500]: train=5.8929, val=6.0119
Step 1550: loss = 6.0795
  [Val @ step 1550]: train=6.0795, val=5.9907
Step 1600: loss = 5.9557
  [Val @ step 1600]: train=5.9557, val=5.9578
Step 1650: loss = 5.9913
  [Val @ step 1650]: train=5.9913, val=5.9380
Step 1700: loss = 5.9169
  [Val @ step 1700]: train=5.9169, val=5.9507
Step 1750: loss = 5.9073
  [Val @ step 1750]: train=5.9073, val=5.9339
Step 1800: loss = 5.8805
  [Val @ step 1800]: train=5.8805, val=5.9349
Step 1850: loss = 5.8235
  [Val @ step 1850]: train=5.8235, val=5.9593
Step 1900: loss = 5.9419
  [Val @ step 1900]: train=5.9419, val=5.9497
Step 1950: loss = 5.9897
  [Val @ step 1950]: train=5.9897, val=5.9029
Step 2000: loss = 5.7213
  [Val @ step 2000]: train=5.7213, val=5.9346
Step 2050: loss = 5.8463
  [Val @ step 2050]: train=5.8463, val=5.8981
Step 2100: loss = 5.8471
  [Val @ step 2100]: train=5.8471, val=5.9389
Step 2150: loss = 5.8296
  [Val @ step 2150]: train=5.8296, val=5.8856
Step 2200: loss = 6.0695
  [Val @ step 2200]: train=6.0695, val=5.9076
Step 2250: loss = 5.8114
  [Val @ step 2250]: train=5.8114, val=5.8827
Step 2300: loss = 5.8637
  [Val @ step 2300]: train=5.8637, val=5.9134
Step 2350: loss = 6.0523
  [Val @ step 2350]: train=6.0523, val=5.8991
Step 2400: loss = 5.8663
  [Val @ step 2400]: train=5.8663, val=5.8854
Step 2450: loss = 5.9335
  [Val @ step 2450]: train=5.9335, val=5.8884
Step 2500: loss = 5.8091
  [Val @ step 2500]: train=5.8091, val=5.8402
Step 2550: loss = 5.8079
  [Val @ step 2550]: train=5.8079, val=5.8459
Step 2600: loss = 5.9262
  [Val @ step 2600]: train=5.9262, val=5.8529
Step 2650: loss = 5.8081
  [Val @ step 2650]: train=5.8081, val=5.8531
Step 2700: loss = 5.7725
  [Val @ step 2700]: train=5.7725, val=5.8491
Step 2750: loss = 5.6660
  [Val @ step 2750]: train=5.6660, val=5.7905
Step 2800: loss = 5.8940
  [Val @ step 2800]: train=5.8940, val=5.8918
Step 2850: loss = 5.6932
  [Val @ step 2850]: train=5.6932, val=5.8274
Step 2900: loss = 5.8523
  [Val @ step 2900]: train=5.8523, val=5.8585
Step 2950: loss = 5.8600
  [Val @ step 2950]: train=5.8600, val=5.8087
Step 3000: loss = 5.7988
  [Val @ step 3000]: train=5.7988, val=5.8168
Step 3050: loss = 5.8943
  [Val @ step 3050]: train=5.8943, val=5.8088
Step 3100: loss = 5.8628
  [Val @ step 3100]: train=5.8628, val=5.7951
Step 3150: loss = 5.7811
  [Val @ step 3150]: train=5.7811, val=5.7903
Step 3200: loss = 5.8814
  [Val @ step 3200]: train=5.8814, val=5.8127
Step 3250: loss = 5.7951
  [Val @ step 3250]: train=5.7951, val=5.7888
Step 3300: loss = 5.8964
  [Val @ step 3300]: train=5.8964, val=5.8124
Step 3350: loss = 5.7256
  [Val @ step 3350]: train=5.7256, val=5.8350
Step 3400: loss = 5.7893
  [Val @ step 3400]: train=5.7893, val=5.7516
Step 3450: loss = 5.7024
  [Val @ step 3450]: train=5.7024, val=5.8330
Step 3500: loss = 5.9028
  [Val @ step 3500]: train=5.9028, val=5.7870
Step 3550: loss = 5.8309
  [Val @ step 3550]: train=5.8309, val=5.8265
Step 3600: loss = 5.6644
  [Val @ step 3600]: train=5.6644, val=5.8270
Step 3650: loss = 5.7261
  [Val @ step 3650]: train=5.7261, val=5.7800
Step 3700: loss = 5.6936
  [Val @ step 3700]: train=5.6936, val=5.7309
Step 3750: loss = 5.7743
  [Val @ step 3750]: train=5.7743, val=5.7539
Step 3800: loss = 5.6964
  [Val @ step 3800]: train=5.6964, val=5.7868
Step 3850: loss = 5.7292
  [Val @ step 3850]: train=5.7292, val=5.8072
Step 3900: loss = 5.7775
  [Val @ step 3900]: train=5.7775, val=5.7654
Step 3950: loss = 5.5523
  [Val @ step 3950]: train=5.5523, val=5.7583
Step 4000: loss = 5.9964
  [Val @ step 4000]: train=5.9964, val=5.7942
Step 4050: loss = 5.7586
  [Val @ step 4050]: train=5.7586, val=5.8166
Step 4100: loss = 5.7808
  [Val @ step 4100]: train=5.7808, val=5.7770
Step 4150: loss = 5.6960
  [Val @ step 4150]: train=5.6960, val=5.7967
Step 4200: loss = 5.7424
  [Val @ step 4200]: train=5.7424, val=5.7355
Step 4250: loss = 5.6642
  [Val @ step 4250]: train=5.6642, val=5.7561
Step 4300: loss = 5.7796
  [Val @ step 4300]: train=5.7796, val=5.7533
Step 4350: loss = 5.7363
  [Val @ step 4350]: train=5.7363, val=5.7282
Step 4400: loss = 5.8405
  [Val @ step 4400]: train=5.8405, val=5.7630
Step 4450: loss = 5.7401
  [Val @ step 4450]: train=5.7401, val=5.7642
Step 4500: loss = 5.6794
  [Val @ step 4500]: train=5.6794, val=5.7638
Step 4550: loss = 5.7111
  [Val @ step 4550]: train=5.7111, val=5.7770
Step 4600: loss = 5.5805
  [Val @ step 4600]: train=5.5805, val=5.7774
Step 4650: loss = 5.7133
  [Val @ step 4650]: train=5.7133, val=5.7332
Step 4700: loss = 5.7770
  [Val @ step 4700]: train=5.7770, val=5.7306
Step 4750: loss = 5.8589
  [Val @ step 4750]: train=5.8589, val=5.7504
Step 4800: loss = 5.6532
  [Val @ step 4800]: train=5.6532, val=5.7678
Step 4850: loss = 5.4333
  [Val @ step 4850]: train=5.4333, val=5.7215
Step 4900: loss = 5.7602
  [Val @ step 4900]: train=5.7602, val=5.7958
Step 4950: loss = 5.6390
  [Val @ step 4950]: train=5.6390, val=5.7209
--------------------------------------------------------------------------------
Computing final validation loss...
Validation loss: 5.7588

Results saved to time_to_target_stage1_20251225_225546.jsonl
Training time: 3947.9s
Throughput: 5,188 tokens/sec
Projections: 715/5000
nGPT Architectural Experiment: D_lr0.005
================================================================================
Model: 11 layers x 768 dim (matching modded-nanogpt)
Hypothesis: Baseline
================================================================================

Loading FineWeb data (subset for fast experiments)...
  Loading data/fineweb10B/fineweb_train_000001.bin...
  Loading data/fineweb10B/fineweb_train_000002.bin...
  Loading data/fineweb10B/fineweb_train_000003.bin...
  Loading data/fineweb10B/fineweb_train_000004.bin...
  Vocab size: 50257
  Train tokens: 400,001,536
  Val tokens: 100,000,384

Model config (modded-nanogpt size):
  vocab_size: 50257
  n_layer: 11
  n_embd: 768
  n_head: 6
  block_size: 128
Using GatedBlock (H3) with gate_init=0.0

Total parameters: 155,166,743

Optimizer: SimpleMuon (lr=0.005, momentum=0.95, geodesic=baseline, orthog=newton_schulz)
  2D weight params (Muon): 77,856,768
  1D params (Adam): 77,309,975

Starting training (5000 steps)...
--------------------------------------------------------------------------------
Step    0: loss = 10.9751
Step   50: loss = 7.2869
  [Val @ step 50]: train=7.2869, val=7.1570
Step  100: loss = 6.5688
  [Val @ step 100]: train=6.5688, val=6.8307
Step  150: loss = 6.4496
  [Val @ step 150]: train=6.4496, val=6.6984
Step  200: loss = 6.5880
  [Val @ step 200]: train=6.5880, val=6.5983
Step  250: loss = 6.3563
  [Val @ step 250]: train=6.3563, val=6.5198
Step  300: loss = 6.6673
  [Val @ step 300]: train=6.6673, val=6.4190
Step  350: loss = 6.5153
  [Val @ step 350]: train=6.5153, val=6.4303
Step  400: loss = 6.4391
  [Val @ step 400]: train=6.4391, val=6.3741
Step  450: loss = 6.1303
  [Val @ step 450]: train=6.1303, val=6.3449
Step  500: loss = 6.2900
  [Val @ step 500]: train=6.2900, val=6.2690
Step  550: loss = 6.1404
  [Val @ step 550]: train=6.1404, val=6.2766
Step  600: loss = 6.2327
  [Val @ step 600]: train=6.2327, val=6.2539
Step  650: loss = 6.2072
  [Val @ step 650]: train=6.2072, val=6.2253
Step  700: loss = 6.2190
  [Val @ step 700]: train=6.2190, val=6.2295
Step  750: loss = 6.1883
  [Val @ step 750]: train=6.1883, val=6.2041
Step  800: loss = 6.2273
  [Val @ step 800]: train=6.2273, val=6.1677
Step  850: loss = 6.0022
  [Val @ step 850]: train=6.0022, val=6.1412
Step  900: loss = 5.9376
  [Val @ step 900]: train=5.9376, val=6.1664
Step  950: loss = 6.0072
  [Val @ step 950]: train=6.0072, val=6.0832
Step 1000: loss = 6.0970
  [Val @ step 1000]: train=6.0970, val=6.1183
Step 1050: loss = 6.1018
  [Val @ step 1050]: train=6.1018, val=6.1110
Step 1100: loss = 6.0863
  [Val @ step 1100]: train=6.0863, val=6.0681
Step 1150: loss = 6.0367
  [Val @ step 1150]: train=6.0367, val=6.0812
Step 1200: loss = 6.1375
  [Val @ step 1200]: train=6.1375, val=6.0692
Step 1250: loss = 6.0272
  [Val @ step 1250]: train=6.0272, val=6.0853
Step 1300: loss = 6.1493
  [Val @ step 1300]: train=6.1493, val=6.0892
Step 1350: loss = 6.0762
  [Val @ step 1350]: train=6.0762, val=6.0407
Step 1400: loss = 5.7907
  [Val @ step 1400]: train=5.7907, val=6.0099
Step 1450: loss = 5.8441
  [Val @ step 1450]: train=5.8441, val=6.0664
Step 1500: loss = 5.9870
  [Val @ step 1500]: train=5.9870, val=6.0248
Step 1550: loss = 6.1399
  [Val @ step 1550]: train=6.1399, val=6.0463
Step 1600: loss = 5.8791
  [Val @ step 1600]: train=5.8791, val=5.9776
Step 1650: loss = 5.8767
  [Val @ step 1650]: train=5.8767, val=5.9420
Step 1700: loss = 5.9062
  [Val @ step 1700]: train=5.9062, val=5.9729
Step 1750: loss = 5.9408
  [Val @ step 1750]: train=5.9408, val=5.9762
Step 1800: loss = 5.9646
  [Val @ step 1800]: train=5.9646, val=5.9717
Step 1850: loss = 6.1360
  [Val @ step 1850]: train=6.1360, val=5.9949
Step 1900: loss = 6.0321
  [Val @ step 1900]: train=6.0321, val=5.9645
Step 1950: loss = 6.0718
  [Val @ step 1950]: train=6.0718, val=5.9403
Step 2000: loss = 6.0610
  [Val @ step 2000]: train=6.0610, val=5.9573
Step 2050: loss = 5.9707
  [Val @ step 2050]: train=5.9707, val=5.9546
Step 2100: loss = 5.8242
  [Val @ step 2100]: train=5.8242, val=5.9730
Step 2150: loss = 5.9453
  [Val @ step 2150]: train=5.9453, val=5.9605
Step 2200: loss = 5.9131
  [Val @ step 2200]: train=5.9131, val=5.9685
Step 2250: loss = 5.9894
  [Val @ step 2250]: train=5.9894, val=5.9355
Step 2300: loss = 6.1456
  [Val @ step 2300]: train=6.1456, val=5.9621
Step 2350: loss = 5.8898
  [Val @ step 2350]: train=5.8898, val=5.9274
Step 2400: loss = 5.8913
  [Val @ step 2400]: train=5.8913, val=5.9362
Step 2450: loss = 5.9749
  [Val @ step 2450]: train=5.9749, val=5.9696
Step 2500: loss = 5.8038
  [Val @ step 2500]: train=5.8038, val=5.9442
Step 2550: loss = 5.8882
  [Val @ step 2550]: train=5.8882, val=5.9193
Step 2600: loss = 5.8311
  [Val @ step 2600]: train=5.8311, val=5.9295
Step 2650: loss = 6.0649
  [Val @ step 2650]: train=6.0649, val=5.9021
Step 2700: loss = 5.9575
  [Val @ step 2700]: train=5.9575, val=5.9136
Step 2750: loss = 5.8793
  [Val @ step 2750]: train=5.8793, val=5.9243
Step 2800: loss = 5.8000
  [Val @ step 2800]: train=5.8000, val=5.9230
Step 2850: loss = 5.8924
  [Val @ step 2850]: train=5.8924, val=5.9343
Step 2900: loss = 5.8386
  [Val @ step 2900]: train=5.8386, val=5.9098
Step 2950: loss = 5.8565
  [Val @ step 2950]: train=5.8565, val=5.9147
Step 3000: loss = 5.9457
  [Val @ step 3000]: train=5.9457, val=5.9251
Step 3050: loss = 5.8089
  [Val @ step 3050]: train=5.8089, val=5.8930
Step 3100: loss = 5.9819
  [Val @ step 3100]: train=5.9819, val=5.9221
Step 3150: loss = 5.9943
  [Val @ step 3150]: train=5.9943, val=5.9038
Step 3200: loss = 5.8595
  [Val @ step 3200]: train=5.8595, val=5.8960
Step 3250: loss = 5.9478
  [Val @ step 3250]: train=5.9478, val=5.9362
Step 3300: loss = 5.9503
  [Val @ step 3300]: train=5.9503, val=5.8769
Step 3350: loss = 5.8474
  [Val @ step 3350]: train=5.8474, val=5.9247
Step 3400: loss = 5.8176
  [Val @ step 3400]: train=5.8176, val=5.9085
Step 3450: loss = 5.8443
  [Val @ step 3450]: train=5.8443, val=5.9281
Step 3500: loss = 5.9029
  [Val @ step 3500]: train=5.9029, val=5.8975
Step 3550: loss = 5.9030
  [Val @ step 3550]: train=5.9030, val=5.9228
Step 3600: loss = 5.8621
  [Val @ step 3600]: train=5.8621, val=5.8791
Step 3650: loss = 6.1186
  [Val @ step 3650]: train=6.1186, val=5.8776
Step 3700: loss = 5.7092
  [Val @ step 3700]: train=5.7092, val=5.8946
Step 3750: loss = 5.8661
  [Val @ step 3750]: train=5.8661, val=5.8624
Step 3800: loss = 5.8947
  [Val @ step 3800]: train=5.8947, val=5.9045
Step 3850: loss = 5.9521
  [Val @ step 3850]: train=5.9521, val=5.8706
Step 3900: loss = 6.0337
  [Val @ step 3900]: train=6.0337, val=5.8867
Step 3950: loss = 5.9459
  [Val @ step 3950]: train=5.9459, val=5.8711
Step 4000: loss = 5.8458
  [Val @ step 4000]: train=5.8458, val=5.9023
Step 4050: loss = 5.8075
  [Val @ step 4050]: train=5.8075, val=5.8635
Step 4100: loss = 5.8515
  [Val @ step 4100]: train=5.8515, val=5.8868
Step 4150: loss = 5.9344
  [Val @ step 4150]: train=5.9344, val=5.8942
Step 4200: loss = 5.8787
  [Val @ step 4200]: train=5.8787, val=5.9163
Step 4250: loss = 5.8341
  [Val @ step 4250]: train=5.8341, val=5.8682
Step 4300: loss = 5.9143
  [Val @ step 4300]: train=5.9143, val=5.8120
Step 4350: loss = 5.6841
  [Val @ step 4350]: train=5.6841, val=5.8760
Step 4400: loss = 5.9808
  [Val @ step 4400]: train=5.9808, val=5.8472
Step 4450: loss = 5.8435
  [Val @ step 4450]: train=5.8435, val=5.8577
Step 4500: loss = 5.7423
  [Val @ step 4500]: train=5.7423, val=5.8556
Step 4550: loss = 5.8525
  [Val @ step 4550]: train=5.8525, val=5.8712
Step 4600: loss = 5.8239
  [Val @ step 4600]: train=5.8239, val=5.8346
Step 4650: loss = 6.0044
  [Val @ step 4650]: train=6.0044, val=5.8751
Step 4700: loss = 5.7520
  [Val @ step 4700]: train=5.7520, val=5.8521
Step 4750: loss = 5.9307
  [Val @ step 4750]: train=5.9307, val=5.8636
Step 4800: loss = 5.8831
  [Val @ step 4800]: train=5.8831, val=5.8649
Step 4850: loss = 5.8022
  [Val @ step 4850]: train=5.8022, val=5.8564
Step 4900: loss = 5.6556
  [Val @ step 4900]: train=5.6556, val=5.8455
Step 4950: loss = 5.7900
  [Val @ step 4950]: train=5.7900, val=5.8850
--------------------------------------------------------------------------------
Computing final validation loss...
Validation loss: 5.8417

Results saved to time_to_target_stage1_20251225_225546.jsonl
Training time: 3966.9s
Throughput: 5,163 tokens/sec
Projections: 715/5000
nGPT Architectural Experiment: E_batch16
================================================================================
Model: 11 layers x 768 dim (matching modded-nanogpt)
Hypothesis: Baseline
================================================================================

Loading FineWeb data (subset for fast experiments)...
  Loading data/fineweb10B/fineweb_train_000001.bin...
  Loading data/fineweb10B/fineweb_train_000002.bin...
  Loading data/fineweb10B/fineweb_train_000003.bin...
  Loading data/fineweb10B/fineweb_train_000004.bin...
  Vocab size: 50257
  Train tokens: 400,001,536
  Val tokens: 100,000,384

Model config (modded-nanogpt size):
  vocab_size: 50257
  n_layer: 11
  n_embd: 768
  n_head: 6
  block_size: 128
Using GatedBlock (H3) with gate_init=0.0

Total parameters: 155,166,743

Optimizer: SimpleMuon (lr=0.0015, momentum=0.95, geodesic=baseline, orthog=newton_schulz)
  2D weight params (Muon): 77,856,768
  1D params (Adam): 77,309,975

Starting training (5000 steps)...
--------------------------------------------------------------------------------
Step    0: loss = 10.9650
Step   50: loss = 8.1765
  [Val @ step 50]: train=8.1765, val=8.2265
Step  100: loss = 7.3838
  [Val @ step 100]: train=7.3838, val=7.4628
Step  150: loss = 7.1245
  [Val @ step 150]: train=7.1245, val=7.1559
Step  200: loss = 7.0334
  [Val @ step 200]: train=7.0334, val=7.0302
Step  250: loss = 6.9190
  [Val @ step 250]: train=6.9190, val=7.0251
Step  300: loss = 6.6146
  [Val @ step 300]: train=6.6146, val=6.8228
Step  350: loss = 6.4391
  [Val @ step 350]: train=6.4391, val=6.7663
Step  400: loss = 6.8644
  [Val @ step 400]: train=6.8644, val=6.7256
Step  450: loss = 7.2468
  [Val @ step 450]: train=7.2468, val=6.7027
Step  500: loss = 6.6139
  [Val @ step 500]: train=6.6139, val=6.6584
Step  550: loss = 6.6760
  [Val @ step 550]: train=6.6760, val=6.6057
Step  600: loss = 6.7260
  [Val @ step 600]: train=6.7260, val=6.5877
Step  650: loss = 6.7588
  [Val @ step 650]: train=6.7588, val=6.5819
Step  700: loss = 6.3169
  [Val @ step 700]: train=6.3169, val=6.4487
Step  750: loss = 6.3351
  [Val @ step 750]: train=6.3351, val=6.4220
Step  800: loss = 6.4733
  [Val @ step 800]: train=6.4733, val=6.4705
Step  850: loss = 6.2024
  [Val @ step 850]: train=6.2024, val=6.4372
Step  900: loss = 6.3517
  [Val @ step 900]: train=6.3517, val=6.3844
Step  950: loss = 6.7916
  [Val @ step 950]: train=6.7916, val=6.3499
Step 1000: loss = 6.3864
  [Val @ step 1000]: train=6.3864, val=6.3701
Step 1050: loss = 6.3591
  [Val @ step 1050]: train=6.3591, val=6.3622
Step 1100: loss = 6.4092
  [Val @ step 1100]: train=6.4092, val=6.3204
Step 1150: loss = 6.1732
  [Val @ step 1150]: train=6.1732, val=6.3257
Step 1200: loss = 6.3973
  [Val @ step 1200]: train=6.3973, val=6.2932
Step 1250: loss = 6.2436
  [Val @ step 1250]: train=6.2436, val=6.2940
Step 1300: loss = 6.4014
  [Val @ step 1300]: train=6.4014, val=6.3122
Step 1350: loss = 6.2860
  [Val @ step 1350]: train=6.2860, val=6.2509
Step 1400: loss = 6.1844
  [Val @ step 1400]: train=6.1844, val=6.2936
Step 1450: loss = 6.1040
  [Val @ step 1450]: train=6.1040, val=6.3040
Step 1500: loss = 6.2510
  [Val @ step 1500]: train=6.2510, val=6.2987
Step 1550: loss = 5.9660
  [Val @ step 1550]: train=5.9660, val=6.2612
Step 1600: loss = 6.2488
  [Val @ step 1600]: train=6.2488, val=6.2414
Step 1650: loss = 6.1845
  [Val @ step 1650]: train=6.1845, val=6.2171
Step 1700: loss = 6.1197
  [Val @ step 1700]: train=6.1197, val=6.2419
Step 1750: loss = 6.1086
  [Val @ step 1750]: train=6.1086, val=6.2150
Step 1800: loss = 6.0915
  [Val @ step 1800]: train=6.0915, val=6.1603
Step 1850: loss = 6.1961
  [Val @ step 1850]: train=6.1961, val=6.1557
Step 1900: loss = 6.0501
  [Val @ step 1900]: train=6.0501, val=6.1853
Step 1950: loss = 6.0182
  [Val @ step 1950]: train=6.0182, val=6.1723
Step 2000: loss = 6.0614
  [Val @ step 2000]: train=6.0614, val=6.1749
Step 2050: loss = 6.1280
  [Val @ step 2050]: train=6.1280, val=6.1257
Step 2100: loss = 6.2600
  [Val @ step 2100]: train=6.2600, val=6.1070
Step 2150: loss = 6.2862
  [Val @ step 2150]: train=6.2862, val=6.1417
Step 2200: loss = 5.9647
  [Val @ step 2200]: train=5.9647, val=6.1434
Step 2250: loss = 6.0906
  [Val @ step 2250]: train=6.0906, val=6.0698
Step 2300: loss = 6.2923
  [Val @ step 2300]: train=6.2923, val=6.0858
Step 2350: loss = 6.0451
  [Val @ step 2350]: train=6.0451, val=6.0724
Step 2400: loss = 6.2808
  [Val @ step 2400]: train=6.2808, val=6.0726
Step 2450: loss = 6.1408
  [Val @ step 2450]: train=6.1408, val=6.0463
Step 2500: loss = 6.3060
  [Val @ step 2500]: train=6.3060, val=6.0299
Step 2550: loss = 5.8337
  [Val @ step 2550]: train=5.8337, val=5.9855
Step 2600: loss = 5.7820
  [Val @ step 2600]: train=5.7820, val=6.0607
Step 2650: loss = 6.1977
  [Val @ step 2650]: train=6.1977, val=6.0465
Step 2700: loss = 6.1198
  [Val @ step 2700]: train=6.1198, val=6.0985
Step 2750: loss = 6.2282
  [Val @ step 2750]: train=6.2282, val=6.0454
Step 2800: loss = 5.8032
  [Val @ step 2800]: train=5.8032, val=6.0219
Step 2850: loss = 5.9588
  [Val @ step 2850]: train=5.9588, val=5.9879
Step 2900: loss = 6.0111
  [Val @ step 2900]: train=6.0111, val=6.0086
Step 2950: loss = 5.7979
  [Val @ step 2950]: train=5.7979, val=6.0407
Step 3000: loss = 5.8137
  [Val @ step 3000]: train=5.8137, val=6.0223
Step 3050: loss = 5.9217
  [Val @ step 3050]: train=5.9217, val=5.9523
Step 3100: loss = 5.9038
  [Val @ step 3100]: train=5.9038, val=6.0151
Step 3150: loss = 6.0863
  [Val @ step 3150]: train=6.0863, val=5.9943
Step 3200: loss = 6.0664
  [Val @ step 3200]: train=6.0664, val=5.9425
Step 3250: loss = 6.0206
  [Val @ step 3250]: train=6.0206, val=6.0240
Step 3300: loss = 5.9714
  [Val @ step 3300]: train=5.9714, val=6.0280
Step 3350: loss = 6.0595
  [Val @ step 3350]: train=6.0595, val=5.9701
Step 3400: loss = 5.8971
  [Val @ step 3400]: train=5.8971, val=6.0399
Step 3450: loss = 6.0514
  [Val @ step 3450]: train=6.0514, val=5.9561
Step 3500: loss = 5.6626
  [Val @ step 3500]: train=5.6626, val=5.9866
Step 3550: loss = 5.9129
  [Val @ step 3550]: train=5.9129, val=5.9940
Step 3600: loss = 5.8398
  [Val @ step 3600]: train=5.8398, val=6.0107
Step 3650: loss = 5.7535
  [Val @ step 3650]: train=5.7535, val=5.9147
Step 3700: loss = 6.2086
  [Val @ step 3700]: train=6.2086, val=5.9867
Step 3750: loss = 5.8194
  [Val @ step 3750]: train=5.8194, val=5.9392
Step 3800: loss = 5.8580
  [Val @ step 3800]: train=5.8580, val=5.9260
Step 3850: loss = 5.9736
  [Val @ step 3850]: train=5.9736, val=5.9166
Step 3900: loss = 5.8061
  [Val @ step 3900]: train=5.8061, val=5.9302
Step 3950: loss = 6.0335
  [Val @ step 3950]: train=6.0335, val=5.9158
Step 4000: loss = 5.8194
  [Val @ step 4000]: train=5.8194, val=5.9529
Step 4050: loss = 6.0929
  [Val @ step 4050]: train=6.0929, val=5.9610
Step 4100: loss = 5.8141
  [Val @ step 4100]: train=5.8141, val=5.8730
Step 4150: loss = 5.8631
  [Val @ step 4150]: train=5.8631, val=5.9139
Step 4200: loss = 5.9600
  [Val @ step 4200]: train=5.9600, val=5.9722
Step 4250: loss = 5.8175
  [Val @ step 4250]: train=5.8175, val=5.9044
Step 4300: loss = 6.0898
  [Val @ step 4300]: train=6.0898, val=5.9006
Step 4350: loss = 5.8463
  [Val @ step 4350]: train=5.8463, val=5.9358
Step 4400: loss = 5.8059
  [Val @ step 4400]: train=5.8059, val=5.9301
Step 4450: loss = 5.8227
  [Val @ step 4450]: train=5.8227, val=5.8665
Step 4500: loss = 5.7686
  [Val @ step 4500]: train=5.7686, val=5.8898
Step 4550: loss = 5.6846
  [Val @ step 4550]: train=5.6846, val=5.8943
Step 4600: loss = 5.9194
  [Val @ step 4600]: train=5.9194, val=5.8561
Step 4650: loss = 5.5811
  [Val @ step 4650]: train=5.5811, val=5.8989
Step 4700: loss = 6.0107
  [Val @ step 4700]: train=6.0107, val=5.8677
Step 4750: loss = 5.7685
  [Val @ step 4750]: train=5.7685, val=5.8194
Step 4800: loss = 5.8117
  [Val @ step 4800]: train=5.8117, val=5.9187
Step 4850: loss = 6.1962
  [Val @ step 4850]: train=6.1962, val=5.8043
Step 4900: loss = 6.1825
  [Val @ step 4900]: train=6.1825, val=5.8981
Step 4950: loss = 6.0420
  [Val @ step 4950]: train=6.0420, val=5.9340
--------------------------------------------------------------------------------
Computing final validation loss...
Validation loss: 5.8344

Results saved to time_to_target_stage1_20251225_225546.jsonl
Training time: 2460.8s
Throughput: 4,161 tokens/sec
Projections: 715/5000
nGPT Architectural Experiment: D_lr0.01
================================================================================
Model: 11 layers x 768 dim (matching modded-nanogpt)
Hypothesis: Baseline
================================================================================

Loading FineWeb data (subset for fast experiments)...
  Loading data/fineweb10B/fineweb_train_000001.bin...
  Loading data/fineweb10B/fineweb_train_000002.bin...
  Loading data/fineweb10B/fineweb_train_000003.bin...
  Loading data/fineweb10B/fineweb_train_000004.bin...
  Vocab size: 50257
  Train tokens: 400,001,536
  Val tokens: 100,000,384

Model config (modded-nanogpt size):
  vocab_size: 50257
  n_layer: 11
  n_embd: 768
  n_head: 6
  block_size: 128
Using GatedBlock (H3) with gate_init=0.0

Total parameters: 155,166,743

Optimizer: SimpleMuon (lr=0.01, momentum=0.95, geodesic=baseline, orthog=newton_schulz)
  2D weight params (Muon): 77,856,768
  1D params (Adam): 77,309,975

Starting training (5000 steps)...
--------------------------------------------------------------------------------
Step    0: loss = 10.9942
Step   50: loss = 6.9700
  [Val @ step 50]: train=6.9700, val=6.9978
Step  100: loss = 6.7658
  [Val @ step 100]: train=6.7658, val=6.8000
Step  150: loss = 6.7956
  [Val @ step 150]: train=6.7956, val=6.7251
Step  200: loss = 6.6384
  [Val @ step 200]: train=6.6384, val=6.5857
Step  250: loss = 6.6120
  [Val @ step 250]: train=6.6120, val=6.5147
Step  300: loss = 6.2512
  [Val @ step 300]: train=6.2512, val=6.4564
Step  350: loss = 6.2414
  [Val @ step 350]: train=6.2414, val=6.5099
Step  400: loss = 6.0184
  [Val @ step 400]: train=6.0184, val=6.4546
Step  450: loss = 6.2706
  [Val @ step 450]: train=6.2706, val=6.4049
Step  500: loss = 6.4797
  [Val @ step 500]: train=6.4797, val=6.3730
Step  550: loss = 6.3712
  [Val @ step 550]: train=6.3712, val=6.3977
Step  600: loss = 6.4044
  [Val @ step 600]: train=6.4044, val=6.3433
Step  650: loss = 6.1662
  [Val @ step 650]: train=6.1662, val=6.3628
Step  700: loss = 6.2999
  [Val @ step 700]: train=6.2999, val=6.2894
Step  750: loss = 6.5052
  [Val @ step 750]: train=6.5052, val=6.3277
Step  800: loss = 6.1236
  [Val @ step 800]: train=6.1236, val=6.2596
Step  850: loss = 6.1747
  [Val @ step 850]: train=6.1747, val=6.2720
Step  900: loss = 6.3387
  [Val @ step 900]: train=6.3387, val=6.2589
Step  950: loss = 6.3574
  [Val @ step 950]: train=6.3574, val=6.2474
Step 1000: loss = 6.1427
  [Val @ step 1000]: train=6.1427, val=6.2671
Step 1050: loss = 6.3086
  [Val @ step 1050]: train=6.3086, val=6.2478
Step 1100: loss = 6.1947
  [Val @ step 1100]: train=6.1947, val=6.2610
Step 1150: loss = 6.2028
  [Val @ step 1150]: train=6.2028, val=6.2556
Step 1200: loss = 6.3500
  [Val @ step 1200]: train=6.3500, val=6.2552
Step 1250: loss = 6.2256
  [Val @ step 1250]: train=6.2256, val=6.2436
Step 1300: loss = 6.2489
  [Val @ step 1300]: train=6.2489, val=6.2127
Step 1350: loss = 6.1357
  [Val @ step 1350]: train=6.1357, val=6.2316
Step 1400: loss = 6.0768
  [Val @ step 1400]: train=6.0768, val=6.2205
Step 1450: loss = 6.2644
  [Val @ step 1450]: train=6.2644, val=6.1808
Step 1500: loss = 6.0556
  [Val @ step 1500]: train=6.0556, val=6.1662
Step 1550: loss = 6.2658
  [Val @ step 1550]: train=6.2658, val=6.1775
Step 1600: loss = 6.0131
  [Val @ step 1600]: train=6.0131, val=6.2093
Step 1650: loss = 6.2168
  [Val @ step 1650]: train=6.2168, val=6.2101
Step 1700: loss = 6.3357
  [Val @ step 1700]: train=6.3357, val=6.1711
Step 1750: loss = 6.4136
  [Val @ step 1750]: train=6.4136, val=6.2044
Step 1800: loss = 6.2157
  [Val @ step 1800]: train=6.2157, val=6.1713
Step 1850: loss = 6.2810
  [Val @ step 1850]: train=6.2810, val=6.1874
Step 1900: loss = 6.0730
  [Val @ step 1900]: train=6.0730, val=6.1668
Step 1950: loss = 6.2316
  [Val @ step 1950]: train=6.2316, val=6.1919
Step 2000: loss = 6.1219
  [Val @ step 2000]: train=6.1219, val=6.1663
Step 2050: loss = 6.1412
  [Val @ step 2050]: train=6.1412, val=6.1548
Step 2100: loss = 6.1696
  [Val @ step 2100]: train=6.1696, val=6.1852
Step 2150: loss = 6.2893
  [Val @ step 2150]: train=6.2893, val=6.1795
Step 2200: loss = 6.1342
  [Val @ step 2200]: train=6.1342, val=6.1925
Step 2250: loss = 6.0174
  [Val @ step 2250]: train=6.0174, val=6.1703
Step 2300: loss = 6.1203
  [Val @ step 2300]: train=6.1203, val=6.1751
Step 2350: loss = 6.3269
  [Val @ step 2350]: train=6.3269, val=6.1641
Step 2400: loss = 6.1405
  [Val @ step 2400]: train=6.1405, val=6.1896
Step 2450: loss = 6.0251
  [Val @ step 2450]: train=6.0251, val=6.1472
Step 2500: loss = 6.0647
  [Val @ step 2500]: train=6.0647, val=6.1260
Step 2550: loss = 6.1113
  [Val @ step 2550]: train=6.1113, val=6.1355
Step 2600: loss = 6.1924
  [Val @ step 2600]: train=6.1924, val=6.1413
Step 2650: loss = 6.2278
  [Val @ step 2650]: train=6.2278, val=6.1535
Step 2700: loss = 6.0180
  [Val @ step 2700]: train=6.0180, val=6.1217
Step 2750: loss = 6.0518
  [Val @ step 2750]: train=6.0518, val=6.1616
Step 2800: loss = 6.2517
  [Val @ step 2800]: train=6.2517, val=6.1628
Step 2850: loss = 6.3036
  [Val @ step 2850]: train=6.3036, val=6.1552
Step 2900: loss = 6.1135
  [Val @ step 2900]: train=6.1135, val=6.1205
Step 2950: loss = 6.0546
  [Val @ step 2950]: train=6.0546, val=6.1861
Step 3000: loss = 6.1950
  [Val @ step 3000]: train=6.1950, val=6.1325
Step 3050: loss = 6.0810
  [Val @ step 3050]: train=6.0810, val=6.1447
Step 3100: loss = 5.9224
  [Val @ step 3100]: train=5.9224, val=6.1328
Step 3150: loss = 6.1860
  [Val @ step 3150]: train=6.1860, val=6.1644
Step 3200: loss = 6.3396
  [Val @ step 3200]: train=6.3396, val=6.1669
Step 3250: loss = 6.0615
  [Val @ step 3250]: train=6.0615, val=6.1331
Step 3300: loss = 6.5011
  [Val @ step 3300]: train=6.5011, val=6.1791
Step 3350: loss = 6.2394
  [Val @ step 3350]: train=6.2394, val=6.1641
Step 3400: loss = 6.1445
  [Val @ step 3400]: train=6.1445, val=6.1139
Step 3450: loss = 6.1047
  [Val @ step 3450]: train=6.1047, val=6.1338
Step 3500: loss = 6.0508
  [Val @ step 3500]: train=6.0508, val=6.1318
Step 3550: loss = 6.1935
  [Val @ step 3550]: train=6.1935, val=6.1551
Step 3600: loss = 5.9292
  [Val @ step 3600]: train=5.9292, val=6.1500
Step 3650: loss = 5.9992
  [Val @ step 3650]: train=5.9992, val=6.1568
Step 3700: loss = 5.9311
  [Val @ step 3700]: train=5.9311, val=6.1431
Step 3750: loss = 6.1134
  [Val @ step 3750]: train=6.1134, val=6.1358
Step 3800: loss = 6.1027
  [Val @ step 3800]: train=6.1027, val=6.1483
Step 3850: loss = 6.1547
  [Val @ step 3850]: train=6.1547, val=6.1164
Step 3900: loss = 5.9784
  [Val @ step 3900]: train=5.9784, val=6.1271
Step 3950: loss = 6.1556
  [Val @ step 3950]: train=6.1556, val=6.1456
Step 4000: loss = 6.0620
  [Val @ step 4000]: train=6.0620, val=6.1322
Step 4050: loss = 6.1488
  [Val @ step 4050]: train=6.1488, val=6.1309
Step 4100: loss = 6.1439
  [Val @ step 4100]: train=6.1439, val=6.1334
Step 4150: loss = 5.9863
  [Val @ step 4150]: train=5.9863, val=6.1002
Step 4200: loss = 5.9948
  [Val @ step 4200]: train=5.9948, val=6.1246
Step 4250: loss = 6.1492
  [Val @ step 4250]: train=6.1492, val=6.1342
Step 4300: loss = 6.1409
  [Val @ step 4300]: train=6.1409, val=6.1148
Step 4350: loss = 6.2207
  [Val @ step 4350]: train=6.2207, val=6.1229
Step 4400: loss = 6.0883
  [Val @ step 4400]: train=6.0883, val=6.1579
Step 4450: loss = 6.0174
  [Val @ step 4450]: train=6.0174, val=6.0751
Step 4500: loss = 6.0268
  [Val @ step 4500]: train=6.0268, val=6.0972
Step 4550: loss = 6.1409
  [Val @ step 4550]: train=6.1409, val=6.1190
Step 4600: loss = 6.0587
  [Val @ step 4600]: train=6.0587, val=6.0914
Step 4650: loss = 6.0179
  [Val @ step 4650]: train=6.0179, val=6.1159
Step 4700: loss = 5.9983
  [Val @ step 4700]: train=5.9983, val=6.0949
Step 4750: loss = 5.9667
  [Val @ step 4750]: train=5.9667, val=6.1082
Step 4800: loss = 5.9362
  [Val @ step 4800]: train=5.9362, val=6.1064
Step 4850: loss = 6.2603
  [Val @ step 4850]: train=6.2603, val=6.1066
Step 4900: loss = 6.0943
  [Val @ step 4900]: train=6.0943, val=6.1410
Step 4950: loss = 6.0347
  [Val @ step 4950]: train=6.0347, val=6.1041
--------------------------------------------------------------------------------
Computing final validation loss...
Validation loss: 6.1013

Results saved to time_to_target_stage1_20251225_225546.jsonl
Training time: 3955.8s
Throughput: 5,177 tokens/sec
Projections: 715/5000
nGPT Architectural Experiment: D_lr0.02
================================================================================
Model: 11 layers x 768 dim (matching modded-nanogpt)
Hypothesis: Baseline
================================================================================

Loading FineWeb data (subset for fast experiments)...
  Loading data/fineweb10B/fineweb_train_000001.bin...
  Loading data/fineweb10B/fineweb_train_000002.bin...
  Loading data/fineweb10B/fineweb_train_000003.bin...
  Loading data/fineweb10B/fineweb_train_000004.bin...
  Vocab size: 50257
  Train tokens: 400,001,536
  Val tokens: 100,000,384

Model config (modded-nanogpt size):
  vocab_size: 50257
  n_layer: 11
  n_embd: 768
  n_head: 6
  block_size: 128
Using GatedBlock (H3) with gate_init=0.0

Total parameters: 155,166,743

Optimizer: SimpleMuon (lr=0.02, momentum=0.95, geodesic=baseline, orthog=newton_schulz)
  2D weight params (Muon): 77,856,768
  1D params (Adam): 77,309,975

Starting training (5000 steps)...
--------------------------------------------------------------------------------
Step    0: loss = 10.9909
Step   50: loss = 6.9746
  [Val @ step 50]: train=6.9746, val=7.0578
Step  100: loss = 6.9845
  [Val @ step 100]: train=6.9845, val=6.9285
Step  150: loss = 6.6686
  [Val @ step 150]: train=6.6686, val=6.8078
Step  200: loss = 6.9148
  [Val @ step 200]: train=6.9148, val=6.7729
Step  250: loss = 6.7476
  [Val @ step 250]: train=6.7476, val=6.7394
Step  300: loss = 6.6650
  [Val @ step 300]: train=6.6650, val=6.6876
Step  350: loss = 6.6334
  [Val @ step 350]: train=6.6334, val=6.6729
Step  400: loss = 6.6379
  [Val @ step 400]: train=6.6379, val=6.6919
Step  450: loss = 6.4735
  [Val @ step 450]: train=6.4735, val=6.5951
Step  500: loss = 6.6280
  [Val @ step 500]: train=6.6280, val=6.6066
Step  550: loss = 6.5107
  [Val @ step 550]: train=6.5107, val=6.5874
Step  600: loss = 6.4291
  [Val @ step 600]: train=6.4291, val=6.5526
Step  650: loss = 6.6341
  [Val @ step 650]: train=6.6341, val=6.5369
Step  700: loss = 6.4399
  [Val @ step 700]: train=6.4399, val=6.5413
Step  750: loss = 6.5680
  [Val @ step 750]: train=6.5680, val=6.5090
Step  800: loss = 6.5864
  [Val @ step 800]: train=6.5864, val=6.5386
Step  850: loss = 6.4938
  [Val @ step 850]: train=6.4938, val=6.5126
Step  900: loss = 6.4931
  [Val @ step 900]: train=6.4931, val=6.5084
Step  950: loss = 6.4352
  [Val @ step 950]: train=6.4352, val=6.5012
Step 1000: loss = 6.6039
  [Val @ step 1000]: train=6.6039, val=6.4657
Step 1050: loss = 6.4543
  [Val @ step 1050]: train=6.4543, val=6.4755
Step 1100: loss = 6.4310
  [Val @ step 1100]: train=6.4310, val=6.4890
Step 1150: loss = 6.5313
  [Val @ step 1150]: train=6.5313, val=6.4697
Step 1200: loss = 6.5535
  [Val @ step 1200]: train=6.5535, val=6.4843
Step 1250: loss = 6.5001
  [Val @ step 1250]: train=6.5001, val=6.4668
Step 1300: loss = 6.4597
  [Val @ step 1300]: train=6.4597, val=6.4965
Step 1350: loss = 6.6656
  [Val @ step 1350]: train=6.6656, val=6.4890
Step 1400: loss = 6.5685
  [Val @ step 1400]: train=6.5685, val=6.4754
Step 1450: loss = 6.2798
  [Val @ step 1450]: train=6.2798, val=6.4878
Step 1500: loss = 6.3901
  [Val @ step 1500]: train=6.3901, val=6.4792
Step 1550: loss = 6.4640
  [Val @ step 1550]: train=6.4640, val=6.4719
Step 1600: loss = 6.4768
  [Val @ step 1600]: train=6.4768, val=6.4480
Step 1650: loss = 6.4604
  [Val @ step 1650]: train=6.4604, val=6.5058
Step 1700: loss = 6.4796
  [Val @ step 1700]: train=6.4796, val=6.4410
Step 1750: loss = 6.4220
  [Val @ step 1750]: train=6.4220, val=6.4593
Step 1800: loss = 6.5126
  [Val @ step 1800]: train=6.5126, val=6.4375
Step 1850: loss = 6.4954
  [Val @ step 1850]: train=6.4954, val=6.4159
Step 1900: loss = 6.4226
  [Val @ step 1900]: train=6.4226, val=6.4585
Step 1950: loss = 6.3594
  [Val @ step 1950]: train=6.3594, val=6.4494
Step 2000: loss = 6.4427
  [Val @ step 2000]: train=6.4427, val=6.4832
Step 2050: loss = 6.6444
  [Val @ step 2050]: train=6.6444, val=6.4793
Step 2100: loss = 6.4540
  [Val @ step 2100]: train=6.4540, val=6.4532
Step 2150: loss = 6.4509
  [Val @ step 2150]: train=6.4509, val=6.4131
Step 2200: loss = 6.3352
  [Val @ step 2200]: train=6.3352, val=6.4468
Step 2250: loss = 6.4687
  [Val @ step 2250]: train=6.4687, val=6.4471
Step 2300: loss = 6.5149
  [Val @ step 2300]: train=6.5149, val=6.4827
Step 2350: loss = 6.4868
  [Val @ step 2350]: train=6.4868, val=6.4527
Step 2400: loss = 6.5993
  [Val @ step 2400]: train=6.5993, val=6.4522
Step 2450: loss = 6.4722
  [Val @ step 2450]: train=6.4722, val=6.4287
Step 2500: loss = 6.4066
  [Val @ step 2500]: train=6.4066, val=6.4077
Step 2550: loss = 6.3878
  [Val @ step 2550]: train=6.3878, val=6.4746
Step 2600: loss = 6.3744
  [Val @ step 2600]: train=6.3744, val=6.4543
Step 2650: loss = 6.3340
  [Val @ step 2650]: train=6.3340, val=6.4580
Step 2700: loss = 6.4423
  [Val @ step 2700]: train=6.4423, val=6.4381
Step 2750: loss = 6.4030
  [Val @ step 2750]: train=6.4030, val=6.4180
Step 2800: loss = 6.6226
  [Val @ step 2800]: train=6.6226, val=6.4670
Step 2850: loss = 6.4735
  [Val @ step 2850]: train=6.4735, val=6.3702
Step 2900: loss = 6.3270
  [Val @ step 2900]: train=6.3270, val=6.4322
Step 2950: loss = 6.3490
  [Val @ step 2950]: train=6.3490, val=6.4449
Step 3000: loss = 6.5473
  [Val @ step 3000]: train=6.5473, val=6.4291
Step 3050: loss = 6.4082
  [Val @ step 3050]: train=6.4082, val=6.4418
Step 3100: loss = 6.4072
  [Val @ step 3100]: train=6.4072, val=6.4931
Step 3150: loss = 6.5308
  [Val @ step 3150]: train=6.5308, val=6.4164
Step 3200: loss = 6.1966
  [Val @ step 3200]: train=6.1966, val=6.4012
Step 3250: loss = 6.3175
  [Val @ step 3250]: train=6.3175, val=6.4383
Step 3300: loss = 6.5259
  [Val @ step 3300]: train=6.5259, val=6.4004
Step 3350: loss = 6.3736
  [Val @ step 3350]: train=6.3736, val=6.3957
Step 3400: loss = 6.4122
  [Val @ step 3400]: train=6.4122, val=6.4641
Step 3450: loss = 6.4106
  [Val @ step 3450]: train=6.4106, val=6.4496
Step 3500: loss = 6.5900
  [Val @ step 3500]: train=6.5900, val=6.4283
Step 3550: loss = 6.4802
  [Val @ step 3550]: train=6.4802, val=6.4569
Step 3600: loss = 6.3332
  [Val @ step 3600]: train=6.3332, val=6.4064
Step 3650: loss = 6.3509
  [Val @ step 3650]: train=6.3509, val=6.4198
Step 3700: loss = 6.3851
  [Val @ step 3700]: train=6.3851, val=6.4353
Step 3750: loss = 6.4440
  [Val @ step 3750]: train=6.4440, val=6.4390
Step 3800: loss = 6.4545
  [Val @ step 3800]: train=6.4545, val=6.4215
Step 3850: loss = 6.4665
  [Val @ step 3850]: train=6.4665, val=6.4643
Step 3900: loss = 6.2307
  [Val @ step 3900]: train=6.2307, val=6.4105
Step 3950: loss = 6.4792
  [Val @ step 3950]: train=6.4792, val=6.3770
Step 4000: loss = 6.3872
  [Val @ step 4000]: train=6.3872, val=6.4356
Step 4050: loss = 6.2400
  [Val @ step 4050]: train=6.2400, val=6.4028
Step 4100: loss = 6.3912
  [Val @ step 4100]: train=6.3912, val=6.5105
Step 4150: loss = 6.4425
  [Val @ step 4150]: train=6.4425, val=6.4534
Step 4200: loss = 6.2984
  [Val @ step 4200]: train=6.2984, val=6.4199
Step 4250: loss = 6.4777
  [Val @ step 4250]: train=6.4777, val=6.3785
Step 4300: loss = 6.3611
  [Val @ step 4300]: train=6.3611, val=6.4287
Step 4350: loss = 6.5258
  [Val @ step 4350]: train=6.5258, val=6.4643
Step 4400: loss = 6.5284
  [Val @ step 4400]: train=6.5284, val=6.4224
Step 4450: loss = 6.5272
  [Val @ step 4450]: train=6.5272, val=6.4287
Step 4500: loss = 6.3822
  [Val @ step 4500]: train=6.3822, val=6.3957
Step 4550: loss = 6.5151
  [Val @ step 4550]: train=6.5151, val=6.4382
Step 4600: loss = 6.3805
  [Val @ step 4600]: train=6.3805, val=6.4084
Step 4650: loss = 6.2879
  [Val @ step 4650]: train=6.2879, val=6.4154
Step 4700: loss = 6.4763
  [Val @ step 4700]: train=6.4763, val=6.4235
Step 4750: loss = 6.4148
  [Val @ step 4750]: train=6.4148, val=6.4262
Step 4800: loss = 6.5305
  [Val @ step 4800]: train=6.5305, val=6.4035
Step 4850: loss = 6.3546
  [Val @ step 4850]: train=6.3546, val=6.4313
Step 4900: loss = 6.5318
  [Val @ step 4900]: train=6.5318, val=6.4531
Step 4950: loss = 6.4847
  [Val @ step 4950]: train=6.4847, val=6.3822
--------------------------------------------------------------------------------
Computing final validation loss...
Validation loss: 6.4059

Results saved to time_to_target_stage1_20251225_225546.jsonl
Training time: 3893.5s
Throughput: 5,260 tokens/sec
Projections: 715/5000
nGPT Architectural Experiment: F_mom0.98
================================================================================
Model: 11 layers x 768 dim (matching modded-nanogpt)
Hypothesis: Baseline
================================================================================

Loading FineWeb data (subset for fast experiments)...
  Loading data/fineweb10B/fineweb_train_000001.bin...
  Loading data/fineweb10B/fineweb_train_000002.bin...
  Loading data/fineweb10B/fineweb_train_000003.bin...
  Loading data/fineweb10B/fineweb_train_000004.bin...
  Vocab size: 50257
  Train tokens: 400,001,536
  Val tokens: 100,000,384

Model config (modded-nanogpt size):
  vocab_size: 50257
  n_layer: 11
  n_embd: 768
  n_head: 6
  block_size: 128
Using GatedBlock (H3) with gate_init=0.0

Total parameters: 155,166,743

Optimizer: SimpleMuon (lr=0.003, momentum=0.98, geodesic=baseline, orthog=newton_schulz)
  2D weight params (Muon): 77,856,768
  1D params (Adam): 77,309,975

Starting training (5000 steps)...
--------------------------------------------------------------------------------
Step    0: loss = 10.9912
Traceback (most recent call last):
  File "/home/grace/Projects/nGPT-muon/modded-nanogpt/train_architectural.py", line 822, in <module>
    result = main()
             ^^^^^^
  File "/home/grace/Projects/nGPT-muon/modded-nanogpt/train_architectural.py", line 716, in main
    logits, loss = model(x, y)
                   ^^^^^^^^^^^
  File "/home/grace/Projects/nGPT-muon/modded-nanogpt/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1780, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/grace/Projects/nGPT-muon/modded-nanogpt/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1791, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/grace/Projects/nGPT-muon/modded-nanogpt/train_architectural.py", line 570, in forward
    logits = F.linear(x_norm, w_norm) * self.logit_scale
             ~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 786.00 MiB. GPU 0 has a total capacity of 95.00 GiB of which 325.25 MiB is free. Process 82129 has 10.08 GiB memory in use. Process 82969 has 17.06 GiB memory in use. Process 83815 has 29.49 GiB memory in use. Process 84640 has 10.08 GiB memory in use. Process 85486 has 10.08 GiB memory in use. Process 86327 has 10.08 GiB memory in use. Including non-PyTorch memory, this process has 7.77 GiB memory in use. Of the allocated memory 6.16 GiB is allocated by PyTorch, and 876.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
nGPT Architectural Experiment: G01_fast
================================================================================
Model: 11 layers x 768 dim (matching modded-nanogpt)
Hypothesis: Baseline
================================================================================

Loading FineWeb data (subset for fast experiments)...
  Loading data/fineweb10B/fineweb_train_000001.bin...
  Loading data/fineweb10B/fineweb_train_000002.bin...
  Loading data/fineweb10B/fineweb_train_000003.bin...
  Loading data/fineweb10B/fineweb_train_000004.bin...
  Vocab size: 50257
  Train tokens: 400,001,536
  Val tokens: 100,000,384

Model config (modded-nanogpt size):
  vocab_size: 50257
  n_layer: 11
  n_embd: 768
  n_head: 6
  block_size: 128
Using GatedBlock (H3) with gate_init=0.0

Total parameters: 155,166,743

Optimizer: SimpleMuon (lr=0.01, momentum=0.95, geodesic=baseline, orthog=newton_schulz)
  2D weight params (Muon): 77,856,768
  1D params (Adam): 77,309,975

Starting training (5000 steps)...
--------------------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/grace/Projects/nGPT-muon/modded-nanogpt/train_architectural.py", line 822, in <module>
    result = main()
             ^^^^^^
  File "/home/grace/Projects/nGPT-muon/modded-nanogpt/train_architectural.py", line 716, in main
    logits, loss = model(x, y)
                   ^^^^^^^^^^^
  File "/home/grace/Projects/nGPT-muon/modded-nanogpt/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1780, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/grace/Projects/nGPT-muon/modded-nanogpt/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1791, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/grace/Projects/nGPT-muon/modded-nanogpt/train_architectural.py", line 570, in forward
    logits = F.linear(x_norm, w_norm) * self.logit_scale
             ~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.54 GiB. GPU 0 has a total capacity of 95.00 GiB of which 308.19 MiB is free. Process 82129 has 10.08 GiB memory in use. Process 82969 has 17.06 GiB memory in use. Process 83815 has 29.49 GiB memory in use. Process 84640 has 10.08 GiB memory in use. Process 85486 has 10.08 GiB memory in use. Process 86327 has 10.08 GiB memory in use. Including non-PyTorch memory, this process has 7.79 GiB memory in use. Of the allocated memory 7.05 GiB is allocated by PyTorch, and 45.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
nGPT Architectural Experiment: G02_fast_safe
================================================================================
Model: 11 layers x 768 dim (matching modded-nanogpt)
Hypothesis: Baseline
================================================================================

Loading FineWeb data (subset for fast experiments)...
  Loading data/fineweb10B/fineweb_train_000001.bin...
  Loading data/fineweb10B/fineweb_train_000002.bin...
  Loading data/fineweb10B/fineweb_train_000003.bin...
  Loading data/fineweb10B/fineweb_train_000004.bin...
  Vocab size: 50257
  Train tokens: 400,001,536
  Val tokens: 100,000,384

Model config (modded-nanogpt size):
  vocab_size: 50257
  n_layer: 11
  n_embd: 768
  n_head: 6
  block_size: 128
Using GatedBlock (H3) with gate_init=0.0

Total parameters: 155,166,743

Optimizer: SimpleMuon (lr=0.01, momentum=0.9, geodesic=baseline, orthog=newton_schulz)
  2D weight params (Muon): 77,856,768
  1D params (Adam): 77,309,975

Starting training (5000 steps)...
--------------------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/grace/Projects/nGPT-muon/modded-nanogpt/train_architectural.py", line 822, in <module>
    result = main()
             ^^^^^^
  File "/home/grace/Projects/nGPT-muon/modded-nanogpt/train_architectural.py", line 716, in main
    logits, loss = model(x, y)
                   ^^^^^^^^^^^
  File "/home/grace/Projects/nGPT-muon/modded-nanogpt/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1780, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/grace/Projects/nGPT-muon/modded-nanogpt/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1791, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/grace/Projects/nGPT-muon/modded-nanogpt/train_architectural.py", line 570, in forward
    logits = F.linear(x_norm, w_norm) * self.logit_scale
             ~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.54 GiB. GPU 0 has a total capacity of 95.00 GiB of which 306.38 MiB is free. Process 82129 has 10.08 GiB memory in use. Process 82969 has 17.06 GiB memory in use. Process 83815 has 29.49 GiB memory in use. Process 84640 has 10.08 GiB memory in use. Process 85486 has 10.08 GiB memory in use. Process 86327 has 10.08 GiB memory in use. Including non-PyTorch memory, this process has 7.79 GiB memory in use. Of the allocated memory 7.05 GiB is allocated by PyTorch, and 45.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
nGPT Architectural Experiment: G03_fast_quality
================================================================================
Model: 11 layers x 768 dim (matching modded-nanogpt)
Hypothesis: Baseline
================================================================================

Loading FineWeb data (subset for fast experiments)...
  Loading data/fineweb10B/fineweb_train_000001.bin...
  Loading data/fineweb10B/fineweb_train_000002.bin...
  Loading data/fineweb10B/fineweb_train_000003.bin...
  Loading data/fineweb10B/fineweb_train_000004.bin...
  Vocab size: 50257
  Train tokens: 400,001,536
  Val tokens: 100,000,384

Model config (modded-nanogpt size):
  vocab_size: 50257
  n_layer: 11
  n_embd: 768
  n_head: 6
  block_size: 128
Using GatedBlock (H3) with gate_init=0.0

Total parameters: 155,166,743

Optimizer: SimpleMuon (lr=0.01, momentum=0.95, geodesic=baseline, orthog=newton_schulz)
  2D weight params (Muon): 77,856,768
  1D params (Adam): 77,309,975

Starting training (5000 steps)...
--------------------------------------------------------------------------------
Step    0: loss = 10.9757
Traceback (most recent call last):
  File "/home/grace/Projects/nGPT-muon/modded-nanogpt/train_architectural.py", line 822, in <module>
    result = main()
             ^^^^^^
  File "/home/grace/Projects/nGPT-muon/modded-nanogpt/train_architectural.py", line 716, in main
    logits, loss = model(x, y)
                   ^^^^^^^^^^^
  File "/home/grace/Projects/nGPT-muon/modded-nanogpt/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1780, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/grace/Projects/nGPT-muon/modded-nanogpt/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1791, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/grace/Projects/nGPT-muon/modded-nanogpt/train_architectural.py", line 570, in forward
    logits = F.linear(x_norm, w_norm) * self.logit_scale
             ~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 786.00 MiB. GPU 0 has a total capacity of 95.00 GiB of which 319.69 MiB is free. Process 82129 has 10.08 GiB memory in use. Process 82969 has 17.06 GiB memory in use. Process 83815 has 29.49 GiB memory in use. Process 84640 has 10.08 GiB memory in use. Process 85486 has 10.08 GiB memory in use. Process 86327 has 10.08 GiB memory in use. Including non-PyTorch memory, this process has 7.77 GiB memory in use. Of the allocated memory 6.16 GiB is allocated by PyTorch, and 876.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
nGPT Architectural Experiment: G04_pe_quality
================================================================================
Model: 11 layers x 768 dim (matching modded-nanogpt)
Hypothesis: Baseline
================================================================================

Loading FineWeb data (subset for fast experiments)...
  Loading data/fineweb10B/fineweb_train_000001.bin...
  Loading data/fineweb10B/fineweb_train_000002.bin...
  Loading data/fineweb10B/fineweb_train_000003.bin...
  Loading data/fineweb10B/fineweb_train_000004.bin...
  Vocab size: 50257
  Train tokens: 400,001,536
  Val tokens: 100,000,384

Model config (modded-nanogpt size):
  vocab_size: 50257
  n_layer: 11
  n_embd: 768
  n_head: 6
  block_size: 128
Using GatedBlock (H3) with gate_init=0.0

Total parameters: 155,166,743

Optimizer: SimpleMuon (lr=0.003, momentum=0.95, geodesic=baseline, orthog=polar_express)
  2D weight params (Muon): 77,856,768
  1D params (Adam): 77,309,975

Starting training (5000 steps)...
--------------------------------------------------------------------------------
Step    0: loss = 10.9588
Traceback (most recent call last):
  File "/home/grace/Projects/nGPT-muon/modded-nanogpt/train_architectural.py", line 822, in <module>
    result = main()
             ^^^^^^
  File "/home/grace/Projects/nGPT-muon/modded-nanogpt/train_architectural.py", line 716, in main
    logits, loss = model(x, y)
                   ^^^^^^^^^^^
  File "/home/grace/Projects/nGPT-muon/modded-nanogpt/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1780, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/grace/Projects/nGPT-muon/modded-nanogpt/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1791, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/grace/Projects/nGPT-muon/modded-nanogpt/train_architectural.py", line 570, in forward
    logits = F.linear(x_norm, w_norm) * self.logit_scale
             ~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 786.00 MiB. GPU 0 has a total capacity of 95.00 GiB of which 318.88 MiB is free. Process 82129 has 10.08 GiB memory in use. Process 82969 has 17.06 GiB memory in use. Process 83815 has 29.49 GiB memory in use. Process 84640 has 10.08 GiB memory in use. Process 85486 has 10.08 GiB memory in use. Process 86327 has 10.08 GiB memory in use. Including non-PyTorch memory, this process has 7.77 GiB memory in use. Of the allocated memory 6.16 GiB is allocated by PyTorch, and 876.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
nGPT Architectural Experiment: G05_balanced
================================================================================
Model: 11 layers x 768 dim (matching modded-nanogpt)
Hypothesis: Baseline
================================================================================

Loading FineWeb data (subset for fast experiments)...
  Loading data/fineweb10B/fineweb_train_000001.bin...
  Loading data/fineweb10B/fineweb_train_000002.bin...
  Loading data/fineweb10B/fineweb_train_000003.bin...
  Loading data/fineweb10B/fineweb_train_000004.bin...
  Vocab size: 50257
  Train tokens: 400,001,536
  Val tokens: 100,000,384

Model config (modded-nanogpt size):
  vocab_size: 50257
  n_layer: 11
  n_embd: 768
  n_head: 6
  block_size: 128
Using GatedBlock (H3) with gate_init=0.0

Total parameters: 155,166,743

Optimizer: SimpleMuon (lr=0.008, momentum=0.94, geodesic=baseline, orthog=newton_schulz)
  2D weight params (Muon): 77,856,768
  1D params (Adam): 77,309,975

Starting training (5000 steps)...
--------------------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/grace/Projects/nGPT-muon/modded-nanogpt/train_architectural.py", line 822, in <module>
    result = main()
             ^^^^^^
  File "/home/grace/Projects/nGPT-muon/modded-nanogpt/train_architectural.py", line 719, in main
    loss.backward()
  File "/home/grace/Projects/nGPT-muon/modded-nanogpt/.venv/lib/python3.12/site-packages/torch/_tensor.py", line 631, in backward
    torch.autograd.backward(
  File "/home/grace/Projects/nGPT-muon/modded-nanogpt/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py", line 365, in backward
    _engine_run_backward(
  File "/home/grace/Projects/nGPT-muon/modded-nanogpt/.venv/lib/python3.12/site-packages/torch/autograd/graph.py", line 865, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.15 GiB. GPU 0 has a total capacity of 95.00 GiB of which 759.12 MiB is free. Process 82129 has 10.08 GiB memory in use. Process 82969 has 17.06 GiB memory in use. Process 83815 has 29.49 GiB memory in use. Process 84640 has 10.08 GiB memory in use. Process 85486 has 10.08 GiB memory in use. Process 86327 has 10.08 GiB memory in use. Including non-PyTorch memory, this process has 7.34 GiB memory in use. Of the allocated memory 6.61 GiB is allocated by PyTorch, and 43.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
nGPT Architectural Experiment: G06_aggressive
================================================================================
Model: 11 layers x 768 dim (matching modded-nanogpt)
Hypothesis: Baseline
================================================================================

Loading FineWeb data (subset for fast experiments)...
  Loading data/fineweb10B/fineweb_train_000001.bin...
  Loading data/fineweb10B/fineweb_train_000002.bin...
  Loading data/fineweb10B/fineweb_train_000003.bin...
  Loading data/fineweb10B/fineweb_train_000004.bin...
  Vocab size: 50257
  Train tokens: 400,001,536
  Val tokens: 100,000,384

Model config (modded-nanogpt size):
  vocab_size: 50257
  n_layer: 11
  n_embd: 768
  n_head: 6
  block_size: 128
Using GatedBlock (H3) with gate_init=0.0

Total parameters: 155,166,743

Optimizer: SimpleMuon (lr=0.012, momentum=0.92, geodesic=baseline, orthog=newton_schulz)
  2D weight params (Muon): 77,856,768
  1D params (Adam): 77,309,975

Starting training (5000 steps)...
--------------------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/grace/Projects/nGPT-muon/modded-nanogpt/train_architectural.py", line 822, in <module>
    result = main()
             ^^^^^^
  File "/home/grace/Projects/nGPT-muon/modded-nanogpt/train_architectural.py", line 716, in main
    logits, loss = model(x, y)
                   ^^^^^^^^^^^
  File "/home/grace/Projects/nGPT-muon/modded-nanogpt/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1780, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/grace/Projects/nGPT-muon/modded-nanogpt/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1791, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/grace/Projects/nGPT-muon/modded-nanogpt/train_architectural.py", line 570, in forward
    logits = F.linear(x_norm, w_norm) * self.logit_scale
             ~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.54 GiB. GPU 0 has a total capacity of 95.00 GiB of which 304.38 MiB is free. Process 82129 has 10.08 GiB memory in use. Process 82969 has 17.06 GiB memory in use. Process 83815 has 29.49 GiB memory in use. Process 84640 has 10.08 GiB memory in use. Process 85486 has 10.08 GiB memory in use. Process 86327 has 10.08 GiB memory in use. Including non-PyTorch memory, this process has 7.79 GiB memory in use. Of the allocated memory 7.05 GiB is allocated by PyTorch, and 45.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
nGPT Architectural Experiment: G07_vr_only
================================================================================
Model: 11 layers x 768 dim (matching modded-nanogpt)
Hypothesis: Baseline
================================================================================

Loading FineWeb data (subset for fast experiments)...
  Loading data/fineweb10B/fineweb_train_000001.bin...
  Loading data/fineweb10B/fineweb_train_000002.bin...
  Loading data/fineweb10B/fineweb_train_000003.bin...
  Loading data/fineweb10B/fineweb_train_000004.bin...
  Vocab size: 50257
  Train tokens: 400,001,536
  Val tokens: 100,000,384

Model config (modded-nanogpt size):
  vocab_size: 50257
  n_layer: 11
  n_embd: 768
  n_head: 6
  block_size: 128
Using GatedBlock (H3) with gate_init=0.0

Total parameters: 155,166,743

Optimizer: SimpleMuon (lr=0.005, momentum=0.96, geodesic=baseline, orthog=newton_schulz)
  2D weight params (Muon): 77,856,768
  1D params (Adam): 77,309,975

Starting training (5000 steps)...
--------------------------------------------------------------------------------
Step    0: loss = 10.9609
Traceback (most recent call last):
  File "/home/grace/Projects/nGPT-muon/modded-nanogpt/train_architectural.py", line 822, in <module>
    result = main()
             ^^^^^^
  File "/home/grace/Projects/nGPT-muon/modded-nanogpt/train_architectural.py", line 716, in main
    logits, loss = model(x, y)
                   ^^^^^^^^^^^
  File "/home/grace/Projects/nGPT-muon/modded-nanogpt/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1780, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/grace/Projects/nGPT-muon/modded-nanogpt/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1791, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/grace/Projects/nGPT-muon/modded-nanogpt/train_architectural.py", line 570, in forward
    logits = F.linear(x_norm, w_norm) * self.logit_scale
             ~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 786.00 MiB. GPU 0 has a total capacity of 95.00 GiB of which 317.50 MiB is free. Process 82129 has 10.08 GiB memory in use. Process 82969 has 17.06 GiB memory in use. Process 83815 has 29.49 GiB memory in use. Process 84640 has 10.08 GiB memory in use. Process 85486 has 10.08 GiB memory in use. Process 86327 has 10.08 GiB memory in use. Including non-PyTorch memory, this process has 7.77 GiB memory in use. Of the allocated memory 6.16 GiB is allocated by PyTorch, and 876.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
nGPT Architectural Experiment: G08_cwd_only
================================================================================
Model: 11 layers x 768 dim (matching modded-nanogpt)
Hypothesis: Baseline
================================================================================

Loading FineWeb data (subset for fast experiments)...
  Loading data/fineweb10B/fineweb_train_000001.bin...
  Loading data/fineweb10B/fineweb_train_000002.bin...
  Loading data/fineweb10B/fineweb_train_000003.bin...
  Loading data/fineweb10B/fineweb_train_000004.bin...
  Vocab size: 50257
  Train tokens: 400,001,536
  Val tokens: 100,000,384

Model config (modded-nanogpt size):
  vocab_size: 50257
  n_layer: 11
  n_embd: 768
  n_head: 6
  block_size: 128
Using GatedBlock (H3) with gate_init=0.0

Total parameters: 155,166,743

Optimizer: SimpleMuon (lr=0.005, momentum=0.96, geodesic=baseline, orthog=newton_schulz)
  2D weight params (Muon): 77,856,768
  1D params (Adam): 77,309,975

Starting training (5000 steps)...
--------------------------------------------------------------------------------
Step    0: loss = 10.9761
Traceback (most recent call last):
  File "/home/grace/Projects/nGPT-muon/modded-nanogpt/train_architectural.py", line 822, in <module>
    result = main()
             ^^^^^^
  File "/home/grace/Projects/nGPT-muon/modded-nanogpt/train_architectural.py", line 716, in main
    logits, loss = model(x, y)
                   ^^^^^^^^^^^
  File "/home/grace/Projects/nGPT-muon/modded-nanogpt/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1780, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/grace/Projects/nGPT-muon/modded-nanogpt/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1791, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/grace/Projects/nGPT-muon/modded-nanogpt/train_architectural.py", line 570, in forward
    logits = F.linear(x_norm, w_norm) * self.logit_scale
             ~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 786.00 MiB. GPU 0 has a total capacity of 95.00 GiB of which 316.31 MiB is free. Process 82129 has 10.08 GiB memory in use. Process 82969 has 17.06 GiB memory in use. Process 83815 has 29.49 GiB memory in use. Process 84640 has 10.08 GiB memory in use. Process 85486 has 10.08 GiB memory in use. Process 86327 has 10.08 GiB memory in use. Including non-PyTorch memory, this process has 7.77 GiB memory in use. Of the allocated memory 6.16 GiB is allocated by PyTorch, and 876.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
nGPT Architectural Experiment: G09_pe_fast
================================================================================
Model: 11 layers x 768 dim (matching modded-nanogpt)
Hypothesis: Baseline
================================================================================

Loading FineWeb data (subset for fast experiments)...
  Loading data/fineweb10B/fineweb_train_000001.bin...
  Loading data/fineweb10B/fineweb_train_000002.bin...
  Loading data/fineweb10B/fineweb_train_000003.bin...
  Loading data/fineweb10B/fineweb_train_000004.bin...
  Vocab size: 50257
  Train tokens: 400,001,536
  Val tokens: 100,000,384

Model config (modded-nanogpt size):
  vocab_size: 50257
  n_layer: 11
  n_embd: 768
  n_head: 6
  block_size: 128
Using GatedBlock (H3) with gate_init=0.0

Total parameters: 155,166,743

Optimizer: SimpleMuon (lr=0.008, momentum=0.95, geodesic=baseline, orthog=polar_express)
  2D weight params (Muon): 77,856,768
  1D params (Adam): 77,309,975

Starting training (5000 steps)...
--------------------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/grace/Projects/nGPT-muon/modded-nanogpt/train_architectural.py", line 822, in <module>
    result = main()
             ^^^^^^
  File "/home/grace/Projects/nGPT-muon/modded-nanogpt/train_architectural.py", line 716, in main
    logits, loss = model(x, y)
                   ^^^^^^^^^^^
  File "/home/grace/Projects/nGPT-muon/modded-nanogpt/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1780, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/grace/Projects/nGPT-muon/modded-nanogpt/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1791, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/grace/Projects/nGPT-muon/modded-nanogpt/train_architectural.py", line 570, in forward
    logits = F.linear(x_norm, w_norm) * self.logit_scale
             ~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.54 GiB. GPU 0 has a total capacity of 95.00 GiB of which 301.75 MiB is free. Process 82129 has 10.08 GiB memory in use. Process 82969 has 17.06 GiB memory in use. Process 83815 has 29.49 GiB memory in use. Process 84640 has 10.08 GiB memory in use. Process 85486 has 10.08 GiB memory in use. Process 86327 has 10.08 GiB memory in use. Including non-PyTorch memory, this process has 7.79 GiB memory in use. Of the allocated memory 7.05 GiB is allocated by PyTorch, and 45.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
nGPT Architectural Experiment: G10_ultra_fast
================================================================================
Model: 11 layers x 768 dim (matching modded-nanogpt)
Hypothesis: Baseline
================================================================================

Loading FineWeb data (subset for fast experiments)...
  Loading data/fineweb10B/fineweb_train_000001.bin...
  Loading data/fineweb10B/fineweb_train_000002.bin...
  Loading data/fineweb10B/fineweb_train_000003.bin...
  Loading data/fineweb10B/fineweb_train_000004.bin...
  Vocab size: 50257
  Train tokens: 400,001,536
  Val tokens: 100,000,384

Model config (modded-nanogpt size):
  vocab_size: 50257
  n_layer: 11
  n_embd: 768
  n_head: 6
  block_size: 128
Using GatedBlock (H3) with gate_init=0.0

Total parameters: 155,166,743

Optimizer: SimpleMuon (lr=0.015, momentum=0.9, geodesic=baseline, orthog=newton_schulz)
  2D weight params (Muon): 77,856,768
  1D params (Adam): 77,309,975

Starting training (5000 steps)...
--------------------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/grace/Projects/nGPT-muon/modded-nanogpt/train_architectural.py", line 822, in <module>
    result = main()
             ^^^^^^
  File "/home/grace/Projects/nGPT-muon/modded-nanogpt/train_architectural.py", line 716, in main
    logits, loss = model(x, y)
                   ^^^^^^^^^^^
  File "/home/grace/Projects/nGPT-muon/modded-nanogpt/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1780, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/grace/Projects/nGPT-muon/modded-nanogpt/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1791, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/grace/Projects/nGPT-muon/modded-nanogpt/train_architectural.py", line 570, in forward
    logits = F.linear(x_norm, w_norm) * self.logit_scale
             ~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.54 GiB. GPU 0 has a total capacity of 95.00 GiB of which 303.12 MiB is free. Process 82129 has 10.08 GiB memory in use. Process 82969 has 17.06 GiB memory in use. Process 83815 has 29.49 GiB memory in use. Process 84640 has 10.08 GiB memory in use. Process 85486 has 10.08 GiB memory in use. Process 86327 has 10.08 GiB memory in use. Including non-PyTorch memory, this process has 7.79 GiB memory in use. Of the allocated memory 7.05 GiB is allocated by PyTorch, and 45.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
nGPT Architectural Experiment: G11_conservative
================================================================================
Model: 11 layers x 768 dim (matching modded-nanogpt)
Hypothesis: Baseline
================================================================================

Loading FineWeb data (subset for fast experiments)...
  Loading data/fineweb10B/fineweb_train_000001.bin...
  Loading data/fineweb10B/fineweb_train_000002.bin...
  Loading data/fineweb10B/fineweb_train_000003.bin...
  Loading data/fineweb10B/fineweb_train_000004.bin...
  Vocab size: 50257
  Train tokens: 400,001,536
  Val tokens: 100,000,384

Model config (modded-nanogpt size):
  vocab_size: 50257
  n_layer: 11
  n_embd: 768
  n_head: 6
  block_size: 128
Using GatedBlock (H3) with gate_init=0.0

Total parameters: 155,166,743

Optimizer: SimpleMuon (lr=0.003, momentum=0.97, geodesic=baseline, orthog=newton_schulz)
  2D weight params (Muon): 77,856,768
  1D params (Adam): 77,309,975

Starting training (5000 steps)...
--------------------------------------------------------------------------------
Step    0: loss = 10.9734
Traceback (most recent call last):
  File "/home/grace/Projects/nGPT-muon/modded-nanogpt/train_architectural.py", line 822, in <module>
    result = main()
             ^^^^^^
  File "/home/grace/Projects/nGPT-muon/modded-nanogpt/train_architectural.py", line 716, in main
    logits, loss = model(x, y)
                   ^^^^^^^^^^^
  File "/home/grace/Projects/nGPT-muon/modded-nanogpt/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1780, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/grace/Projects/nGPT-muon/modded-nanogpt/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1791, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/grace/Projects/nGPT-muon/modded-nanogpt/train_architectural.py", line 570, in forward
    logits = F.linear(x_norm, w_norm) * self.logit_scale
             ~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 786.00 MiB. GPU 0 has a total capacity of 95.00 GiB of which 317.00 MiB is free. Process 82129 has 10.08 GiB memory in use. Process 82969 has 17.06 GiB memory in use. Process 83815 has 29.49 GiB memory in use. Process 84640 has 10.08 GiB memory in use. Process 85486 has 10.08 GiB memory in use. Process 86327 has 10.08 GiB memory in use. Including non-PyTorch memory, this process has 7.77 GiB memory in use. Of the allocated memory 6.16 GiB is allocated by PyTorch, and 876.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
nGPT Architectural Experiment: G12_mid_quality
================================================================================
Model: 11 layers x 768 dim (matching modded-nanogpt)
Hypothesis: Baseline
================================================================================

Loading FineWeb data (subset for fast experiments)...
  Loading data/fineweb10B/fineweb_train_000001.bin...
  Loading data/fineweb10B/fineweb_train_000002.bin...
  Loading data/fineweb10B/fineweb_train_000003.bin...
  Loading data/fineweb10B/fineweb_train_000004.bin...
  Vocab size: 50257
  Train tokens: 400,001,536
  Val tokens: 100,000,384

Model config (modded-nanogpt size):
  vocab_size: 50257
  n_layer: 11
  n_embd: 768
  n_head: 6
  block_size: 128
Using GatedBlock (H3) with gate_init=0.0

Total parameters: 155,166,743

Optimizer: SimpleMuon (lr=0.007, momentum=0.95, geodesic=baseline, orthog=newton_schulz)
  2D weight params (Muon): 77,856,768
  1D params (Adam): 77,309,975

Starting training (5000 steps)...
--------------------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/grace/Projects/nGPT-muon/modded-nanogpt/train_architectural.py", line 822, in <module>
    result = main()
             ^^^^^^
  File "/home/grace/Projects/nGPT-muon/modded-nanogpt/train_architectural.py", line 719, in main
    loss.backward()
  File "/home/grace/Projects/nGPT-muon/modded-nanogpt/.venv/lib/python3.12/site-packages/torch/_tensor.py", line 631, in backward
    torch.autograd.backward(
  File "/home/grace/Projects/nGPT-muon/modded-nanogpt/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py", line 365, in backward
    _engine_run_backward(
  File "/home/grace/Projects/nGPT-muon/modded-nanogpt/.venv/lib/python3.12/site-packages/torch/autograd/graph.py", line 865, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.15 GiB. GPU 0 has a total capacity of 95.00 GiB of which 759.81 MiB is free. Process 82129 has 10.08 GiB memory in use. Process 82969 has 17.06 GiB memory in use. Process 83815 has 29.49 GiB memory in use. Process 84640 has 10.08 GiB memory in use. Process 85486 has 10.08 GiB memory in use. Process 86327 has 10.08 GiB memory in use. Including non-PyTorch memory, this process has 7.34 GiB memory in use. Of the allocated memory 6.61 GiB is allocated by PyTorch, and 43.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
nGPT Architectural Experiment: G13_cwd_fast
================================================================================
Model: 11 layers x 768 dim (matching modded-nanogpt)
Hypothesis: Baseline
================================================================================

Loading FineWeb data (subset for fast experiments)...
  Loading data/fineweb10B/fineweb_train_000001.bin...
  Loading data/fineweb10B/fineweb_train_000002.bin...
  Loading data/fineweb10B/fineweb_train_000003.bin...
  Loading data/fineweb10B/fineweb_train_000004.bin...
  Vocab size: 50257
  Train tokens: 400,001,536
  Val tokens: 100,000,384

Model config (modded-nanogpt size):
  vocab_size: 50257
  n_layer: 11
  n_embd: 768
  n_head: 6
  block_size: 128
Using GatedBlock (H3) with gate_init=0.0

Total parameters: 155,166,743

Optimizer: SimpleMuon (lr=0.01, momentum=0.93, geodesic=baseline, orthog=newton_schulz)
  2D weight params (Muon): 77,856,768
  1D params (Adam): 77,309,975

Starting training (5000 steps)...
--------------------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/grace/Projects/nGPT-muon/modded-nanogpt/train_architectural.py", line 822, in <module>
    result = main()
             ^^^^^^
  File "/home/grace/Projects/nGPT-muon/modded-nanogpt/train_architectural.py", line 719, in main
    loss.backward()
  File "/home/grace/Projects/nGPT-muon/modded-nanogpt/.venv/lib/python3.12/site-packages/torch/_tensor.py", line 631, in backward
    torch.autograd.backward(
  File "/home/grace/Projects/nGPT-muon/modded-nanogpt/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py", line 365, in backward
    _engine_run_backward(
  File "/home/grace/Projects/nGPT-muon/modded-nanogpt/.venv/lib/python3.12/site-packages/torch/autograd/graph.py", line 865, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.15 GiB. GPU 0 has a total capacity of 95.00 GiB of which 758.62 MiB is free. Process 82129 has 10.08 GiB memory in use. Process 82969 has 17.06 GiB memory in use. Process 83815 has 29.49 GiB memory in use. Process 84640 has 10.08 GiB memory in use. Process 85486 has 10.08 GiB memory in use. Process 86327 has 10.08 GiB memory in use. Including non-PyTorch memory, this process has 7.34 GiB memory in use. Of the allocated memory 6.61 GiB is allocated by PyTorch, and 43.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
nGPT Architectural Experiment: G14_vr_mid
================================================================================
Model: 11 layers x 768 dim (matching modded-nanogpt)
Hypothesis: Baseline
================================================================================

Loading FineWeb data (subset for fast experiments)...
  Loading data/fineweb10B/fineweb_train_000001.bin...
  Loading data/fineweb10B/fineweb_train_000002.bin...
  Loading data/fineweb10B/fineweb_train_000003.bin...
  Loading data/fineweb10B/fineweb_train_000004.bin...
  Vocab size: 50257
  Train tokens: 400,001,536
  Val tokens: 100,000,384

Model config (modded-nanogpt size):
  vocab_size: 50257
  n_layer: 11
  n_embd: 768
  n_head: 6
  block_size: 128
Using GatedBlock (H3) with gate_init=0.0

Total parameters: 155,166,743

Optimizer: SimpleMuon (lr=0.006, momentum=0.95, geodesic=baseline, orthog=newton_schulz)
  2D weight params (Muon): 77,856,768
  1D params (Adam): 77,309,975

Starting training (5000 steps)...
--------------------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/grace/Projects/nGPT-muon/modded-nanogpt/train_architectural.py", line 822, in <module>
    result = main()
             ^^^^^^
  File "/home/grace/Projects/nGPT-muon/modded-nanogpt/train_architectural.py", line 719, in main
    loss.backward()
  File "/home/grace/Projects/nGPT-muon/modded-nanogpt/.venv/lib/python3.12/site-packages/torch/_tensor.py", line 631, in backward
    torch.autograd.backward(
  File "/home/grace/Projects/nGPT-muon/modded-nanogpt/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py", line 365, in backward
    _engine_run_backward(
  File "/home/grace/Projects/nGPT-muon/modded-nanogpt/.venv/lib/python3.12/site-packages/torch/autograd/graph.py", line 865, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 982.00 MiB. GPU 0 has a total capacity of 95.00 GiB of which 681.25 MiB is free. Process 82129 has 10.08 GiB memory in use. Process 82969 has 17.06 GiB memory in use. Process 83815 has 29.49 GiB memory in use. Process 84640 has 10.08 GiB memory in use. Process 85486 has 10.08 GiB memory in use. Process 86327 has 10.08 GiB memory in use. Including non-PyTorch memory, this process has 7.42 GiB memory in use. Of the allocated memory 6.68 GiB is allocated by PyTorch, and 44.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
nGPT Architectural Experiment: G15_balanced2
================================================================================
Model: 11 layers x 768 dim (matching modded-nanogpt)
Hypothesis: Baseline
================================================================================

Loading FineWeb data (subset for fast experiments)...
  Loading data/fineweb10B/fineweb_train_000001.bin...
  Loading data/fineweb10B/fineweb_train_000002.bin...
  Loading data/fineweb10B/fineweb_train_000003.bin...
  Loading data/fineweb10B/fineweb_train_000004.bin...
  Vocab size: 50257
  Train tokens: 400,001,536
  Val tokens: 100,000,384

Model config (modded-nanogpt size):
  vocab_size: 50257
  n_layer: 11
  n_embd: 768
  n_head: 6
  block_size: 128
Using GatedBlock (H3) with gate_init=0.0

Total parameters: 155,166,743

Optimizer: SimpleMuon (lr=0.009, momentum=0.94, geodesic=baseline, orthog=newton_schulz)
  2D weight params (Muon): 77,856,768
  1D params (Adam): 77,309,975

Starting training (5000 steps)...
--------------------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/grace/Projects/nGPT-muon/modded-nanogpt/train_architectural.py", line 822, in <module>
    result = main()
             ^^^^^^
  File "/home/grace/Projects/nGPT-muon/modded-nanogpt/train_architectural.py", line 716, in main
    logits, loss = model(x, y)
                   ^^^^^^^^^^^
  File "/home/grace/Projects/nGPT-muon/modded-nanogpt/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1780, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/grace/Projects/nGPT-muon/modded-nanogpt/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1791, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/grace/Projects/nGPT-muon/modded-nanogpt/train_architectural.py", line 570, in forward
    logits = F.linear(x_norm, w_norm) * self.logit_scale
             ~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.34 GiB. GPU 0 has a total capacity of 95.00 GiB of which 1012.94 MiB is free. Process 82129 has 10.08 GiB memory in use. Process 82969 has 17.06 GiB memory in use. Process 83815 has 29.49 GiB memory in use. Process 84640 has 10.08 GiB memory in use. Process 85486 has 10.08 GiB memory in use. Process 86327 has 10.08 GiB memory in use. Including non-PyTorch memory, this process has 7.09 GiB memory in use. Of the allocated memory 6.35 GiB is allocated by PyTorch, and 46.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
nGPT Architectural Experiment: G16_pe_stable
================================================================================
Model: 11 layers x 768 dim (matching modded-nanogpt)
Hypothesis: Baseline
================================================================================

Loading FineWeb data (subset for fast experiments)...
  Loading data/fineweb10B/fineweb_train_000001.bin...
  Loading data/fineweb10B/fineweb_train_000002.bin...
  Loading data/fineweb10B/fineweb_train_000003.bin...
  Loading data/fineweb10B/fineweb_train_000004.bin...
  Vocab size: 50257
  Train tokens: 400,001,536
  Val tokens: 100,000,384

Model config (modded-nanogpt size):
  vocab_size: 50257
  n_layer: 11
  n_embd: 768
  n_head: 6
  block_size: 128
Using GatedBlock (H3) with gate_init=0.0

Total parameters: 155,166,743

Optimizer: SimpleMuon (lr=0.004, momentum=0.96, geodesic=baseline, orthog=polar_express)
  2D weight params (Muon): 77,856,768
  1D params (Adam): 77,309,975

Starting training (5000 steps)...
--------------------------------------------------------------------------------
Step    0: loss = 10.9764
Traceback (most recent call last):
  File "/home/grace/Projects/nGPT-muon/modded-nanogpt/train_architectural.py", line 822, in <module>
    result = main()
             ^^^^^^
  File "/home/grace/Projects/nGPT-muon/modded-nanogpt/train_architectural.py", line 716, in main
    logits, loss = model(x, y)
                   ^^^^^^^^^^^
  File "/home/grace/Projects/nGPT-muon/modded-nanogpt/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1780, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/grace/Projects/nGPT-muon/modded-nanogpt/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1791, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/grace/Projects/nGPT-muon/modded-nanogpt/train_architectural.py", line 570, in forward
    logits = F.linear(x_norm, w_norm) * self.logit_scale
             ~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 786.00 MiB. GPU 0 has a total capacity of 95.00 GiB of which 315.00 MiB is free. Process 82129 has 10.08 GiB memory in use. Process 82969 has 17.06 GiB memory in use. Process 83815 has 29.49 GiB memory in use. Process 84640 has 10.08 GiB memory in use. Process 85486 has 10.08 GiB memory in use. Process 86327 has 10.08 GiB memory in use. Including non-PyTorch memory, this process has 7.77 GiB memory in use. Of the allocated memory 6.16 GiB is allocated by PyTorch, and 876.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
================================================================================
TWO-STAGE HYPERPARAMETER SWEEP
================================================================================
Objective: Minimize wall-clock time to reach val_loss < 5.0
Model: 11L  768D (155M params)
Max steps: 5000, Validation freq: 50
Timeout per experiment: 600s (10.0 min)
================================================================================

================================================================================
STAGE 1: BROAD SWEEP (48 CONFIGURATIONS)
================================================================================
Total configurations: 48
Estimated time: 2.4 - 4.0 hours

[1/48] 
================================================================================
[Stage 1] Running: A1_ne
================================================================================
  LR: 0.0030, Batch: 32, Lazy: 7
  Orthog: newton_schulz, VR: False, CWD: False
  Momentum: 0.950
 TIMEOUT after 600.0s (hit 600s limit)

[2/48] 
================================================================================
[Stage 1] Running: A1_po
================================================================================
  LR: 0.0030, Batch: 32, Lazy: 7
  Orthog: polar_express, VR: False, CWD: False
  Momentum: 0.950
 TIMEOUT after 600.0s (hit 600s limit)

[3/48] 
================================================================================
[Stage 1] Running: A2_ne
================================================================================
  LR: 0.0100, Batch: 32, Lazy: 10
  Orthog: newton_schulz, VR: False, CWD: False
  Momentum: 0.950
 TIMEOUT after 600.0s (hit 600s limit)

[4/48] 
================================================================================
[Stage 1] Running: A2_po
================================================================================
  LR: 0.0100, Batch: 32, Lazy: 10
  Orthog: polar_express, VR: False, CWD: False
  Momentum: 0.950
 TIMEOUT after 600.0s (hit 600s limit)

[5/48] 
================================================================================
[Stage 1] Running: A3_ne
================================================================================
  LR: 0.0030, Batch: 32, Lazy: 7
  Orthog: newton_schulz, VR: True, CWD: True
  Momentum: 0.950
 TIMEOUT after 600.0s (hit 600s limit)

[6/48] 
================================================================================
[Stage 1] Running: A3_po
================================================================================
  LR: 0.0030, Batch: 32, Lazy: 7
  Orthog: polar_express, VR: True, CWD: True
  Momentum: 0.950
 TIMEOUT after 600.0s (hit 600s limit)

[7/48] 
================================================================================
[Stage 1] Running: A4_ne
================================================================================
  LR: 0.0100, Batch: 64, Lazy: 10
  Orthog: newton_schulz, VR: False, CWD: False
  Momentum: 0.900
 TIMEOUT after 600.0s (hit 600s limit)

[8/48] 
================================================================================
[Stage 1] Running: A4_po
================================================================================
  LR: 0.0100, Batch: 64, Lazy: 10
  Orthog: polar_express, VR: False, CWD: False
  Momentum: 0.900
 TIMEOUT after 600.0s (hit 600s limit)

[9/48] 
================================================================================
[Stage 1] Running: B1_vr0_cwd0
================================================================================
  LR: 0.0030, Batch: 32, Lazy: 7
  Orthog: newton_schulz, VR: False, CWD: False
  Momentum: 0.950
 TIMEOUT after 600.0s (hit 600s limit)

[10/48] 
================================================================================
[Stage 1] Running: B2_vr1_cwd0
================================================================================
  LR: 0.0030, Batch: 32, Lazy: 7
  Orthog: newton_schulz, VR: True, CWD: False
  Momentum: 0.950
 TIMEOUT after 600.0s (hit 600s limit)

[11/48] 
================================================================================
[Stage 1] Running: B3_vr0_cwd1
================================================================================
  LR: 0.0030, Batch: 32, Lazy: 7
  Orthog: newton_schulz, VR: False, CWD: True
  Momentum: 0.950
 TIMEOUT after 600.0s (hit 600s limit)

[12/48] 
================================================================================
[Stage 1] Running: B4_vr1_cwd1
================================================================================
  LR: 0.0030, Batch: 32, Lazy: 7
  Orthog: newton_schulz, VR: True, CWD: True
  Momentum: 0.950
 TIMEOUT after 600.0s (hit 600s limit)

[13/48] 
================================================================================
[Stage 1] Running: C_lazy3
================================================================================
  LR: 0.0030, Batch: 32, Lazy: 3
  Orthog: newton_schulz, VR: False, CWD: False
  Momentum: 0.950
 TIMEOUT after 600.0s (hit 600s limit)

[14/48] 
================================================================================
[Stage 1] Running: C_lazy5
================================================================================
  LR: 0.0030, Batch: 32, Lazy: 5
  Orthog: newton_schulz, VR: False, CWD: False
  Momentum: 0.950
 TIMEOUT after 600.0s (hit 600s limit)

[15/48] 
================================================================================
[Stage 1] Running: C_lazy7
================================================================================
  LR: 0.0030, Batch: 32, Lazy: 7
  Orthog: newton_schulz, VR: False, CWD: False
  Momentum: 0.950
 TIMEOUT after 600.0s (hit 600s limit)

[16/48] 
================================================================================
[Stage 1] Running: C_lazy10
================================================================================
  LR: 0.0030, Batch: 32, Lazy: 10
  Orthog: newton_schulz, VR: False, CWD: False
  Momentum: 0.950
 TIMEOUT after 600.0s (hit 600s limit)

[17/48] 
================================================================================
[Stage 1] Running: C_lazy15
================================================================================
  LR: 0.0030, Batch: 32, Lazy: 15
  Orthog: newton_schulz, VR: False, CWD: False
  Momentum: 0.950
 TIMEOUT after 600.0s (hit 600s limit)

[18/48] 
================================================================================
[Stage 1] Running: C_lazy20
================================================================================
  LR: 0.0030, Batch: 32, Lazy: 20
  Orthog: newton_schulz, VR: False, CWD: False
  Momentum: 0.950
 TIMEOUT after 600.0s (hit 600s limit)

[19/48] 
================================================================================
[Stage 1] Running: D_lr0.0005
================================================================================
  LR: 0.0005, Batch: 32, Lazy: 7
  Orthog: newton_schulz, VR: False, CWD: False
  Momentum: 0.950
 TIMEOUT after 600.0s (hit 600s limit)

[20/48] 
================================================================================
[Stage 1] Running: D_lr0.001
================================================================================
  LR: 0.0010, Batch: 32, Lazy: 7
  Orthog: newton_schulz, VR: False, CWD: False
  Momentum: 0.950
 TIMEOUT after 600.0s (hit 600s limit)

[21/48] 
================================================================================
[Stage 1] Running: D_lr0.003
================================================================================
  LR: 0.0030, Batch: 32, Lazy: 7
  Orthog: newton_schulz, VR: False, CWD: False
  Momentum: 0.950
 TIMEOUT after 600.0s (hit 600s limit)

[22/48] 
================================================================================
[Stage 1] Running: D_lr0.005
================================================================================
  LR: 0.0050, Batch: 32, Lazy: 7
  Orthog: newton_schulz, VR: False, CWD: False
  Momentum: 0.950
 TIMEOUT after 600.0s (hit 600s limit)

[23/48] 
================================================================================
[Stage 1] Running: D_lr0.01
================================================================================
  LR: 0.0100, Batch: 32, Lazy: 7
  Orthog: newton_schulz, VR: False, CWD: False
  Momentum: 0.950
 TIMEOUT after 600.0s (hit 600s limit)

[24/48] 
================================================================================
[Stage 1] Running: D_lr0.02
================================================================================
  LR: 0.0200, Batch: 32, Lazy: 7
  Orthog: newton_schulz, VR: False, CWD: False
  Momentum: 0.950
 TIMEOUT after 600.0s (hit 600s limit)

[25/48] 
================================================================================
[Stage 1] Running: E_batch16
================================================================================
  LR: 0.0015, Batch: 16, Lazy: 7
  Orthog: newton_schulz, VR: False, CWD: False
  Momentum: 0.950
 TIMEOUT after 600.0s (hit 600s limit)

[26/48] 
================================================================================
[Stage 1] Running: E_batch32
================================================================================
  LR: 0.0030, Batch: 32, Lazy: 7
  Orthog: newton_schulz, VR: False, CWD: False
  Momentum: 0.950
 TIMEOUT after 600.0s (hit 600s limit)

[27/48] 
================================================================================
[Stage 1] Running: E_batch64
================================================================================
  LR: 0.0060, Batch: 64, Lazy: 7
  Orthog: newton_schulz, VR: False, CWD: False
  Momentum: 0.950
 TIMEOUT after 600.0s (hit 600s limit)

[28/48] 
================================================================================
[Stage 1] Running: E_batch128
================================================================================
  LR: 0.0120, Batch: 128, Lazy: 7
  Orthog: newton_schulz, VR: False, CWD: False
  Momentum: 0.950
 TIMEOUT after 600.0s (hit 600s limit)

[29/48] 
================================================================================
[Stage 1] Running: F_mom0.85
================================================================================
  LR: 0.0030, Batch: 32, Lazy: 7
  Orthog: newton_schulz, VR: False, CWD: False
  Momentum: 0.850
 TIMEOUT after 600.0s (hit 600s limit)

[30/48] 
================================================================================
[Stage 1] Running: F_mom0.9
================================================================================
  LR: 0.0030, Batch: 32, Lazy: 7
  Orthog: newton_schulz, VR: False, CWD: False
  Momentum: 0.900
 TIMEOUT after 600.0s (hit 600s limit)

[31/48] 
================================================================================
[Stage 1] Running: F_mom0.95
================================================================================
  LR: 0.0030, Batch: 32, Lazy: 7
  Orthog: newton_schulz, VR: False, CWD: False
  Momentum: 0.950
 TIMEOUT after 600.0s (hit 600s limit)

[32/48] 
================================================================================
[Stage 1] Running: F_mom0.98
================================================================================
  LR: 0.0030, Batch: 32, Lazy: 7
  Orthog: newton_schulz, VR: False, CWD: False
  Momentum: 0.980
 FAILED with return code 1 after 6.8s

[33/48] 
================================================================================
[Stage 1] Running: G01_fast
================================================================================
  LR: 0.0100, Batch: 64, Lazy: 10
  Orthog: newton_schulz, VR: False, CWD: False
  Momentum: 0.950
 FAILED with return code 1 after 6.8s

[34/48] 
================================================================================
[Stage 1] Running: G02_fast_safe
================================================================================
  LR: 0.0100, Batch: 64, Lazy: 7
  Orthog: newton_schulz, VR: False, CWD: False
  Momentum: 0.900
 FAILED with return code 1 after 6.1s

[35/48] 
================================================================================
[Stage 1] Running: G03_fast_quality
================================================================================
  LR: 0.0100, Batch: 32, Lazy: 10
  Orthog: newton_schulz, VR: True, CWD: True
  Momentum: 0.950
 FAILED with return code 1 after 7.3s

[36/48] 
================================================================================
[Stage 1] Running: G04_pe_quality
================================================================================
  LR: 0.0030, Batch: 32, Lazy: 5
  Orthog: polar_express, VR: True, CWD: True
  Momentum: 0.950
 FAILED with return code 1 after 7.0s

[37/48] 
================================================================================
[Stage 1] Running: G05_balanced
================================================================================
  LR: 0.0080, Batch: 48, Lazy: 8
  Orthog: newton_schulz, VR: False, CWD: False
  Momentum: 0.940
 FAILED with return code 1 after 7.3s

[38/48] 
================================================================================
[Stage 1] Running: G06_aggressive
================================================================================
  LR: 0.0120, Batch: 64, Lazy: 12
  Orthog: newton_schulz, VR: False, CWD: False
  Momentum: 0.920
 FAILED with return code 1 after 5.8s

[39/48] 
================================================================================
[Stage 1] Running: G07_vr_only
================================================================================
  LR: 0.0050, Batch: 32, Lazy: 7
  Orthog: newton_schulz, VR: True, CWD: False
  Momentum: 0.960
 FAILED with return code 1 after 7.4s

[40/48] 
================================================================================
[Stage 1] Running: G08_cwd_only
================================================================================
  LR: 0.0050, Batch: 32, Lazy: 7
  Orthog: newton_schulz, VR: False, CWD: True
  Momentum: 0.960
 FAILED with return code 1 after 7.3s

[41/48] 
================================================================================
[Stage 1] Running: G09_pe_fast
================================================================================
  LR: 0.0080, Batch: 64, Lazy: 10
  Orthog: polar_express, VR: False, CWD: False
  Momentum: 0.950
 FAILED with return code 1 after 7.0s

[42/48] 
================================================================================
[Stage 1] Running: G10_ultra_fast
================================================================================
  LR: 0.0150, Batch: 64, Lazy: 15
  Orthog: newton_schulz, VR: False, CWD: False
  Momentum: 0.900
 FAILED with return code 1 after 6.7s

[43/48] 
================================================================================
[Stage 1] Running: G11_conservative
================================================================================
  LR: 0.0030, Batch: 32, Lazy: 3
  Orthog: newton_schulz, VR: True, CWD: True
  Momentum: 0.970
 FAILED with return code 1 after 7.1s

[44/48] 
================================================================================
[Stage 1] Running: G12_mid_quality
================================================================================
  LR: 0.0070, Batch: 48, Lazy: 7
  Orthog: newton_schulz, VR: True, CWD: True
  Momentum: 0.950
 FAILED with return code 1 after 7.0s

[45/48] 
================================================================================
[Stage 1] Running: G13_cwd_fast
================================================================================
  LR: 0.0100, Batch: 48, Lazy: 10
  Orthog: newton_schulz, VR: False, CWD: True
  Momentum: 0.930
 FAILED with return code 1 after 8.1s

[46/48] 
================================================================================
[Stage 1] Running: G14_vr_mid
================================================================================
  LR: 0.0060, Batch: 40, Lazy: 6
  Orthog: newton_schulz, VR: True, CWD: False
  Momentum: 0.950
 FAILED with return code 1 after 7.2s

[47/48] 
================================================================================
[Stage 1] Running: G15_balanced2
================================================================================
  LR: 0.0090, Batch: 56, Lazy: 9
  Orthog: newton_schulz, VR: False, CWD: False
  Momentum: 0.940
 FAILED with return code 1 after 7.1s

[48/48] 
================================================================================
[Stage 1] Running: G16_pe_stable
================================================================================
  LR: 0.0040, Batch: 32, Lazy: 8
  Orthog: polar_express, VR: True, CWD: True
  Momentum: 0.960
 FAILED with return code 1 after 7.5s

================================================================================
STAGE 1 COMPLETE
================================================================================
Total time: 312.0 minutes
Successful: 0/48
Failed: 48/48

================================================================================
STAGE 1 ANALYSIS
================================================================================
Total experiments: 25
  Reached target (<5.0): 0
  Failed to reach target: 25

 WARNING: Only 0 configs reached target!
  May need to adjust target loss or extend max steps.

 NO CONFIGS REACHED TARGET!
  Falling back to analysis of best performers...

================================================================================
TOP 5 CONFIGURATIONS
================================================================================
Rank   Name                 Time (s)     Steps    Val Loss   Reached
--------------------------------------------------------------------------------
1      A3_po                TIMEOUT      4999     5.6084     
2      A3_ne                TIMEOUT      4999     5.6997     
3      A1_po                TIMEOUT      4999     5.7006     
4      C_lazy10             TIMEOUT      4999     5.7060     
5      A1_ne                TIMEOUT      4999     5.7084     

================================================================================
OPTIMAL PARAMETER RANGES (from top 5)
================================================================================
  lr: [0.003, 0.003]
  lazy_proj_freq: [7, 10]
  batch_size: [32, 32]

  Orthog methods in top 5:
    newton_schulz: 3/5
    polar_express: 2/5

  Advanced features in top 5:
    Variance Reduction: 2/5
    Cautious WD: 2/5

 No configurations reached target. Stopping sweep.
  Consider:
  - Increasing max steps
  - Relaxing target loss threshold
  - Checking if target <5.0 is achievable with this model size
nGPT Architectural Experiment: E_batch32
================================================================================
Model: 11 layers x 768 dim (matching modded-nanogpt)
Hypothesis: Baseline
================================================================================

Loading FineWeb data (subset for fast experiments)...
  Loading data/fineweb10B/fineweb_train_000001.bin...
  Loading data/fineweb10B/fineweb_train_000002.bin...
  Loading data/fineweb10B/fineweb_train_000003.bin...
  Loading data/fineweb10B/fineweb_train_000004.bin...
  Vocab size: 50257
  Train tokens: 400,001,536
  Val tokens: 100,000,384

Model config (modded-nanogpt size):
  vocab_size: 50257
  n_layer: 11
  n_embd: 768
  n_head: 6
  block_size: 128
Using GatedBlock (H3) with gate_init=0.0

Total parameters: 155,166,743

Optimizer: SimpleMuon (lr=0.003, momentum=0.95, geodesic=baseline, orthog=newton_schulz)
  2D weight params (Muon): 77,856,768
  1D params (Adam): 77,309,975

Starting training (5000 steps)...
--------------------------------------------------------------------------------
Step    0: loss = 10.9837
Step   50: loss = 7.3624
  [Val @ step 50]: train=7.3624, val=7.4132
Step  100: loss = 7.1193
  [Val @ step 100]: train=7.1193, val=6.9383
Step  150: loss = 6.7721
  [Val @ step 150]: train=6.7721, val=6.7473
Step  200: loss = 6.6238
  [Val @ step 200]: train=6.6238, val=6.6466
Step  250: loss = 6.3265
  [Val @ step 250]: train=6.3265, val=6.5464
Step  300: loss = 6.4794
  [Val @ step 300]: train=6.4794, val=6.4858
Step  350: loss = 6.4115
  [Val @ step 350]: train=6.4115, val=6.4665
Step  400: loss = 6.5829
  [Val @ step 400]: train=6.5829, val=6.4084
Step  450: loss = 6.5614
  [Val @ step 450]: train=6.5614, val=6.3652
Step  500: loss = 6.2560
  [Val @ step 500]: train=6.2560, val=6.3358
Step  550: loss = 6.2553
  [Val @ step 550]: train=6.2553, val=6.2495
Step  600: loss = 6.2888
  [Val @ step 600]: train=6.2888, val=6.2805
Step  650: loss = 6.2082
  [Val @ step 650]: train=6.2082, val=6.1635
Step  700: loss = 6.0503
  [Val @ step 700]: train=6.0503, val=6.2831
Step  750: loss = 6.1438
  [Val @ step 750]: train=6.1438, val=6.2125
Step  800: loss = 6.1519
  [Val @ step 800]: train=6.1519, val=6.1806
Step  850: loss = 5.9895
  [Val @ step 850]: train=5.9895, val=6.1256
Step  900: loss = 6.1322
  [Val @ step 900]: train=6.1322, val=6.1652
Step  950: loss = 6.0577
  [Val @ step 950]: train=6.0577, val=6.1151
Step 1000: loss = 6.1021
  [Val @ step 1000]: train=6.1021, val=6.0887
Step 1050: loss = 6.0712
  [Val @ step 1050]: train=6.0712, val=6.0474
Step 1100: loss = 6.0484
  [Val @ step 1100]: train=6.0484, val=6.1256
Step 1150: loss = 6.0001
  [Val @ step 1150]: train=6.0001, val=6.0816
Step 1200: loss = 5.9163
  [Val @ step 1200]: train=5.9163, val=6.0962
Step 1250: loss = 6.0198
  [Val @ step 1250]: train=6.0198, val=6.0367
Step 1300: loss = 6.1776
  [Val @ step 1300]: train=6.1776, val=6.0504
Step 1350: loss = 5.9638
  [Val @ step 1350]: train=5.9638, val=6.0255
Step 1400: loss = 6.0497
  [Val @ step 1400]: train=6.0497, val=6.0288
Step 1450: loss = 5.8110
  [Val @ step 1450]: train=5.8110, val=5.9952
Step 1500: loss = 5.9107
  [Val @ step 1500]: train=5.9107, val=5.9600
Step 1550: loss = 5.9597
  [Val @ step 1550]: train=5.9597, val=5.9953
Step 1600: loss = 5.8781
  [Val @ step 1600]: train=5.8781, val=5.9741
Step 1650: loss = 5.7372
  [Val @ step 1650]: train=5.7372, val=5.9766
Step 1700: loss = 6.0140
  [Val @ step 1700]: train=6.0140, val=5.9683
Step 1750: loss = 6.0395
  [Val @ step 1750]: train=6.0395, val=5.9528
Step 1800: loss = 5.9373
  [Val @ step 1800]: train=5.9373, val=5.9598
Step 1850: loss = 5.7769
  [Val @ step 1850]: train=5.7769, val=5.9134
Step 1900: loss = 5.9370
  [Val @ step 1900]: train=5.9370, val=5.9473
Step 1950: loss = 6.0005
  [Val @ step 1950]: train=6.0005, val=5.9194
Step 2000: loss = 5.7804
  [Val @ step 2000]: train=5.7804, val=5.8765
Step 2050: loss = 6.0020
  [Val @ step 2050]: train=6.0020, val=5.8983
Step 2100: loss = 5.9398
  [Val @ step 2100]: train=5.9398, val=5.9078
Step 2150: loss = 5.8127
  [Val @ step 2150]: train=5.8127, val=5.9120
Step 2200: loss = 5.9177
  [Val @ step 2200]: train=5.9177, val=5.8880
Step 2250: loss = 5.9055
  [Val @ step 2250]: train=5.9055, val=5.9338
Step 2300: loss = 5.8400
  [Val @ step 2300]: train=5.8400, val=5.9085
Step 2350: loss = 5.7968
  [Val @ step 2350]: train=5.7968, val=5.8609
Step 2400: loss = 5.9075
  [Val @ step 2400]: train=5.9075, val=5.8526
Step 2450: loss = 5.9930
  [Val @ step 2450]: train=5.9930, val=5.8686
Step 2500: loss = 5.8177
  [Val @ step 2500]: train=5.8177, val=5.8673
Step 2550: loss = 5.7845
  [Val @ step 2550]: train=5.7845, val=5.8293
Step 2600: loss = 5.6874
  [Val @ step 2600]: train=5.6874, val=5.8541
Step 2650: loss = 5.8212
  [Val @ step 2650]: train=5.8212, val=5.8332
Step 2700: loss = 5.9042
  [Val @ step 2700]: train=5.9042, val=5.8362
Step 2750: loss = 5.9011
  [Val @ step 2750]: train=5.9011, val=5.8090
Step 2800: loss = 5.8391
  [Val @ step 2800]: train=5.8391, val=5.8363
Step 2850: loss = 5.9431
  [Val @ step 2850]: train=5.9431, val=5.8383
Step 2900: loss = 5.9293
  [Val @ step 2900]: train=5.9293, val=5.8342
Step 2950: loss = 5.8484
  [Val @ step 2950]: train=5.8484, val=5.8334
Step 3000: loss = 5.8955
  [Val @ step 3000]: train=5.8955, val=5.8187
Step 3050: loss = 5.7512
  [Val @ step 3050]: train=5.7512, val=5.7741
Step 3100: loss = 5.8730
  [Val @ step 3100]: train=5.8730, val=5.8131
Step 3150: loss = 5.7484
  [Val @ step 3150]: train=5.7484, val=5.7821
Step 3200: loss = 5.8802
  [Val @ step 3200]: train=5.8802, val=5.8405
Step 3250: loss = 5.9276
  [Val @ step 3250]: train=5.9276, val=5.8178
Step 3300: loss = 5.5331
  [Val @ step 3300]: train=5.5331, val=5.8138
Step 3350: loss = 5.8090
  [Val @ step 3350]: train=5.8090, val=5.8124
Step 3400: loss = 5.6572
  [Val @ step 3400]: train=5.6572, val=5.8024
Step 3450: loss = 5.7465
  [Val @ step 3450]: train=5.7465, val=5.8138
Step 3500: loss = 5.7402
  [Val @ step 3500]: train=5.7402, val=5.7970
Step 3550: loss = 5.7920
  [Val @ step 3550]: train=5.7920, val=5.7986
Step 3600: loss = 5.7326
  [Val @ step 3600]: train=5.7326, val=5.7991
Step 3650: loss = 5.7566
  [Val @ step 3650]: train=5.7566, val=5.7496
Step 3700: loss = 5.5834
  [Val @ step 3700]: train=5.5834, val=5.8093
Step 3750: loss = 5.8689
  [Val @ step 3750]: train=5.8689, val=5.7847
Step 3800: loss = 5.6412
  [Val @ step 3800]: train=5.6412, val=5.8078
Step 3850: loss = 5.5963
  [Val @ step 3850]: train=5.5963, val=5.8120
Step 3900: loss = 5.6357
  [Val @ step 3900]: train=5.6357, val=5.7771
Step 3950: loss = 5.5614
  [Val @ step 3950]: train=5.5614, val=5.7527
Step 4000: loss = 5.8221
  [Val @ step 4000]: train=5.8221, val=5.7417
Step 4050: loss = 5.8638
  [Val @ step 4050]: train=5.8638, val=5.7716
Step 4100: loss = 5.7101
  [Val @ step 4100]: train=5.7101, val=5.7316
Step 4150: loss = 5.6978
  [Val @ step 4150]: train=5.6978, val=5.7895
Step 4200: loss = 5.8276
  [Val @ step 4200]: train=5.8276, val=5.7881
Step 4250: loss = 5.7250
  [Val @ step 4250]: train=5.7250, val=5.7143
Step 4300: loss = 5.5439
  [Val @ step 4300]: train=5.5439, val=5.7512
Step 4350: loss = 5.7599
  [Val @ step 4350]: train=5.7599, val=5.7767
Step 4400: loss = 5.6253
  [Val @ step 4400]: train=5.6253, val=5.7846
Step 4450: loss = 5.9777
  [Val @ step 4450]: train=5.9777, val=5.7722
Step 4500: loss = 5.6322
  [Val @ step 4500]: train=5.6322, val=5.7678
Step 4550: loss = 5.6032
  [Val @ step 4550]: train=5.6032, val=5.7422
Step 4600: loss = 5.6505
  [Val @ step 4600]: train=5.6505, val=5.7870
Step 4650: loss = 5.7705
  [Val @ step 4650]: train=5.7705, val=5.7796
Step 4700: loss = 5.9863
  [Val @ step 4700]: train=5.9863, val=5.7312
Step 4750: loss = 5.7718
  [Val @ step 4750]: train=5.7718, val=5.7547
Step 4800: loss = 5.7935
  [Val @ step 4800]: train=5.7935, val=5.7330
Step 4850: loss = 5.7919
  [Val @ step 4850]: train=5.7919, val=5.7531
Step 4900: loss = 5.7771
  [Val @ step 4900]: train=5.7771, val=5.7791
Step 4950: loss = 5.6503
  [Val @ step 4950]: train=5.6503, val=5.7431
--------------------------------------------------------------------------------
Computing final validation loss...
Validation loss: 5.7470

Results saved to time_to_target_stage1_20251225_225546.jsonl
Training time: 3785.2s
Throughput: 5,411 tokens/sec
Projections: 715/5000
nGPT Architectural Experiment: F_mom0.85
================================================================================
Model: 11 layers x 768 dim (matching modded-nanogpt)
Hypothesis: Baseline
================================================================================

Loading FineWeb data (subset for fast experiments)...
  Loading data/fineweb10B/fineweb_train_000001.bin...
  Loading data/fineweb10B/fineweb_train_000002.bin...
  Loading data/fineweb10B/fineweb_train_000003.bin...
  Loading data/fineweb10B/fineweb_train_000004.bin...
  Vocab size: 50257
  Train tokens: 400,001,536
  Val tokens: 100,000,384

Model config (modded-nanogpt size):
  vocab_size: 50257
  n_layer: 11
  n_embd: 768
  n_head: 6
  block_size: 128
Using GatedBlock (H3) with gate_init=0.0

Total parameters: 155,166,743

Optimizer: SimpleMuon (lr=0.003, momentum=0.85, geodesic=baseline, orthog=newton_schulz)
  2D weight params (Muon): 77,856,768
  1D params (Adam): 77,309,975

Starting training (5000 steps)...
--------------------------------------------------------------------------------
Step    0: loss = 10.9599
Step   50: loss = 7.3704
  [Val @ step 50]: train=7.3704, val=7.3191
Step  100: loss = 7.0315
  [Val @ step 100]: train=7.0315, val=6.9506
Step  150: loss = 6.6918
  [Val @ step 150]: train=6.6918, val=6.7449
Step  200: loss = 6.5075
  [Val @ step 200]: train=6.5075, val=6.6993
Step  250: loss = 6.5449
  [Val @ step 250]: train=6.5449, val=6.5505
Step  300: loss = 6.4921
  [Val @ step 300]: train=6.4921, val=6.4812
Step  350: loss = 6.4326
  [Val @ step 350]: train=6.4326, val=6.4383
Step  400: loss = 6.5713
  [Val @ step 400]: train=6.5713, val=6.3220
Step  450: loss = 6.3639
  [Val @ step 450]: train=6.3639, val=6.3581
Step  500: loss = 6.3534
  [Val @ step 500]: train=6.3534, val=6.3460
Step  550: loss = 6.4756
  [Val @ step 550]: train=6.4756, val=6.2905
Step  600: loss = 5.9864
  [Val @ step 600]: train=5.9864, val=6.2768
Step  650: loss = 6.4972
  [Val @ step 650]: train=6.4972, val=6.2274
Step  700: loss = 6.1187
  [Val @ step 700]: train=6.1187, val=6.2437
Step  750: loss = 6.1350
  [Val @ step 750]: train=6.1350, val=6.2404
Step  800: loss = 6.2110
  [Val @ step 800]: train=6.2110, val=6.1864
Step  850: loss = 6.0719
  [Val @ step 850]: train=6.0719, val=6.1615
Step  900: loss = 6.2199
  [Val @ step 900]: train=6.2199, val=6.1973
Step  950: loss = 6.0575
  [Val @ step 950]: train=6.0575, val=6.1426
Step 1000: loss = 5.9306
  [Val @ step 1000]: train=5.9306, val=6.0988
Step 1050: loss = 6.0408
  [Val @ step 1050]: train=6.0408, val=6.1368
Step 1100: loss = 5.9963
  [Val @ step 1100]: train=5.9963, val=6.1005
Step 1150: loss = 5.9435
  [Val @ step 1150]: train=5.9435, val=6.0784
Step 1200: loss = 6.0379
  [Val @ step 1200]: train=6.0379, val=6.1075
Step 1250: loss = 6.2417
  [Val @ step 1250]: train=6.2417, val=6.0390
Step 1300: loss = 5.9824
  [Val @ step 1300]: train=5.9824, val=6.0332
Step 1350: loss = 6.0480
  [Val @ step 1350]: train=6.0480, val=6.0469
Step 1400: loss = 6.1161
  [Val @ step 1400]: train=6.1161, val=5.9819
Step 1450: loss = 5.9554
  [Val @ step 1450]: train=5.9554, val=6.0253
Step 1500: loss = 5.9857
  [Val @ step 1500]: train=5.9857, val=6.0206
Step 1550: loss = 5.8968
  [Val @ step 1550]: train=5.8968, val=5.9940
Step 1600: loss = 6.0064
  [Val @ step 1600]: train=6.0064, val=5.9888
Step 1650: loss = 6.0492
  [Val @ step 1650]: train=6.0492, val=5.9863
Step 1700: loss = 5.9946
  [Val @ step 1700]: train=5.9946, val=5.9656
Step 1750: loss = 6.1260
  [Val @ step 1750]: train=6.1260, val=5.9349
Step 1800: loss = 5.9122
  [Val @ step 1800]: train=5.9122, val=5.9718
Step 1850: loss = 5.9844
  [Val @ step 1850]: train=5.9844, val=5.9659
Step 1900: loss = 5.7979
  [Val @ step 1900]: train=5.7979, val=5.9210
Step 1950: loss = 5.8205
  [Val @ step 1950]: train=5.8205, val=5.9269
Step 2000: loss = 5.7948
  [Val @ step 2000]: train=5.7948, val=5.9088
Step 2050: loss = 5.9338
  [Val @ step 2050]: train=5.9338, val=5.8744
Step 2100: loss = 5.6813
  [Val @ step 2100]: train=5.6813, val=5.9043
Step 2150: loss = 6.0309
  [Val @ step 2150]: train=6.0309, val=5.9310
Step 2200: loss = 5.9739
  [Val @ step 2200]: train=5.9739, val=5.9340
Step 2250: loss = 5.9952
  [Val @ step 2250]: train=5.9952, val=5.8972
Step 2300: loss = 5.8234
  [Val @ step 2300]: train=5.8234, val=5.8608
Step 2350: loss = 6.0239
  [Val @ step 2350]: train=6.0239, val=5.9058
Step 2400: loss = 5.6914
  [Val @ step 2400]: train=5.6914, val=5.9044
Step 2450: loss = 5.6858
  [Val @ step 2450]: train=5.6858, val=5.8796
Step 2500: loss = 5.8579
  [Val @ step 2500]: train=5.8579, val=5.8859
Step 2550: loss = 6.0430
  [Val @ step 2550]: train=6.0430, val=5.8910
Step 2600: loss = 6.0248
  [Val @ step 2600]: train=6.0248, val=5.8508
Step 2650: loss = 5.8470
  [Val @ step 2650]: train=5.8470, val=5.8697
Step 2700: loss = 5.8451
  [Val @ step 2700]: train=5.8451, val=5.8427
Step 2750: loss = 5.8652
  [Val @ step 2750]: train=5.8652, val=5.8477
Step 2800: loss = 5.7915
  [Val @ step 2800]: train=5.7915, val=5.8429
Step 2850: loss = 5.7609
  [Val @ step 2850]: train=5.7609, val=5.8429
Step 2900: loss = 5.7890
  [Val @ step 2900]: train=5.7890, val=5.8693
Step 2950: loss = 5.9514
  [Val @ step 2950]: train=5.9514, val=5.8350
Step 3000: loss = 5.7329
  [Val @ step 3000]: train=5.7329, val=5.8646
Step 3050: loss = 5.8768
  [Val @ step 3050]: train=5.8768, val=5.8361
Step 3100: loss = 6.0102
  [Val @ step 3100]: train=6.0102, val=5.8162
Step 3150: loss = 5.8289
  [Val @ step 3150]: train=5.8289, val=5.8630
Step 3200: loss = 5.7350
  [Val @ step 3200]: train=5.7350, val=5.8478
Step 3250: loss = 5.8095
  [Val @ step 3250]: train=5.8095, val=5.8470
Step 3300: loss = 5.8373
  [Val @ step 3300]: train=5.8373, val=5.8122
Step 3350: loss = 5.9872
  [Val @ step 3350]: train=5.9872, val=5.8316
Step 3400: loss = 5.7670
  [Val @ step 3400]: train=5.7670, val=5.8523
Step 3450: loss = 5.8793
  [Val @ step 3450]: train=5.8793, val=5.8160
Step 3500: loss = 5.9487
  [Val @ step 3500]: train=5.9487, val=5.8292
Step 3550: loss = 5.8901
  [Val @ step 3550]: train=5.8901, val=5.7924
Step 3600: loss = 5.7382
  [Val @ step 3600]: train=5.7382, val=5.8385
Step 3650: loss = 5.9941
  [Val @ step 3650]: train=5.9941, val=5.7948
Step 3700: loss = 5.8452
  [Val @ step 3700]: train=5.8452, val=5.7931
Step 3750: loss = 5.9423
  [Val @ step 3750]: train=5.9423, val=5.8155
Step 3800: loss = 5.9420
  [Val @ step 3800]: train=5.9420, val=5.7506
Step 3850: loss = 5.8467
  [Val @ step 3850]: train=5.8467, val=5.8251
Step 3900: loss = 5.7480
  [Val @ step 3900]: train=5.7480, val=5.7882
Step 3950: loss = 5.8535
  [Val @ step 3950]: train=5.8535, val=5.7938
Step 4000: loss = 5.7277
  [Val @ step 4000]: train=5.7277, val=5.7879
Step 4050: loss = 5.5562
  [Val @ step 4050]: train=5.5562, val=5.7874
Step 4100: loss = 5.7825
  [Val @ step 4100]: train=5.7825, val=5.8025
Step 4150: loss = 5.6901
  [Val @ step 4150]: train=5.6901, val=5.7971
Step 4200: loss = 5.7523
  [Val @ step 4200]: train=5.7523, val=5.7951
Step 4250: loss = 5.9006
  [Val @ step 4250]: train=5.9006, val=5.7890
Step 4300: loss = 5.8420
  [Val @ step 4300]: train=5.8420, val=5.7828
Step 4350: loss = 5.7286
  [Val @ step 4350]: train=5.7286, val=5.8057
Step 4400: loss = 5.8216
  [Val @ step 4400]: train=5.8216, val=5.7853
Step 4450: loss = 5.8245
  [Val @ step 4450]: train=5.8245, val=5.7595
Step 4500: loss = 5.6463
  [Val @ step 4500]: train=5.6463, val=5.8164
Step 4550: loss = 5.7562
  [Val @ step 4550]: train=5.7562, val=5.7828
Step 4600: loss = 5.8055
  [Val @ step 4600]: train=5.8055, val=5.7729
Step 4650: loss = 5.7354
  [Val @ step 4650]: train=5.7354, val=5.7528
Step 4700: loss = 5.6661
  [Val @ step 4700]: train=5.6661, val=5.8123
Step 4750: loss = 5.6848
  [Val @ step 4750]: train=5.6848, val=5.7974
Step 4800: loss = 5.6826
  [Val @ step 4800]: train=5.6826, val=5.7903
Step 4850: loss = 5.7088
  [Val @ step 4850]: train=5.7088, val=5.7553
Step 4900: loss = 5.7488
  [Val @ step 4900]: train=5.7488, val=5.7511
Step 4950: loss = 5.8706
  [Val @ step 4950]: train=5.8706, val=5.7952
--------------------------------------------------------------------------------
Computing final validation loss...
Validation loss: 5.7348

Results saved to time_to_target_stage1_20251225_225546.jsonl
Training time: 3392.5s
Throughput: 6,037 tokens/sec
Projections: 715/5000
nGPT Architectural Experiment: F_mom0.9
================================================================================
Model: 11 layers x 768 dim (matching modded-nanogpt)
Hypothesis: Baseline
================================================================================

Loading FineWeb data (subset for fast experiments)...
  Loading data/fineweb10B/fineweb_train_000001.bin...
  Loading data/fineweb10B/fineweb_train_000002.bin...
  Loading data/fineweb10B/fineweb_train_000003.bin...
  Loading data/fineweb10B/fineweb_train_000004.bin...
  Vocab size: 50257
  Train tokens: 400,001,536
  Val tokens: 100,000,384

Model config (modded-nanogpt size):
  vocab_size: 50257
  n_layer: 11
  n_embd: 768
  n_head: 6
  block_size: 128
Using GatedBlock (H3) with gate_init=0.0

Total parameters: 155,166,743

Optimizer: SimpleMuon (lr=0.003, momentum=0.9, geodesic=baseline, orthog=newton_schulz)
  2D weight params (Muon): 77,856,768
  1D params (Adam): 77,309,975

Starting training (5000 steps)...
--------------------------------------------------------------------------------
Step    0: loss = 10.9894
Step   50: loss = 7.4464
  [Val @ step 50]: train=7.4464, val=7.3947
Step  100: loss = 7.0108
  [Val @ step 100]: train=7.0108, val=6.9526
Step  150: loss = 6.6538
  [Val @ step 150]: train=6.6538, val=6.7796
Step  200: loss = 6.6607
  [Val @ step 200]: train=6.6607, val=6.6453
Step  250: loss = 6.5439
  [Val @ step 250]: train=6.5439, val=6.5785
Step  300: loss = 6.5726
  [Val @ step 300]: train=6.5726, val=6.5087
Step  350: loss = 6.5575
  [Val @ step 350]: train=6.5575, val=6.4629
Step  400: loss = 6.3479
  [Val @ step 400]: train=6.3479, val=6.4411
Step  450: loss = 6.2530
  [Val @ step 450]: train=6.2530, val=6.3338
Step  500: loss = 6.1937
  [Val @ step 500]: train=6.1937, val=6.3465
Step  550: loss = 6.2703
  [Val @ step 550]: train=6.2703, val=6.2637
Step  600: loss = 6.2527
  [Val @ step 600]: train=6.2527, val=6.2628
Step  650: loss = 6.0011
  [Val @ step 650]: train=6.0011, val=6.2342
Step  700: loss = 6.0958
  [Val @ step 700]: train=6.0958, val=6.2226
Step  750: loss = 6.2607
  [Val @ step 750]: train=6.2607, val=6.2411
Step  800: loss = 6.1700
  [Val @ step 800]: train=6.1700, val=6.1843
Step  850: loss = 6.0188
  [Val @ step 850]: train=6.0188, val=6.1775
Step  900: loss = 6.1954
  [Val @ step 900]: train=6.1954, val=6.1326
Step  950: loss = 6.2176
  [Val @ step 950]: train=6.2176, val=6.1392
Step 1000: loss = 6.0063
  [Val @ step 1000]: train=6.0063, val=6.1402
Step 1050: loss = 5.8639
  [Val @ step 1050]: train=5.8639, val=6.0822
Step 1100: loss = 6.0508
  [Val @ step 1100]: train=6.0508, val=6.0898
Step 1150: loss = 6.2550
  [Val @ step 1150]: train=6.2550, val=6.0839
Step 1200: loss = 6.0181
  [Val @ step 1200]: train=6.0181, val=6.0735
Step 1250: loss = 6.2117
  [Val @ step 1250]: train=6.2117, val=6.0685
Step 1300: loss = 6.0007
  [Val @ step 1300]: train=6.0007, val=6.0170
Step 1350: loss = 6.1213
  [Val @ step 1350]: train=6.1213, val=6.0210
Step 1400: loss = 5.8211
  [Val @ step 1400]: train=5.8211, val=6.0381
Step 1450: loss = 5.8217
  [Val @ step 1450]: train=5.8217, val=6.0066
Step 1500: loss = 5.8509
  [Val @ step 1500]: train=5.8509, val=6.0006
Step 1550: loss = 6.0788
  [Val @ step 1550]: train=6.0788, val=5.9800
Step 1600: loss = 6.0870
  [Val @ step 1600]: train=6.0870, val=5.9574
Step 1650: loss = 5.9355
  [Val @ step 1650]: train=5.9355, val=5.9753
Step 1700: loss = 5.9868
  [Val @ step 1700]: train=5.9868, val=5.9913
Step 1750: loss = 5.9468
  [Val @ step 1750]: train=5.9468, val=5.9543
Step 1800: loss = 5.9031
  [Val @ step 1800]: train=5.9031, val=5.9742
Step 1850: loss = 6.1304
  [Val @ step 1850]: train=6.1304, val=5.9351
Step 1900: loss = 5.8566
  [Val @ step 1900]: train=5.8566, val=5.9364
Step 1950: loss = 5.7125
  [Val @ step 1950]: train=5.7125, val=5.9402
Step 2000: loss = 5.9368
  [Val @ step 2000]: train=5.9368, val=5.9185
Step 2050: loss = 5.7803
  [Val @ step 2050]: train=5.7803, val=5.9325
Step 2100: loss = 6.0075
  [Val @ step 2100]: train=6.0075, val=5.9000
Step 2150: loss = 5.9081
  [Val @ step 2150]: train=5.9081, val=5.9510
Step 2200: loss = 5.7778
  [Val @ step 2200]: train=5.7778, val=5.9004
Step 2250: loss = 5.7961
  [Val @ step 2250]: train=5.7961, val=5.8626
Step 2300: loss = 5.8929
  [Val @ step 2300]: train=5.8929, val=5.9092
Step 2350: loss = 5.9288
  [Val @ step 2350]: train=5.9288, val=5.8967
Step 2400: loss = 5.8677
  [Val @ step 2400]: train=5.8677, val=5.8773
Step 2450: loss = 5.9023
  [Val @ step 2450]: train=5.9023, val=5.8919
Step 2500: loss = 5.8834
  [Val @ step 2500]: train=5.8834, val=5.8862
Step 2550: loss = 5.8292
  [Val @ step 2550]: train=5.8292, val=5.8914
Step 2600: loss = 5.7930
  [Val @ step 2600]: train=5.7930, val=5.8562
Step 2650: loss = 5.5710
  [Val @ step 2650]: train=5.5710, val=5.8730
Step 2700: loss = 5.8983
  [Val @ step 2700]: train=5.8983, val=5.8543
Step 2750: loss = 5.9186
  [Val @ step 2750]: train=5.9186, val=5.8677
Step 2800: loss = 5.7093
  [Val @ step 2800]: train=5.7093, val=5.8684
Step 2850: loss = 5.8922
  [Val @ step 2850]: train=5.8922, val=5.8517
Step 2900: loss = 5.8853
  [Val @ step 2900]: train=5.8853, val=5.8244
Step 2950: loss = 5.6697
  [Val @ step 2950]: train=5.6697, val=5.8246
Step 3000: loss = 5.7610
  [Val @ step 3000]: train=5.7610, val=5.8380
Step 3050: loss = 5.7387
  [Val @ step 3050]: train=5.7387, val=5.8263
Step 3100: loss = 5.5719
  [Val @ step 3100]: train=5.5719, val=5.8346
Step 3150: loss = 5.7949
  [Val @ step 3150]: train=5.7949, val=5.8150
Step 3200: loss = 5.9418
  [Val @ step 3200]: train=5.9418, val=5.8539
Step 3250: loss = 5.8010
  [Val @ step 3250]: train=5.8010, val=5.7759
Step 3300: loss = 5.8064
  [Val @ step 3300]: train=5.8064, val=5.8077
Step 3350: loss = 5.7325
  [Val @ step 3350]: train=5.7325, val=5.8079
Step 3400: loss = 5.7997
  [Val @ step 3400]: train=5.7997, val=5.8432
Step 3450: loss = 5.8696
  [Val @ step 3450]: train=5.8696, val=5.8093
Step 3500: loss = 5.8340
  [Val @ step 3500]: train=5.8340, val=5.8403
Step 3550: loss = 5.7068
  [Val @ step 3550]: train=5.7068, val=5.7806
Step 3600: loss = 5.7740
  [Val @ step 3600]: train=5.7740, val=5.7773
Step 3650: loss = 5.5783
  [Val @ step 3650]: train=5.5783, val=5.8046
Step 3700: loss = 5.7148
  [Val @ step 3700]: train=5.7148, val=5.7974
Step 3750: loss = 5.9885
  [Val @ step 3750]: train=5.9885, val=5.8294
Step 3800: loss = 5.7078
  [Val @ step 3800]: train=5.7078, val=5.8231
Step 3850: loss = 5.8745
  [Val @ step 3850]: train=5.8745, val=5.7953
Step 3900: loss = 5.7260
  [Val @ step 3900]: train=5.7260, val=5.7945
Step 3950: loss = 5.9642
  [Val @ step 3950]: train=5.9642, val=5.7657
Step 4000: loss = 5.9566
  [Val @ step 4000]: train=5.9566, val=5.7937
Step 4050: loss = 5.8517
  [Val @ step 4050]: train=5.8517, val=5.7443
Step 4100: loss = 5.8084
  [Val @ step 4100]: train=5.8084, val=5.7916
Step 4150: loss = 5.6522
  [Val @ step 4150]: train=5.6522, val=5.8291
Step 4200: loss = 5.9746
  [Val @ step 4200]: train=5.9746, val=5.7838
Step 4250: loss = 5.6547
  [Val @ step 4250]: train=5.6547, val=5.7819
Step 4300: loss = 5.8887
  [Val @ step 4300]: train=5.8887, val=5.7646
Step 4350: loss = 5.5651
  [Val @ step 4350]: train=5.5651, val=5.7570
Step 4400: loss = 5.6111
  [Val @ step 4400]: train=5.6111, val=5.8071
Step 4450: loss = 5.6829
  [Val @ step 4450]: train=5.6829, val=5.7827
Step 4500: loss = 5.6323
  [Val @ step 4500]: train=5.6323, val=5.7806
Step 4550: loss = 5.8978
  [Val @ step 4550]: train=5.8978, val=5.7800
Step 4600: loss = 5.6853
  [Val @ step 4600]: train=5.6853, val=5.7996
Step 4650: loss = 5.8087
  [Val @ step 4650]: train=5.8087, val=5.7694
Step 4700: loss = 5.5942
  [Val @ step 4700]: train=5.5942, val=5.7470
Step 4750: loss = 5.7067
  [Val @ step 4750]: train=5.7067, val=5.7774
Step 4800: loss = 5.6526
  [Val @ step 4800]: train=5.6526, val=5.7479
Step 4850: loss = 5.7460
  [Val @ step 4850]: train=5.7460, val=5.7657
Step 4900: loss = 5.6343
  [Val @ step 4900]: train=5.6343, val=5.7636
Step 4950: loss = 5.7835
  [Val @ step 4950]: train=5.7835, val=5.7699
--------------------------------------------------------------------------------
Computing final validation loss...
Validation loss: 5.7461

Results saved to time_to_target_stage1_20251225_225546.jsonl
Training time: 3235.2s
Throughput: 6,330 tokens/sec
Projections: 715/5000
nGPT Architectural Experiment: F_mom0.95
================================================================================
Model: 11 layers x 768 dim (matching modded-nanogpt)
Hypothesis: Baseline
================================================================================

Loading FineWeb data (subset for fast experiments)...
  Loading data/fineweb10B/fineweb_train_000001.bin...
  Loading data/fineweb10B/fineweb_train_000002.bin...
  Loading data/fineweb10B/fineweb_train_000003.bin...
  Loading data/fineweb10B/fineweb_train_000004.bin...
  Vocab size: 50257
  Train tokens: 400,001,536
  Val tokens: 100,000,384

Model config (modded-nanogpt size):
  vocab_size: 50257
  n_layer: 11
  n_embd: 768
  n_head: 6
  block_size: 128
Using GatedBlock (H3) with gate_init=0.0

Total parameters: 155,166,743

Optimizer: SimpleMuon (lr=0.003, momentum=0.95, geodesic=baseline, orthog=newton_schulz)
  2D weight params (Muon): 77,856,768
  1D params (Adam): 77,309,975

Starting training (5000 steps)...
--------------------------------------------------------------------------------
Step    0: loss = 10.9774
Step   50: loss = 7.2360
  [Val @ step 50]: train=7.2360, val=7.3779
Step  100: loss = 6.8715
  [Val @ step 100]: train=6.8715, val=6.9807
Step  150: loss = 6.9360
  [Val @ step 150]: train=6.9360, val=6.7490
Step  200: loss = 6.5768
  [Val @ step 200]: train=6.5768, val=6.6256
Step  250: loss = 6.6820
  [Val @ step 250]: train=6.6820, val=6.5347
Step  300: loss = 6.3864
  [Val @ step 300]: train=6.3864, val=6.4164
Step  350: loss = 6.4322
  [Val @ step 350]: train=6.4322, val=6.4448
Step  400: loss = 6.2361
  [Val @ step 400]: train=6.2361, val=6.4279
Step  450: loss = 6.2032
  [Val @ step 450]: train=6.2032, val=6.3805
Step  500: loss = 6.3616
  [Val @ step 500]: train=6.3616, val=6.3229
Step  550: loss = 6.2588
  [Val @ step 550]: train=6.2588, val=6.3070
Step  600: loss = 6.2137
  [Val @ step 600]: train=6.2137, val=6.2558
Step  650: loss = 6.1737
  [Val @ step 650]: train=6.1737, val=6.2400
Step  700: loss = 6.1014
  [Val @ step 700]: train=6.1014, val=6.2443
Step  750: loss = 6.0692
  [Val @ step 750]: train=6.0692, val=6.1902
Step  800: loss = 6.0542
  [Val @ step 800]: train=6.0542, val=6.1727
Step  850: loss = 6.0740
  [Val @ step 850]: train=6.0740, val=6.2029
Step  900: loss = 6.1330
  [Val @ step 900]: train=6.1330, val=6.1419
Step  950: loss = 6.0214
  [Val @ step 950]: train=6.0214, val=6.1057
Step 1000: loss = 6.1291
  [Val @ step 1000]: train=6.1291, val=6.1143
Step 1050: loss = 6.2697
  [Val @ step 1050]: train=6.2697, val=6.0705
Step 1100: loss = 5.9518
  [Val @ step 1100]: train=5.9518, val=6.0552
Step 1150: loss = 5.9995
  [Val @ step 1150]: train=5.9995, val=6.0178
Step 1200: loss = 5.8232
  [Val @ step 1200]: train=5.8232, val=6.0676
Step 1250: loss = 6.0753
  [Val @ step 1250]: train=6.0753, val=6.0182
Step 1300: loss = 5.8771
  [Val @ step 1300]: train=5.8771, val=6.0034
Step 1350: loss = 6.0253
  [Val @ step 1350]: train=6.0253, val=6.0353
Step 1400: loss = 5.9833
  [Val @ step 1400]: train=5.9833, val=6.0052
Step 1450: loss = 5.8508
  [Val @ step 1450]: train=5.8508, val=5.9787
Step 1500: loss = 5.9104
  [Val @ step 1500]: train=5.9104, val=5.9901
Step 1550: loss = 5.9134
  [Val @ step 1550]: train=5.9134, val=5.9647
Step 1600: loss = 5.9498
  [Val @ step 1600]: train=5.9498, val=5.9745
Step 1650: loss = 6.0179
  [Val @ step 1650]: train=6.0179, val=5.9464
Step 1700: loss = 5.9779
  [Val @ step 1700]: train=5.9779, val=5.9365
Step 1750: loss = 6.0257
  [Val @ step 1750]: train=6.0257, val=5.9400
Step 1800: loss = 5.9858
  [Val @ step 1800]: train=5.9858, val=5.9188
Step 1850: loss = 5.8888
  [Val @ step 1850]: train=5.8888, val=5.9307
Step 1900: loss = 5.9151
  [Val @ step 1900]: train=5.9151, val=5.9110
Step 1950: loss = 6.0403
  [Val @ step 1950]: train=6.0403, val=5.9464
Step 2000: loss = 6.0960
  [Val @ step 2000]: train=6.0960, val=5.8772
Step 2050: loss = 5.9578
  [Val @ step 2050]: train=5.9578, val=5.9261
Step 2100: loss = 5.9042
  [Val @ step 2100]: train=5.9042, val=5.9091
Step 2150: loss = 5.8502
  [Val @ step 2150]: train=5.8502, val=5.8922
Step 2200: loss = 5.9196
  [Val @ step 2200]: train=5.9196, val=5.8553
Step 2250: loss = 5.9594
  [Val @ step 2250]: train=5.9594, val=5.8981
Step 2300: loss = 5.8708
  [Val @ step 2300]: train=5.8708, val=5.8898
Step 2350: loss = 5.9203
  [Val @ step 2350]: train=5.9203, val=5.8933
Step 2400: loss = 5.7824
  [Val @ step 2400]: train=5.7824, val=5.8504
Step 2450: loss = 5.6838
  [Val @ step 2450]: train=5.6838, val=5.8624
Step 2500: loss = 5.8133
  [Val @ step 2500]: train=5.8133, val=5.8291
Step 2550: loss = 6.1081
  [Val @ step 2550]: train=6.1081, val=5.8902
Step 2600: loss = 5.9147
  [Val @ step 2600]: train=5.9147, val=5.8755
Step 2650: loss = 5.8110
  [Val @ step 2650]: train=5.8110, val=5.8766
Step 2700: loss = 5.7998
  [Val @ step 2700]: train=5.7998, val=5.8565
Step 2750: loss = 5.7903
  [Val @ step 2750]: train=5.7903, val=5.8698
Step 2800: loss = 5.9060
  [Val @ step 2800]: train=5.9060, val=5.8370
Step 2850: loss = 5.9989
  [Val @ step 2850]: train=5.9989, val=5.8186
Step 2900: loss = 5.8669
  [Val @ step 2900]: train=5.8669, val=5.8684
Step 2950: loss = 5.8059
  [Val @ step 2950]: train=5.8059, val=5.8410
Step 3000: loss = 5.6886
  [Val @ step 3000]: train=5.6886, val=5.8043
Step 3050: loss = 5.8273
  [Val @ step 3050]: train=5.8273, val=5.8265
Step 3100: loss = 5.8376
  [Val @ step 3100]: train=5.8376, val=5.8158
Step 3150: loss = 5.7045
  [Val @ step 3150]: train=5.7045, val=5.7920
Step 3200: loss = 5.6832
  [Val @ step 3200]: train=5.6832, val=5.8377
Step 3250: loss = 5.8597
  [Val @ step 3250]: train=5.8597, val=5.8130
Step 3300: loss = 5.7373
  [Val @ step 3300]: train=5.7373, val=5.8236
Step 3350: loss = 5.7887
  [Val @ step 3350]: train=5.7887, val=5.8039
Step 3400: loss = 5.7000
  [Val @ step 3400]: train=5.7000, val=5.7906
Step 3450: loss = 5.7319
  [Val @ step 3450]: train=5.7319, val=5.8129
Step 3500: loss = 5.6708
  [Val @ step 3500]: train=5.6708, val=5.7910
Step 3550: loss = 5.8754
  [Val @ step 3550]: train=5.8754, val=5.7783
Step 3600: loss = 5.8553
  [Val @ step 3600]: train=5.8553, val=5.8099
Step 3650: loss = 5.6588
  [Val @ step 3650]: train=5.6588, val=5.7905
Step 3700: loss = 5.7751
  [Val @ step 3700]: train=5.7751, val=5.7869
Step 3750: loss = 5.9219
  [Val @ step 3750]: train=5.9219, val=5.7735
Step 3800: loss = 5.6631
  [Val @ step 3800]: train=5.6631, val=5.7853
Step 3850: loss = 5.7447
  [Val @ step 3850]: train=5.7447, val=5.7418
Step 3900: loss = 5.6988
  [Val @ step 3900]: train=5.6988, val=5.7813
Step 3950: loss = 5.6307
  [Val @ step 3950]: train=5.6307, val=5.7850
Step 4000: loss = 5.8613
  [Val @ step 4000]: train=5.8613, val=5.8022
Step 4050: loss = 5.7647
  [Val @ step 4050]: train=5.7647, val=5.7601
Step 4100: loss = 5.8360
  [Val @ step 4100]: train=5.8360, val=5.7650
Step 4150: loss = 5.8824
  [Val @ step 4150]: train=5.8824, val=5.8186
Step 4200: loss = 5.7521
  [Val @ step 4200]: train=5.7521, val=5.7657
Step 4250: loss = 5.7081
  [Val @ step 4250]: train=5.7081, val=5.7540
Step 4300: loss = 5.6568
  [Val @ step 4300]: train=5.6568, val=5.7583
Step 4350: loss = 5.8391
  [Val @ step 4350]: train=5.8391, val=5.7772
Step 4400: loss = 5.7701
  [Val @ step 4400]: train=5.7701, val=5.7695
Step 4450: loss = 5.8112
  [Val @ step 4450]: train=5.8112, val=5.7703
Step 4500: loss = 5.8939
  [Val @ step 4500]: train=5.8939, val=5.7579
Step 4550: loss = 5.7363
  [Val @ step 4550]: train=5.7363, val=5.7441
Step 4600: loss = 5.5566
  [Val @ step 4600]: train=5.5566, val=5.7570
Step 4650: loss = 5.8701
  [Val @ step 4650]: train=5.8701, val=5.7176
Step 4700: loss = 5.7001
  [Val @ step 4700]: train=5.7001, val=5.7641
Step 4750: loss = 5.7319
  [Val @ step 4750]: train=5.7319, val=5.7383
Step 4800: loss = 5.8759
  [Val @ step 4800]: train=5.8759, val=5.7551
Step 4850: loss = 5.5976
  [Val @ step 4850]: train=5.5976, val=5.7621
Step 4900: loss = 5.7685
  [Val @ step 4900]: train=5.7685, val=5.7388
Step 4950: loss = 5.5310
  [Val @ step 4950]: train=5.5310, val=5.8110
--------------------------------------------------------------------------------
Computing final validation loss...
Validation loss: 5.7416

Results saved to time_to_target_stage1_20251225_225546.jsonl
Training time: 2967.4s
Throughput: 6,902 tokens/sec
Projections: 715/5000
nGPT Architectural Experiment: E_batch64
================================================================================
Model: 11 layers x 768 dim (matching modded-nanogpt)
Hypothesis: Baseline
================================================================================

Loading FineWeb data (subset for fast experiments)...
  Loading data/fineweb10B/fineweb_train_000001.bin...
  Loading data/fineweb10B/fineweb_train_000002.bin...
  Loading data/fineweb10B/fineweb_train_000003.bin...
  Loading data/fineweb10B/fineweb_train_000004.bin...
  Vocab size: 50257
  Train tokens: 400,001,536
  Val tokens: 100,000,384

Model config (modded-nanogpt size):
  vocab_size: 50257
  n_layer: 11
  n_embd: 768
  n_head: 6
  block_size: 128
Using GatedBlock (H3) with gate_init=0.0

Total parameters: 155,166,743

Optimizer: SimpleMuon (lr=0.006, momentum=0.95, geodesic=baseline, orthog=newton_schulz)
  2D weight params (Muon): 77,856,768
  1D params (Adam): 77,309,975

Starting training (5000 steps)...
--------------------------------------------------------------------------------
Step    0: loss = 10.9826
Step   50: loss = 6.8312
  [Val @ step 50]: train=6.8312, val=6.9317
Step  100: loss = 6.7028
  [Val @ step 100]: train=6.7028, val=6.6197
Step  150: loss = 6.5743
  [Val @ step 150]: train=6.5743, val=6.5031
Step  200: loss = 6.4415
  [Val @ step 200]: train=6.4415, val=6.3817
Step  250: loss = 6.3118
  [Val @ step 250]: train=6.3118, val=6.3019
Step  300: loss = 6.2407
  [Val @ step 300]: train=6.2407, val=6.2736
Step  350: loss = 6.2930
  [Val @ step 350]: train=6.2930, val=6.2454
Step  400: loss = 6.1401
  [Val @ step 400]: train=6.1401, val=6.1918
Step  450: loss = 6.0885
  [Val @ step 450]: train=6.0885, val=6.1493
Step  500: loss = 5.9018
  [Val @ step 500]: train=5.9018, val=6.0993
Step  550: loss = 6.0448
  [Val @ step 550]: train=6.0448, val=6.0937
Step  600: loss = 6.1496
  [Val @ step 600]: train=6.1496, val=6.0583
Step  650: loss = 6.0161
  [Val @ step 650]: train=6.0161, val=6.0220
Step  700: loss = 6.0606
  [Val @ step 700]: train=6.0606, val=6.0293
Step  750: loss = 6.0710
  [Val @ step 750]: train=6.0710, val=6.0024
Step  800: loss = 5.9949
  [Val @ step 800]: train=5.9949, val=5.9897
Step  850: loss = 6.0603
  [Val @ step 850]: train=6.0603, val=5.9633
Step  900: loss = 5.8955
  [Val @ step 900]: train=5.8955, val=5.9891
Step  950: loss = 5.9374
  [Val @ step 950]: train=5.9374, val=5.9676
Step 1000: loss = 5.9265
  [Val @ step 1000]: train=5.9265, val=5.9479
Step 1050: loss = 5.9868
  [Val @ step 1050]: train=5.9868, val=5.9136
Step 1100: loss = 5.8787
  [Val @ step 1100]: train=5.8787, val=5.9377
Step 1150: loss = 5.8509
  [Val @ step 1150]: train=5.8509, val=5.9034
Step 1200: loss = 5.8006
  [Val @ step 1200]: train=5.8006, val=5.9414
Step 1250: loss = 5.8894
  [Val @ step 1250]: train=5.8894, val=5.9237
Step 1300: loss = 5.8514
  [Val @ step 1300]: train=5.8514, val=5.8906
Step 1350: loss = 5.9539
  [Val @ step 1350]: train=5.9539, val=5.8899
Step 1400: loss = 5.8805
  [Val @ step 1400]: train=5.8805, val=5.8691
Step 1450: loss = 5.9054
  [Val @ step 1450]: train=5.9054, val=5.9089
Step 1500: loss = 5.8228
  [Val @ step 1500]: train=5.8228, val=5.8847
Step 1550: loss = 5.9545
  [Val @ step 1550]: train=5.9545, val=5.8806
Step 1600: loss = 5.9232
  [Val @ step 1600]: train=5.9232, val=5.8726
Step 1650: loss = 5.7698
  [Val @ step 1650]: train=5.7698, val=5.8649
Step 1700: loss = 5.8821
  [Val @ step 1700]: train=5.8821, val=5.8394
Step 1750: loss = 5.7958
  [Val @ step 1750]: train=5.7958, val=5.8858
Step 1800: loss = 5.8056
  [Val @ step 1800]: train=5.8056, val=5.8513
Step 1850: loss = 5.8474
  [Val @ step 1850]: train=5.8474, val=5.8257
Step 1900: loss = 5.8952
  [Val @ step 1900]: train=5.8952, val=5.8414
Step 1950: loss = 5.7185
  [Val @ step 1950]: train=5.7185, val=5.8218
Step 2000: loss = 5.9104
  [Val @ step 2000]: train=5.9104, val=5.8510
Step 2050: loss = 5.7407
  [Val @ step 2050]: train=5.7407, val=5.8405
Step 2100: loss = 5.8632
  [Val @ step 2100]: train=5.8632, val=5.8223
Step 2150: loss = 5.7897
  [Val @ step 2150]: train=5.7897, val=5.8425
Step 2200: loss = 5.8136
  [Val @ step 2200]: train=5.8136, val=5.8401
Step 2250: loss = 5.9089
  [Val @ step 2250]: train=5.9089, val=5.8169
Step 2300: loss = 5.6734
  [Val @ step 2300]: train=5.6734, val=5.8074
Step 2350: loss = 5.7532
  [Val @ step 2350]: train=5.7532, val=5.8286
Step 2400: loss = 5.8270
  [Val @ step 2400]: train=5.8270, val=5.7967
Step 2450: loss = 5.7548
  [Val @ step 2450]: train=5.7548, val=5.8266
Step 2500: loss = 5.8166
  [Val @ step 2500]: train=5.8166, val=5.8266
Step 2550: loss = 5.6700
  [Val @ step 2550]: train=5.6700, val=5.8244
Step 2600: loss = 5.6175
  [Val @ step 2600]: train=5.6175, val=5.8137
Step 2650: loss = 5.7421
  [Val @ step 2650]: train=5.7421, val=5.7958
Step 2700: loss = 5.8691
  [Val @ step 2700]: train=5.8691, val=5.7907
Step 2750: loss = 5.7270
  [Val @ step 2750]: train=5.7270, val=5.8113
Step 2800: loss = 5.7795
  [Val @ step 2800]: train=5.7795, val=5.7722
Step 2850: loss = 5.7388
  [Val @ step 2850]: train=5.7388, val=5.8064
Step 2900: loss = 5.7659
  [Val @ step 2900]: train=5.7659, val=5.7865
Step 2950: loss = 5.7661
  [Val @ step 2950]: train=5.7661, val=5.8092
Step 3000: loss = 5.7085
  [Val @ step 3000]: train=5.7085, val=5.8021
Step 3050: loss = 5.7818
  [Val @ step 3050]: train=5.7818, val=5.7935
Step 3100: loss = 5.6459
  [Val @ step 3100]: train=5.6459, val=5.7717
Step 3150: loss = 5.8759
  [Val @ step 3150]: train=5.8759, val=5.7900
Step 3200: loss = 5.7211
  [Val @ step 3200]: train=5.7211, val=5.8035
Step 3250: loss = 5.7830
  [Val @ step 3250]: train=5.7830, val=5.8029
Step 3300: loss = 5.8486
  [Val @ step 3300]: train=5.8486, val=5.8076
Step 3350: loss = 5.7341
  [Val @ step 3350]: train=5.7341, val=5.7756
Step 3400: loss = 5.9003
  [Val @ step 3400]: train=5.9003, val=5.7806
Step 3450: loss = 5.7059
  [Val @ step 3450]: train=5.7059, val=5.7792
Step 3500: loss = 5.7369
  [Val @ step 3500]: train=5.7369, val=5.7661
Step 3550: loss = 5.7272
  [Val @ step 3550]: train=5.7272, val=5.7869
Step 3600: loss = 5.7255
  [Val @ step 3600]: train=5.7255, val=5.7702
Step 3650: loss = 5.7481
  [Val @ step 3650]: train=5.7481, val=5.7970
Step 3700: loss = 5.7981
  [Val @ step 3700]: train=5.7981, val=5.7773
Step 3750: loss = 5.7549
  [Val @ step 3750]: train=5.7549, val=5.7972
Step 3800: loss = 5.7960
  [Val @ step 3800]: train=5.7960, val=5.7795
Step 3850: loss = 5.7859
  [Val @ step 3850]: train=5.7859, val=5.7925
Step 3900: loss = 5.6909
  [Val @ step 3900]: train=5.6909, val=5.7408
Step 3950: loss = 5.8726
  [Val @ step 3950]: train=5.8726, val=5.7596
Step 4000: loss = 5.7388
  [Val @ step 4000]: train=5.7388, val=5.7745
Step 4050: loss = 5.7070
  [Val @ step 4050]: train=5.7070, val=5.7486
Step 4100: loss = 5.6908
  [Val @ step 4100]: train=5.6908, val=5.7815
Step 4150: loss = 5.6503
  [Val @ step 4150]: train=5.6503, val=5.7534
Step 4200: loss = 5.8753
  [Val @ step 4200]: train=5.8753, val=5.7753
Step 4250: loss = 5.6972
  [Val @ step 4250]: train=5.6972, val=5.7424
Step 4300: loss = 5.7093
  [Val @ step 4300]: train=5.7093, val=5.7455
Step 4350: loss = 5.7523
  [Val @ step 4350]: train=5.7523, val=5.7666
Step 4400: loss = 5.7850
  [Val @ step 4400]: train=5.7850, val=5.7792
Step 4450: loss = 5.7272
  [Val @ step 4450]: train=5.7272, val=5.7511
Step 4500: loss = 5.6920
  [Val @ step 4500]: train=5.6920, val=5.7311
Step 4550: loss = 5.6020
  [Val @ step 4550]: train=5.6020, val=5.7363
Step 4600: loss = 5.8415
  [Val @ step 4600]: train=5.8415, val=5.7363
Step 4650: loss = 5.7196
  [Val @ step 4650]: train=5.7196, val=5.7534
Step 4700: loss = 5.7444
  [Val @ step 4700]: train=5.7444, val=5.7361
Step 4750: loss = 5.8003
  [Val @ step 4750]: train=5.8003, val=5.7345
Step 4800: loss = 5.6828
  [Val @ step 4800]: train=5.6828, val=5.7261
Step 4850: loss = 5.7855
  [Val @ step 4850]: train=5.7855, val=5.7407
Step 4900: loss = 5.7293
  [Val @ step 4900]: train=5.7293, val=5.7671
Step 4950: loss = 5.7252
  [Val @ step 4950]: train=5.7252, val=5.7432
--------------------------------------------------------------------------------
Computing final validation loss...
Validation loss: 5.7289

Results saved to time_to_target_stage1_20251225_225546.jsonl
Training time: 5537.3s
Throughput: 7,397 tokens/sec
Projections: 715/5000
nGPT Architectural Experiment: E_batch128
================================================================================
Model: 11 layers x 768 dim (matching modded-nanogpt)
Hypothesis: Baseline
================================================================================

Loading FineWeb data (subset for fast experiments)...
  Loading data/fineweb10B/fineweb_train_000001.bin...
  Loading data/fineweb10B/fineweb_train_000002.bin...
  Loading data/fineweb10B/fineweb_train_000003.bin...
  Loading data/fineweb10B/fineweb_train_000004.bin...
  Vocab size: 50257
  Train tokens: 400,001,536
  Val tokens: 100,000,384

Model config (modded-nanogpt size):
  vocab_size: 50257
  n_layer: 11
  n_embd: 768
  n_head: 6
  block_size: 128
Using GatedBlock (H3) with gate_init=0.0

Total parameters: 155,166,743

Optimizer: SimpleMuon (lr=0.012, momentum=0.95, geodesic=baseline, orthog=newton_schulz)
  2D weight params (Muon): 77,856,768
  1D params (Adam): 77,309,975

Starting training (5000 steps)...
--------------------------------------------------------------------------------
Step    0: loss = 10.9717
Step   50: loss = 6.5386
  [Val @ step 50]: train=6.5386, val=6.6117
Step  100: loss = 6.4333
  [Val @ step 100]: train=6.4333, val=6.4246
Step  150: loss = 6.3037
  [Val @ step 150]: train=6.3037, val=6.2939
Step  200: loss = 6.1789
  [Val @ step 200]: train=6.1789, val=6.2409
Step  250: loss = 6.1828
  [Val @ step 250]: train=6.1828, val=6.1482
Step  300: loss = 6.1875
  [Val @ step 300]: train=6.1875, val=6.1148
Step  350: loss = 6.0639
  [Val @ step 350]: train=6.0639, val=6.0777
Step  400: loss = 6.0397
  [Val @ step 400]: train=6.0397, val=6.0675
Step  450: loss = 6.0644
  [Val @ step 450]: train=6.0644, val=6.0552
Step  500: loss = 5.9350
  [Val @ step 500]: train=5.9350, val=6.0180
Step  550: loss = 6.0808
  [Val @ step 550]: train=6.0808, val=6.0066
Step  600: loss = 5.9635
  [Val @ step 600]: train=5.9635, val=6.0151
Step  650: loss = 6.0421
  [Val @ step 650]: train=6.0421, val=5.9970
Step  700: loss = 5.9358
  [Val @ step 700]: train=5.9358, val=5.9795
Step  750: loss = 5.9126
  [Val @ step 750]: train=5.9126, val=5.9654
Step  800: loss = 5.9517
  [Val @ step 800]: train=5.9517, val=5.9464
Step  850: loss = 5.8655
  [Val @ step 850]: train=5.8655, val=5.9511
Step  900: loss = 5.9069
  [Val @ step 900]: train=5.9069, val=5.9454
Step  950: loss = 5.9248
  [Val @ step 950]: train=5.9248, val=5.9489
Step 1000: loss = 5.9472
  [Val @ step 1000]: train=5.9472, val=5.9571
Step 1050: loss = 5.9311
  [Val @ step 1050]: train=5.9311, val=5.9360
Step 1100: loss = 5.9198
  [Val @ step 1100]: train=5.9198, val=5.9372
Step 1150: loss = 5.8625
  [Val @ step 1150]: train=5.8625, val=5.9156
Step 1200: loss = 5.8821
  [Val @ step 1200]: train=5.8821, val=5.9360
Step 1250: loss = 5.8717
  [Val @ step 1250]: train=5.8717, val=5.9082
Step 1300: loss = 5.8677
  [Val @ step 1300]: train=5.8677, val=5.9114
Step 1350: loss = 5.8626
  [Val @ step 1350]: train=5.8626, val=5.9000
Step 1400: loss = 5.8906
  [Val @ step 1400]: train=5.8906, val=5.8951
Step 1450: loss = 5.9430
  [Val @ step 1450]: train=5.9430, val=5.8811
Step 1500: loss = 5.8259
  [Val @ step 1500]: train=5.8259, val=5.8809
Step 1550: loss = 5.7484
  [Val @ step 1550]: train=5.7484, val=5.8948
Step 1600: loss = 5.9517
  [Val @ step 1600]: train=5.9517, val=5.9049
Step 1650: loss = 5.8752
  [Val @ step 1650]: train=5.8752, val=5.8830
Step 1700: loss = 5.7757
  [Val @ step 1700]: train=5.7757, val=5.8739
Step 1750: loss = 5.8730
  [Val @ step 1750]: train=5.8730, val=5.8829
Step 1800: loss = 5.8891
  [Val @ step 1800]: train=5.8891, val=5.8766
Step 1850: loss = 5.7495
  [Val @ step 1850]: train=5.7495, val=5.8768
Step 1900: loss = 5.8431
  [Val @ step 1900]: train=5.8431, val=5.8783
Step 1950: loss = 5.7737
  [Val @ step 1950]: train=5.7737, val=5.8526
Step 2000: loss = 5.7947
  [Val @ step 2000]: train=5.7947, val=5.8590
Step 2050: loss = 5.8265
  [Val @ step 2050]: train=5.8265, val=5.8535
Step 2100: loss = 5.9165
  [Val @ step 2100]: train=5.9165, val=5.8701
Step 2150: loss = 5.8522
  [Val @ step 2150]: train=5.8522, val=5.8777
Step 2200: loss = 5.8525
  [Val @ step 2200]: train=5.8525, val=5.8233
Step 2250: loss = 5.8497
  [Val @ step 2250]: train=5.8497, val=5.8491
Step 2300: loss = 5.7650
  [Val @ step 2300]: train=5.7650, val=5.8567
Step 2350: loss = 5.8821
  [Val @ step 2350]: train=5.8821, val=5.8544
Step 2400: loss = 5.7375
  [Val @ step 2400]: train=5.7375, val=5.8680
Step 2450: loss = 5.7880
  [Val @ step 2450]: train=5.7880, val=5.8601
Step 2500: loss = 5.7926
  [Val @ step 2500]: train=5.7926, val=5.8276
Step 2550: loss = 5.7558
  [Val @ step 2550]: train=5.7558, val=5.8406
Step 2600: loss = 5.8420
  [Val @ step 2600]: train=5.8420, val=5.8304
Step 2650: loss = 5.8399
  [Val @ step 2650]: train=5.8399, val=5.8361
Step 2700: loss = 5.8554
  [Val @ step 2700]: train=5.8554, val=5.8537
Step 2750: loss = 5.8291
  [Val @ step 2750]: train=5.8291, val=5.8319
Step 2800: loss = 5.8701
  [Val @ step 2800]: train=5.8701, val=5.8444
Step 2850: loss = 5.7565
  [Val @ step 2850]: train=5.7565, val=5.8351
Step 2900: loss = 5.7682
  [Val @ step 2900]: train=5.7682, val=5.8325
Step 2950: loss = 5.7819
  [Val @ step 2950]: train=5.7819, val=5.8230
Step 3000: loss = 5.7203
  [Val @ step 3000]: train=5.7203, val=5.8340
Step 3050: loss = 5.7504
  [Val @ step 3050]: train=5.7504, val=5.8175
Step 3100: loss = 5.7758
  [Val @ step 3100]: train=5.7758, val=5.8408
Step 3150: loss = 5.7852
  [Val @ step 3150]: train=5.7852, val=5.8334
Step 3200: loss = 5.8416
  [Val @ step 3200]: train=5.8416, val=5.7998
Step 3250: loss = 5.8536
  [Val @ step 3250]: train=5.8536, val=5.8155
Step 3300: loss = 5.7624
  [Val @ step 3300]: train=5.7624, val=5.8233
Step 3350: loss = 5.8445
  [Val @ step 3350]: train=5.8445, val=5.8204
Step 3400: loss = 5.8804
  [Val @ step 3400]: train=5.8804, val=5.8057
Step 3450: loss = 5.7641
  [Val @ step 3450]: train=5.7641, val=5.8086
Step 3500: loss = 5.7820
  [Val @ step 3500]: train=5.7820, val=5.8032
Step 3550: loss = 5.8512
  [Val @ step 3550]: train=5.8512, val=5.8124
Step 3600: loss = 5.7638
  [Val @ step 3600]: train=5.7638, val=5.7920
Step 3650: loss = 5.8320
  [Val @ step 3650]: train=5.8320, val=5.8044
Step 3700: loss = 5.7812
  [Val @ step 3700]: train=5.7812, val=5.7908
Step 3750: loss = 5.7621
  [Val @ step 3750]: train=5.7621, val=5.8008
Step 3800: loss = 5.8827
  [Val @ step 3800]: train=5.8827, val=5.8013
Step 3850: loss = 5.7365
  [Val @ step 3850]: train=5.7365, val=5.8078
Step 3900: loss = 5.6802
  [Val @ step 3900]: train=5.6802, val=5.7905
Step 3950: loss = 5.7839
  [Val @ step 3950]: train=5.7839, val=5.8132
Step 4000: loss = 5.7369
  [Val @ step 4000]: train=5.7369, val=5.7876
Step 4050: loss = 5.7860
  [Val @ step 4050]: train=5.7860, val=5.8051
Step 4100: loss = 5.7749
  [Val @ step 4100]: train=5.7749, val=5.7944
Step 4150: loss = 5.7962
  [Val @ step 4150]: train=5.7962, val=5.7943
Step 4200: loss = 5.8343
  [Val @ step 4200]: train=5.8343, val=5.7723
Step 4250: loss = 5.7625
  [Val @ step 4250]: train=5.7625, val=5.7903
Step 4300: loss = 5.7655
  [Val @ step 4300]: train=5.7655, val=5.7690
Step 4350: loss = 5.8231
  [Val @ step 4350]: train=5.8231, val=5.7967
Step 4400: loss = 5.7142
  [Val @ step 4400]: train=5.7142, val=5.7834
Step 4450: loss = 5.7938
  [Val @ step 4450]: train=5.7938, val=5.7779
Step 4500: loss = 5.7657
  [Val @ step 4500]: train=5.7657, val=5.7964
Step 4550: loss = 5.7840
  [Val @ step 4550]: train=5.7840, val=5.7821
Step 4600: loss = 5.7929
  [Val @ step 4600]: train=5.7929, val=5.7767
Step 4650: loss = 5.7993
  [Val @ step 4650]: train=5.7993, val=5.8015
Step 4700: loss = 5.7096
  [Val @ step 4700]: train=5.7096, val=5.7706
Step 4750: loss = 5.7629
  [Val @ step 4750]: train=5.7629, val=5.7645
Step 4800: loss = 5.7813
  [Val @ step 4800]: train=5.7813, val=5.7947
Step 4850: loss = 5.8176
  [Val @ step 4850]: train=5.8176, val=5.7819
Step 4900: loss = 5.7042
  [Val @ step 4900]: train=5.7042, val=5.7676
Step 4950: loss = 5.7968
  [Val @ step 4950]: train=5.7968, val=5.7748
--------------------------------------------------------------------------------
Computing final validation loss...
Validation loss: 5.7669

Results saved to time_to_target_stage1_20251225_225546.jsonl
Training time: 5995.9s
Throughput: 13,663 tokens/sec
Projections: 715/5000
