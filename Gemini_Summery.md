# Gemini Summary Report

**Date:** December 25, 2025
**Project:** nGPT-muon
**Focus:** Normalized Transformer (nGPT) implementation optimized with Muon optimizer

---

## 1. Project Overview
The goal of this project is to implement and optimize the **Normalized Transformer (nGPT)** architecture, initially proposed by NVIDIA/Anthropic researchers, and further enhance it using the **Muon optimizer** (MomentUm Orthogonalized by Newton-schulz). The project aims to validate the nGPT claims of faster convergence and apply modern optimization techniques from `modded-nanogpt`.

## 2. Key Findings & Milestones

### Phase 1: Mac Validation (Completed)
- **Initial Baseline:** The original nGPT implementation showed extremely slow learning (32x slower than standard modded-nanogpt).
- **Root Cause:** Default hyperparameters from the paper (Alpha=0.05, Logit Scale ~0.09) were too conservative for this scale.
- **Solution:** Increasing `alpha` to **0.15** and `logit_scale` to **10.0** restored learning speed, achieving parity with standard transformers while maintaining perfect unit norm constraints.

### Phase 2: Alpha Sweep (H100) (Completed)
- **Experiment:** 12-round sweep on H100 GPUs.
- **Finding:** Optimal `alpha` is **0.28**, significantly higher than the paper's recommendation (0.05).
- **Result:** +4.9% improvement in validation loss over baseline.

### Phase 3: Optimization Sweep (Completed)
- **Experiment:** Investigated lazy projection and scalar vs. vector alpha.
- **Finding:** 
    - **Lazy Projection:** Projecting weights every **7-10 steps** (instead of every step) reduces overhead by 90% without quality loss.
    - **Scalar Alpha:** A single scalar alpha per layer is sufficient and cleaner than vector alpha.
- **Result:** +5.3% cumulative improvement.

### Phase 4: Advanced Optimization (Completed)
- **Experiment:** Systematic sweep of batch size, learning rates, and logit scaling.
- **Finding:** 
    - **Batch Size:** Larger batches (**48+**) are critical for normalized gradients.
    - **Logit Scale:** **15.0** is the sweet spot.
- **Result:** Total cumulative improvement of **14.1%** in validation loss and **2x** throughput gain (110K tokens/s).

## 3. Production Configuration
Based on the experiments, the optimal configuration for the 155M parameter model is:

| Parameter | Value | Reason |
|-----------|-------|--------|
| **Alpha** | 0.28 | Optimal signal flow (Phase 2) |
| **Logit Scale** | 15.0 | Maximize gradient dynamic range (Phase 4) |
| **Batch Size** | 48 | Stabilize normalized gradients (Phase 4) |
| **Optimizer** | AdamW / Muon | Muon integration in progress |
| **Lazy Projection** | Every 7 steps | 90% overhead reduction |
| **Scalar Alpha** | True | Parameter efficiency |

## 4. Current Status: Muon Integration
The project is currently in the **Muon Optimizer Optimization** phase.
- **Goal:** Leverage Muon's orthogonal updates which theoretically align perfectly with nGPT's hypersphere geometry.
- **Status:** `SimpleMuon` optimizer has been implemented in `train_architectural.py`.
- **Next Steps:** Validation of Muon, Learning Rate sweep, and testing Geodesic updates.

---
*Report generated by Gemini CLI based on project markdown records.*
