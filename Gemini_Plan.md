# Gemini Action Plan: Muon Optimizer Campaign

**Objective:** Complete the integration and optimization of the Muon optimizer for nGPT.

## Phase 1: Verification (Immediate)
**Goal:** Confirm `SimpleMuon` implementation works and doesn't diverge.

- [ ] **Action:** Run a short verification training run using `SimpleMuon`.
- [ ] **Command:** 
  ```bash
  python modded-nanogpt/train_architectural.py --name muon_verify --optimizer muon --steps 50 --lr 0.02
  ```
- [ ] **Success Criteria:** Training completes without errors; loss decreases; norms stay at 1.0.

## Phase 2: Learning Rate Sweep
**Goal:** Find the optimal Learning Rate (LR) for Muon in the normalized nGPT geometry. The unit norm constraint likely shifts the optimal LR compared to standard GPT.

- [ ] **Action:** Create `modded-nanogpt/muon_lr_sweep.py` to run parallel/serial experiments.
- [ ] **Sweep Values:** `[0.005, 0.01, 0.02, 0.035, 0.05, 0.1]`
- [ ] **Command:** Run the sweep script.
- [ ] **Output:** `MUON_LR_SWEEP_REPORT.md` analyzing the best LR.

## Phase 3: Geodesic Updates (Innovation)
**Goal:** Test if exact geodesic movement along the hypersphere is better than "Linear Step + Projection".

- [ ] **Action:** Modify `SimpleMuon` or add a `--geodesic` flag to `train_architectural.py`.
- [ ] **Math:**
  - *Current:* $W_{new} = \text{normalize}(W - \eta U)$
  - *Geodesic:* $W_{new} = W \cos(\eta) + U \sin(\eta)$ (Rotates $W$ by angle $\eta$ in direction $U$)
- [ ] **Experiment:** Compare Baseline (Projection) vs. Geodesic update at optimal LR.

## Phase 4: Advanced Features (Porting from modded-nanogpt)
**Goal:** Bring in state-of-the-art optimization features.

- [ ] **Action:** Port "Polar Express" orthogonalization (Triton kernel) to replace Newton-Schulz for speed.
- [ ] **Action:** Implement "Cautious Weight Decay" if beneficial.
- [ ] **Action:** Implement "Variance Reduction" for the AdamW-handled parameters (scalars/biases).

## Phase 5: Scale Up
**Goal:** Validate on full H100 setup.

- [ ] **Action:** Use `parallel_experiments.py` to run distributed Muon training.
- [ ] **Action:** Compare final Muon-nGPT against the Production Adam-nGPT baseline (Val Loss: ~6.2).

---
*Plan generated by Gemini CLI based on `MUON_OPTIMIZER_EXPERIMENTS.md`.*
